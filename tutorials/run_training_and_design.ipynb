{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7836ac7b-3e52-4870-b396-583bf42aa7b1",
   "metadata": {},
   "source": [
    "# Modeling + Design\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Rand scripts back-to-back to ensure compatability. \n",
    "- This notebook automatically tries to grab the most recent artifact from `/tmp/model_artifacts_*`\n",
    "    - You may need to update this or hand copy the artifact between `train.py` and `generate.py`\n",
    "- Settings (mostly for training) are adjusted for fast testing and will not reproduce similar results to the paper. \n",
    "- You will need to modify hyperparameters yourself. An obvious change is to set `--min_epochs 60` and `--max_epochs 200`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a853f-cc97-477f-945c-44a43aab7cf1",
   "metadata": {},
   "source": [
    "## Deploy training\n",
    "Deposit model in `/tmp/`. The progress bar doesn't play nice with the notebook, so you'll have to scroll a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b3ccd4-ce31-4608-984c-758a2a99fec1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "--------------------------------------------------\n",
      "\n",
      "K562 | top cut value: 10.54, bottom cut value: -6.43\n",
      "HepG2 | top cut value: 9.78, bottom cut value: -5.74\n",
      "SKNSH | top cut value: 10.36, bottom cut value: -6.4\n",
      "\n",
      "Number of examples discarded from top: 0\n",
      "Number of examples discarded from bottom: 0\n",
      "\n",
      "Number of examples available: 666435\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Padding sequences... \n",
      "\n",
      "Creating train/val/test datasets with tokenized sequences... \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Number of examples in train: 1691500 (253.81%)\n",
      "Number of examples in val:   55841 (8.38%)\n",
      "Number of examples in test:  45330 (6.8%)\n",
      "\n",
      "Excluded from train: -1126236 (-168.99)%\n",
      "--------------------------------------------------\n",
      "Copying gs://tewhey-public-data/CODA_resources/my-model.epoch_5-step_19885.pkl...\n",
      "- [1 files][ 18.5 MiB/ 18.5 MiB]                                                \n",
      "Operation completed over 1 objects/18.5 MiB.                                     \n",
      "Key conv1.conv.weight successfully matched\n",
      "Key conv1.conv.bias successfully matched\n",
      "Key conv1.bn_layer.weight successfully matched\n",
      "Key conv1.bn_layer.bias successfully matched\n",
      "Key conv1.bn_layer.running_mean successfully matched\n",
      "Key conv1.bn_layer.running_var successfully matched\n",
      "Key conv1.bn_layer.num_batches_tracked successfully matched\n",
      "Key conv2.conv.weight successfully matched\n",
      "Key conv2.conv.bias successfully matched\n",
      "Key conv2.bn_layer.weight successfully matched\n",
      "Key conv2.bn_layer.bias successfully matched\n",
      "Key conv2.bn_layer.running_mean successfully matched\n",
      "Key conv2.bn_layer.running_var successfully matched\n",
      "Key conv2.bn_layer.num_batches_tracked successfully matched\n",
      "Key conv3.conv.weight successfully matched\n",
      "Key conv3.conv.bias successfully matched\n",
      "Key conv3.bn_layer.weight successfully matched\n",
      "Key conv3.bn_layer.bias successfully matched\n",
      "Key conv3.bn_layer.running_mean successfully matched\n",
      "Key conv3.bn_layer.running_var successfully matched\n",
      "Key conv3.bn_layer.num_batches_tracked successfully matched\n",
      "Key linear1.linear.weight successfully matched\n",
      "Key linear1.linear.bias successfully matched\n",
      "Key linear1.bn_layer.weight successfully matched\n",
      "Key linear1.bn_layer.bias successfully matched\n",
      "Key linear1.bn_layer.running_mean successfully matched\n",
      "Key linear1.bn_layer.running_var successfully matched\n",
      "Key linear1.bn_layer.num_batches_tracked successfully matched\n",
      "Missing key in dict: branched.branched_layer_1.weight\n",
      "Missing key in dict: branched.branched_layer_1.bias\n",
      "Missing key in dict: branched.branched_layer_2.weight\n",
      "Missing key in dict: branched.branched_layer_2.bias\n",
      "Missing key in dict: branched.branched_layer_3.weight\n",
      "Missing key in dict: branched.branched_layer_3.bias\n",
      "Size mismatch for key: output.weight, expected size torch.Size([3, 140, 1]), got torch.Size([280, 1000])\n",
      "Size mismatch for key: output.bias, expected size torch.Size([3, 1, 1]), got torch.Size([280])\n",
      "Skipped loading key: linear2.linear.weight of size torch.Size([1000, 1000])\n",
      "Skipped loading key: linear2.linear.bias of size torch.Size([1000])\n",
      "Skipped loading key: linear2.bn_layer.weight of size torch.Size([1000])\n",
      "Skipped loading key: linear2.bn_layer.bias of size torch.Size([1000])\n",
      "Skipped loading key: linear2.bn_layer.running_mean of size torch.Size([1000])\n",
      "Skipped loading key: linear2.bn_layer.running_var of size torch.Size([1000])\n",
      "Skipped loading key: linear2.bn_layer.num_batches_tracked of size torch.Size([])\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Found 4107183 parameters\n",
      "\n",
      "  | Name      | Type           | Params\n",
      "---------------------------------------------\n",
      "0 | model     | BassetBranched | 4.1 M \n",
      "1 | criterion | L1KLmixed      | 0     \n",
      "---------------------------------------------\n",
      "4.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 M     Total params\n",
      "8.214     Total estimated model params size (MB)\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2917: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "Sanity Checking DataLoader 0: 100%|███████████████| 2/2 [00:01<00:00,  1.09it/s]/opt/conda/lib/python3.7/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:236: UserWarning: You called `self.log('current_epoch', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | arithmetic_mean_loss: 0.12046 | harmonic_mean_loss: 0.94016 | prediction_mean_spearman: 0.13340 | entropy_spearman: 0.00780 |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 0:   0%|                                         | 0/1625 [00:00<?, ?it/s]starting epoch 0\n",
      "Epoch 0:  97%|████████▋| 1573/1625 [01:54<00:03, 13.78it/s, loss=0.105, v_num=7]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  97%|████████▋| 1574/1625 [01:55<00:03, 13.68it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  97%|████████▋| 1575/1625 [01:55<00:03, 13.69it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  97%|████████▋| 1576/1625 [01:55<00:03, 13.69it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  97%|████████▋| 1577/1625 [01:55<00:03, 13.70it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  97%|████████▋| 1578/1625 [01:55<00:03, 13.70it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  97%|████████▋| 1579/1625 [01:55<00:03, 13.71it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  97%|████████▊| 1580/1625 [01:55<00:03, 13.72it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  97%|████████▊| 1581/1625 [01:55<00:03, 13.72it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  97%|████████▊| 1582/1625 [01:55<00:03, 13.73it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  97%|████████▊| 1583/1625 [01:55<00:03, 13.74it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  97%|████████▊| 1584/1625 [01:55<00:02, 13.74it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1585/1625 [01:55<00:02, 13.75it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1586/1625 [01:55<00:02, 13.75it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1587/1625 [01:55<00:02, 13.76it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1588/1625 [01:55<00:02, 13.77it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1589/1625 [01:55<00:02, 13.77it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1590/1625 [01:55<00:02, 13.78it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1591/1625 [01:55<00:02, 13.78it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1592/1625 [01:55<00:02, 13.79it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1593/1625 [01:55<00:02, 13.80it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1594/1625 [01:55<00:02, 13.80it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1595/1625 [01:55<00:02, 13.81it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1596/1625 [01:55<00:02, 13.81it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1597/1625 [01:55<00:02, 13.82it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1598/1625 [01:55<00:01, 13.83it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1599/1625 [01:55<00:01, 13.83it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1600/1625 [01:55<00:01, 13.84it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 1601/1625 [01:55<00:01, 13.84it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 1602/1625 [01:55<00:01, 13.85it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1603/1625 [01:55<00:01, 13.86it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1604/1625 [01:55<00:01, 13.86it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1605/1625 [01:55<00:01, 13.87it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1606/1625 [01:55<00:01, 13.87it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1607/1625 [01:55<00:01, 13.88it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1608/1625 [01:55<00:01, 13.89it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1609/1625 [01:55<00:01, 13.89it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1610/1625 [01:55<00:01, 13.90it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1611/1625 [01:55<00:01, 13.90it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1612/1625 [01:55<00:00, 13.91it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1613/1625 [01:55<00:00, 13.92it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1614/1625 [01:55<00:00, 13.92it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1615/1625 [01:55<00:00, 13.93it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1616/1625 [01:55<00:00, 13.93it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1617/1625 [01:56<00:00, 13.94it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1618/1625 [01:56<00:00, 13.95it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1619/1625 [01:56<00:00, 13.95it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1620/1625 [01:56<00:00, 13.96it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1621/1625 [01:56<00:00, 13.96it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1622/1625 [01:56<00:00, 13.97it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1623/1625 [01:56<00:00, 13.98it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1624/1625 [01:56<00:00, 13.98it/s, loss=0.105, v_num=7]\u001b[A\n",
      "Epoch 0: 100%|█████████| 1625/1625 [01:56<00:00, 13.99it/s, loss=0.105, v_num=7]\u001b[A\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | arithmetic_mean_loss: 0.09070 | harmonic_mean_loss: 0.48685 | prediction_mean_spearman: 0.69962 | entropy_spearman: 0.39546 |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 0: 100%|█████████| 1625/1625 [01:56<00:00, 13.98it/s, loss=0.105, v_num=7]\n",
      "Epoch 1:   0%|                    | 0/1625 [00:00<?, ?it/s, loss=0.105, v_num=7]starting epoch 1\n",
      "Epoch 1:  97%|███████▋| 1573/1625 [01:53<00:03, 13.88it/s, loss=0.0881, v_num=7]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  97%|███████▋| 1574/1625 [01:54<00:03, 13.78it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1575/1625 [01:54<00:03, 13.79it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1576/1625 [01:54<00:03, 13.79it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1577/1625 [01:54<00:03, 13.80it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1578/1625 [01:54<00:03, 13.81it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1579/1625 [01:54<00:03, 13.81it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1580/1625 [01:54<00:03, 13.82it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1581/1625 [01:54<00:03, 13.82it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1582/1625 [01:54<00:03, 13.83it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1583/1625 [01:54<00:03, 13.84it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  97%|███████▊| 1584/1625 [01:54<00:02, 13.84it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1585/1625 [01:54<00:02, 13.85it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1586/1625 [01:54<00:02, 13.85it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1587/1625 [01:54<00:02, 13.86it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1588/1625 [01:54<00:02, 13.87it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1589/1625 [01:54<00:02, 13.87it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1590/1625 [01:54<00:02, 13.88it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1591/1625 [01:54<00:02, 13.88it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1592/1625 [01:54<00:02, 13.89it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1593/1625 [01:54<00:02, 13.90it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1594/1625 [01:54<00:02, 13.90it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1595/1625 [01:54<00:02, 13.91it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1596/1625 [01:54<00:02, 13.91it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1597/1625 [01:54<00:02, 13.92it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1598/1625 [01:54<00:01, 13.93it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▊| 1599/1625 [01:54<00:01, 13.93it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  98%|███████▉| 1600/1625 [01:54<00:01, 13.94it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1601/1625 [01:54<00:01, 13.94it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1602/1625 [01:54<00:01, 13.95it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1603/1625 [01:54<00:01, 13.96it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1604/1625 [01:54<00:01, 13.96it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1605/1625 [01:54<00:01, 13.97it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1606/1625 [01:54<00:01, 13.98it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1607/1625 [01:54<00:01, 13.98it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1608/1625 [01:54<00:01, 13.99it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1609/1625 [01:54<00:01, 13.99it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1610/1625 [01:55<00:01, 14.00it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1611/1625 [01:55<00:00, 14.01it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1612/1625 [01:55<00:00, 14.01it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1613/1625 [01:55<00:00, 14.02it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1614/1625 [01:55<00:00, 14.02it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1615/1625 [01:55<00:00, 14.03it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1:  99%|███████▉| 1616/1625 [01:55<00:00, 14.04it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1617/1625 [01:55<00:00, 14.04it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1618/1625 [01:55<00:00, 14.05it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1619/1625 [01:55<00:00, 14.05it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1620/1625 [01:55<00:00, 14.06it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1621/1625 [01:55<00:00, 14.07it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1622/1625 [01:55<00:00, 14.07it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1623/1625 [01:55<00:00, 14.08it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1: 100%|███████▉| 1624/1625 [01:55<00:00, 14.08it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "Epoch 1: 100%|████████| 1625/1625 [01:55<00:00, 14.09it/s, loss=0.0881, v_num=7]\u001b[A\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 1.00000 | arithmetic_mean_loss: 0.08046 | harmonic_mean_loss: 0.37448 | prediction_mean_spearman: 0.76120 | entropy_spearman: 0.46547 |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 1: 100%|████████| 1625/1625 [01:55<00:00, 14.09it/s, loss=0.0881, v_num=7]\n",
      "Epoch 2:   0%|                   | 0/1625 [00:00<?, ?it/s, loss=0.0881, v_num=7]starting epoch 2\n",
      "Epoch 2:  97%|███████▋| 1573/1625 [02:35<00:05, 10.15it/s, loss=0.0976, v_num=7]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  97%|███████▋| 1574/1625 [02:35<00:05, 10.10it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1575/1625 [02:35<00:04, 10.10it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1576/1625 [02:35<00:04, 10.11it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1577/1625 [02:35<00:04, 10.11it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1578/1625 [02:35<00:04, 10.12it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1579/1625 [02:36<00:04, 10.12it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1580/1625 [02:36<00:04, 10.13it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1581/1625 [02:36<00:04, 10.13it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1582/1625 [02:36<00:04, 10.14it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1583/1625 [02:36<00:04, 10.14it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  97%|███████▊| 1584/1625 [02:36<00:04, 10.15it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1585/1625 [02:36<00:03, 10.15it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1586/1625 [02:36<00:03, 10.16it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1587/1625 [02:36<00:03, 10.16it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1588/1625 [02:36<00:03, 10.17it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1589/1625 [02:36<00:03, 10.17it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1590/1625 [02:36<00:03, 10.18it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1591/1625 [02:36<00:03, 10.18it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1592/1625 [02:36<00:03, 10.19it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1593/1625 [02:36<00:03, 10.19it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1594/1625 [02:36<00:03, 10.20it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1595/1625 [02:36<00:02, 10.20it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1596/1625 [02:36<00:02, 10.21it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1597/1625 [02:36<00:02, 10.21it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1598/1625 [02:36<00:02, 10.22it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▊| 1599/1625 [02:36<00:02, 10.22it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  98%|███████▉| 1600/1625 [02:36<00:02, 10.23it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1601/1625 [02:36<00:02, 10.23it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1602/1625 [02:36<00:02, 10.24it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1603/1625 [02:36<00:02, 10.24it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1604/1625 [02:36<00:02, 10.24it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1605/1625 [02:36<00:01, 10.25it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1606/1625 [02:36<00:01, 10.25it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1607/1625 [02:36<00:01, 10.26it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1608/1625 [02:36<00:01, 10.26it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1609/1625 [02:36<00:01, 10.27it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1610/1625 [02:36<00:01, 10.27it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1611/1625 [02:36<00:01, 10.28it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1612/1625 [02:36<00:01, 10.28it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1613/1625 [02:36<00:01, 10.29it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1614/1625 [02:36<00:01, 10.29it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1615/1625 [02:36<00:00, 10.30it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2:  99%|███████▉| 1616/1625 [02:36<00:00, 10.30it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1617/1625 [02:36<00:00, 10.31it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1618/1625 [02:36<00:00, 10.31it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1619/1625 [02:36<00:00, 10.32it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1620/1625 [02:36<00:00, 10.32it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1621/1625 [02:36<00:00, 10.33it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1622/1625 [02:36<00:00, 10.33it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1623/1625 [02:36<00:00, 10.34it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2: 100%|███████▉| 1624/1625 [02:37<00:00, 10.34it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "Epoch 2: 100%|████████| 1625/1625 [02:37<00:00, 10.35it/s, loss=0.0976, v_num=7]\u001b[A\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 2.00000 | arithmetic_mean_loss: 0.08558 | harmonic_mean_loss: 0.42811 | prediction_mean_spearman: 0.72668 | entropy_spearman: 0.39296 |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 2: 100%|████████| 1625/1625 [02:37<00:00, 10.35it/s, loss=0.0976, v_num=7]\n",
      "Epoch 2: 100%|████████| 1625/1625 [02:37<00:00, 10.35it/s, loss=0.0976, v_num=7]`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "Epoch 2: 100%|████████| 1625/1625 [02:37<00:00, 10.35it/s, loss=0.0976, v_num=7]\n",
      "Best model stashed at: /tmp/output/artifacts/lightning_logs/version_7/checkpoints/epoch=1-step=3146.ckpt\n",
      "Exists: True\n",
      "Setting model from epoch: 1\n",
      "entropy_spearman at 4719: 0.46547067165374756\n"
     ]
    }
   ],
   "source": [
    "!python /home/ubuntu/boda2/src/train.py \\\n",
    "  --data_module=MPRA_DataModule \\\n",
    "    --datafile_path=gs://tewhey-public-data/CODA_resources/MPRA_ALL_HD_v2.txt \\\n",
    "    --sep space --sequence_column nt_sequence \\\n",
    "    --activity_columns K562_mean HepG2_mean SKNSH_mean \\\n",
    "    --stderr_columns lfcSE_k562 lfcSE_hepg2 lfcSE_sknsh \\\n",
    "    --synth_val_pct=0.0 --synth_test_pct=99.98 \\\n",
    "    --batch_size=1076 --duplication_cutoff=0.5 --std_multiple_cut=6.0 \\\n",
    "    --val_chrs 7 13 --test_chrs 9 21 X \\\n",
    "    --padded_seq_len=600 --use_reverse_complements=True --num_workers=8 \\\n",
    "  --model_module=BassetBranched \\\n",
    "    --input_len 600 \\\n",
    "    --conv1_channels=300 --conv1_kernel_size=19 \\\n",
    "    --conv2_channels=200 --conv2_kernel_size=11 \\\n",
    "    --conv3_channels=200 --conv3_kernel_size=7 \\\n",
    "    --linear_activation=ReLU --linear_channels=1000 \\\n",
    "    --linear_dropout_p=0.11625456877954289 \\\n",
    "    --branched_activation=ReLU --branched_channels=140 \\\n",
    "    --branched_dropout_p=0.5757068086404574 \\\n",
    "    --n_outputs=3 --n_linear_layers=1 \\\n",
    "    --n_branched_layers=3 --n_branched_layers=3 \\\n",
    "    --use_batch_norm=True --use_weight_norm=False \\\n",
    "    --loss_criterion=L1KLmixed --beta=5.0 \\\n",
    "    --reduction=mean \\\n",
    "  --graph_module=CNNTransferLearning \\\n",
    "    --parent_weights=gs://tewhey-public-data/CODA_resources/my-model.epoch_5-step_19885.pkl \\\n",
    "    --frozen_epochs=0 \\\n",
    "    --optimizer=Adam --amsgrad=True \\\n",
    "    --lr=0.0032658700881052086 --eps=1e-08 --weight_decay=0.0003438210249762151 \\\n",
    "    --beta1=0.8661062881299633 --beta2=0.879223105336538 \\\n",
    "    --scheduler=CosineAnnealingWarmRestarts --scheduler_interval=step \\\n",
    "    --T_0=4096 --T_mult=1 --eta_min=0.0 --last_epoch=-1 \\\n",
    "    --checkpoint_monitor=entropy_spearman --stopping_mode=max \\\n",
    "    --stopping_patience=30 --accelerator=gpu --devices=1 --min_epochs=1 --max_epochs=3 \\\n",
    "    --precision=16 --default_root_dir=/tmp/output/artifacts \\\n",
    "    --artifact_path=/tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c057c926-84dc-4c7d-bcd8-f86c8963c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_artifacts__20240216_154316__693863.tar.gz\n",
      "model_artifacts__20240216_160210__448365.tar.gz\n",
      "model_artifacts__20240216_162013__449584.tar.gz\n"
     ]
    }
   ],
   "source": [
    "files = !ls /tmp/\n",
    "\n",
    "_ = [ print(p) for p in files if 'model_artifacts' in p ]\n",
    "model_artifact = [ p for p in files if 'model_artifacts' in p ][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b97e1a-7a9e-4ecb-88dc-1cb50dd5cca5",
   "metadata": {},
   "source": [
    "## Design sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c3e6d-9a9e-41a5-bccc-82776a05dab1",
   "metadata": {},
   "source": [
    "### Fast SeqProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f03e2f42-e895-45bb-a380-86ae700eb8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive unpacked in /tmp/tmpkf2h83rp\n",
      "Loaded model from 20240216_162013 in eval mode\n",
      "Starting round: 0, generate 1000 proposals\n",
      "Steps:   0%|                                            | 0/200 [00:00<?, ?it/s]Penalty not implemented\n",
      "Steps: 100%|█████████████| 200/200 [00:54<00:00,  3.65it/s, Loss=-5.19, LR=1e-6]\n",
      "Steps: 100%|█████████████| 200/200 [00:53<00:00,  3.75it/s, Loss=-5.22, LR=1e-6]\n",
      "Steps: 100%|█████████████| 200/200 [00:53<00:00,  3.76it/s, Loss=-5.16, LR=1e-6]\n",
      "Steps: 100%|█████████████| 200/200 [00:54<00:00,  3.69it/s, Loss=-5.17, LR=1e-6]\n",
      "finished round\n",
      "Proposals deposited at:\n",
      "\t/tmp/test__k562__fsp__20240216_162917__734583.pt\n"
     ]
    }
   ],
   "source": [
    "!python /home/ubuntu/boda2/src/generate.py \\\n",
    "    --params_module StraightThroughParameters \\\n",
    "        --batch_size 256 --n_channels 4 \\\n",
    "        --length 200 --n_samples 10 \\\n",
    "        --use_norm True --use_affine False \\\n",
    "    --energy_module MinGapEnergy \\\n",
    "        --target_feature 0 --bending_factor 1.0 --a_min -2.0 --a_max 6.0 \\\n",
    "        --model_artifact /tmp/{model_artifact} \\\n",
    "    --generator_module FastSeqProp \\\n",
    "         --n_steps 200 --learning_rate 0.5 \\\n",
    "    --energy_threshold -0.5 --max_attempts 40 \\\n",
    "    --n_proposals 1000 \\\n",
    "    --proposal_path /tmp/test__k562__fsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79162b13-a515-4cca-8d42-a3c711c05544",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proposals': [{'states': tensor([[[-3.7377, -2.7889, -3.6888,  ..., -2.0091, -0.8512, -1.1654],\n",
       "            [-2.8875, 15.6401, -4.9318,  ...,  2.0829,  2.9065,  4.5684],\n",
       "            [ 8.8135, -3.0222, -6.0940,  ..., -1.1214, -0.8691, -0.1751],\n",
       "            [-3.1758, -1.7191, 11.4275,  ...,  1.0498, -1.3573, -0.9823]],\n",
       "   \n",
       "           [[-3.5919, -3.8258, -3.3337,  ..., -1.7833, -0.8523, -0.0423],\n",
       "            [ 1.5290,  4.8438, -2.4072,  ..., -0.7916,  1.8525,  1.1274],\n",
       "            [ 5.3218, -2.5268, -2.2526,  ...,  2.0384,  0.2067,  0.7588],\n",
       "            [-1.2801,  1.5561,  5.4194,  ..., -0.0931,  0.0430, -2.0858]],\n",
       "   \n",
       "           [[-3.6997, -3.4999, -3.2232,  ..., -3.7783, -2.9031, -4.1824],\n",
       "            [ 0.4595,  8.3717, -3.2004,  ..., -3.0341, -4.0613, -5.0852],\n",
       "            [ 9.8184, -2.8284, -4.2960,  ..., -4.2540, 11.8247, -5.7616],\n",
       "            [-2.0383, -2.2265,  5.9676,  ...,  7.8807, -4.6413,  8.0427]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[-4.1132, -3.3941, -2.6604,  ..., -4.4621,  6.5884, -2.0731],\n",
       "            [ 0.1368,  8.2110, -2.7441,  ..., 11.3696, -4.7566,  4.6102],\n",
       "            [ 8.9176, -2.2561, -2.8018,  ..., -4.7823, -3.1984,  0.6938],\n",
       "            [-2.7130, -1.9938,  5.0728,  ..., -4.3849, -2.2153, -2.7399]],\n",
       "   \n",
       "           [[-4.7145, -3.5296, -3.3194,  ..., -2.3083, -2.0628, -2.1419],\n",
       "            [ 3.4198, 11.0848, -3.3041,  ...,  2.0049,  8.4542,  8.8260],\n",
       "            [ 7.4287, -2.6671, -4.1315,  ..., -2.3984, -2.0252, -1.9243],\n",
       "            [-2.4269, -1.7483,  7.8385,  ...,  1.8806, -2.4114, -2.4145]],\n",
       "   \n",
       "           [[-3.9611, -3.2279, -2.4753,  ..., -5.0117, -4.0866,  8.1072],\n",
       "            [ 0.9185,  9.6686, -2.6827,  ..., -6.2565, 17.2734, -6.7961],\n",
       "            [ 7.5395, -2.9704, -4.4435,  ..., -6.6460, -5.1694, -4.6506],\n",
       "            [-1.9794, -1.7493,  6.2873,  ..., 11.8298, -4.2452, -2.4307]]]),\n",
       "   'proposals': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "            [1., 0., 0.,  ..., 1., 0., 1.],\n",
       "            [0., 0., 1.,  ..., 0., 1., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 1., 0.,  ..., 0., 1., 0.],\n",
       "            [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "            [1., 0., 1.,  ..., 0., 0., 1.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [0., 0., 1.,  ..., 1., 0., 1.]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [0., 1., 0.,  ..., 1., 0., 1.],\n",
       "            [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 1.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 1., 0.,  ..., 0., 1., 1.],\n",
       "            [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 1.,  ..., 1., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "            [0., 1., 0.,  ..., 0., 1., 0.],\n",
       "            [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 1.,  ..., 1., 0., 0.]]]),\n",
       "   'energies': tensor([-5.8443, -6.4940, -6.6285, -6.1666, -6.6114, -6.6377, -6.1308, -6.6988,\n",
       "           -6.0169, -6.2640, -7.1710, -6.3077, -6.6822, -6.7830, -6.0085, -6.1354,\n",
       "           -6.1039, -6.0758, -5.6823, -6.3940, -7.1561, -6.9848, -6.2935, -7.0770,\n",
       "           -5.7090, -5.8604, -6.9897, -6.2229, -6.8322, -6.3599, -6.4772, -6.5417,\n",
       "           -6.5364, -6.4144, -6.6997, -6.3333, -6.4570, -6.7890, -6.3524, -6.3225,\n",
       "           -5.9046, -6.3817, -5.7894, -6.2676, -5.8167, -6.7370, -6.7409, -7.1776,\n",
       "           -5.6081, -6.2513, -6.5601, -6.0649, -6.1692, -6.2282, -5.9269, -6.3920,\n",
       "           -5.9371, -6.2040, -6.5516, -6.8826, -6.7122, -5.9063, -5.2918, -6.5943,\n",
       "           -6.8725, -6.0507, -6.3486, -6.4035, -6.6715, -5.6501, -6.4985, -5.7732,\n",
       "           -6.6985, -5.9739, -6.4771, -6.5033, -6.0728, -6.1825, -6.1832, -6.1820,\n",
       "           -7.2883, -7.1754, -6.0405, -6.3509, -6.5296, -6.0821, -6.8098, -6.3714,\n",
       "           -6.1029, -6.0082, -6.5515, -6.2032, -5.3792, -5.9929, -6.4558, -6.4782,\n",
       "           -5.9793, -6.8567, -5.9863, -6.7326, -6.5506, -6.8252, -6.0874, -6.4303,\n",
       "           -5.6326, -6.3179, -6.5267, -6.2395, -7.0791, -5.8341, -6.3073, -6.9802,\n",
       "           -5.0656, -5.6640, -7.0429, -7.5127, -6.4768, -5.0162, -6.7206, -7.0121,\n",
       "           -6.8311, -6.5231, -7.2084, -6.2617, -6.7213, -6.5292, -7.2661, -6.4150,\n",
       "           -6.1500, -7.0927, -6.3680, -7.1489, -6.0372, -5.9287, -5.7838, -6.4486,\n",
       "           -6.7957, -6.9440, -6.2535, -6.3283, -7.0080, -6.4698, -5.6041, -7.1839,\n",
       "           -6.7297, -6.3902, -6.4910, -6.2024, -5.7628, -6.5043, -6.8037, -6.9616,\n",
       "           -6.5337, -6.2658, -5.8632, -6.8522, -6.0881, -6.4020, -6.9035, -6.7283,\n",
       "           -6.3800, -7.1497, -6.1793, -6.0774, -6.0904, -6.6583, -5.3120, -6.9008,\n",
       "           -6.7748, -5.8439, -6.5123, -6.5589, -6.9680, -6.7969, -6.1187, -5.8010,\n",
       "           -6.5484, -5.7692, -7.3311, -6.6631, -6.1763, -5.6258, -7.2292, -6.6790,\n",
       "           -6.1901, -6.3170, -6.5939, -6.3696, -5.3629, -5.7011, -6.1413, -5.8621,\n",
       "           -6.7412, -6.7333, -5.7522, -6.9028, -6.4299, -7.1003, -6.4659, -5.3450,\n",
       "           -6.4666, -6.6644, -6.7796, -6.3458, -7.0358, -5.3295, -6.1472, -6.4196,\n",
       "           -6.5685, -6.8529, -6.5807, -7.1355, -6.5515, -6.4600, -7.3619, -6.2691,\n",
       "           -7.0688, -5.8368, -5.4393, -6.3911, -5.7406, -6.4856, -6.3666, -7.0195,\n",
       "           -6.4778, -6.9107, -6.7201, -5.8173, -6.7587, -6.9675, -6.4332, -6.4572,\n",
       "           -6.0703, -6.5015, -6.2145, -6.5100, -5.4989, -6.1605, -6.4973, -6.6355,\n",
       "           -6.6168, -7.0843, -6.7471, -6.1133, -6.4386, -6.0582, -5.6439, -6.9790,\n",
       "           -5.1439, -6.8850, -6.0386, -6.1433, -6.7296, -6.5780, -6.9862, -6.9029,\n",
       "           -5.9099, -6.6180, -6.2938, -6.4042, -6.3522, -5.7797, -6.2856, -6.5307,\n",
       "           -6.3725, -5.5476, -6.8588, -6.5489, -5.6693, -6.7831, -6.5309, -6.5507,\n",
       "           -5.9711, -6.0128, -5.6742, -6.0639, -6.4213, -6.0705, -6.6610, -6.7431,\n",
       "           -6.3655, -6.9284, -6.2461, -6.0291, -5.4650, -6.4855, -5.6812, -6.5803,\n",
       "           -6.0483, -6.3333, -6.9692, -6.5166, -5.9708, -6.3641, -6.5787, -6.1535,\n",
       "           -6.4109, -6.9232, -6.2709, -5.8549, -6.2723, -6.0353, -6.7909, -7.2156,\n",
       "           -6.6450, -6.7182, -7.0833, -6.4045, -5.6490, -6.7584, -6.8831, -6.1887,\n",
       "           -5.3538, -6.3025, -6.4537, -6.1125, -6.3775, -6.1154, -6.2683, -7.3552,\n",
       "           -6.3311, -6.4782, -7.2894, -6.5045, -7.3275, -6.1498, -6.7847, -6.8108,\n",
       "           -6.6692, -7.3155, -6.1105, -6.3799, -5.5060, -6.5361, -6.1363, -6.8506,\n",
       "           -6.3374, -5.8202, -7.4229, -6.7921, -6.1137, -6.3794, -6.3786, -6.9683,\n",
       "           -6.1933, -5.5060, -6.2203, -5.9795, -6.4320, -6.4170, -7.1340, -6.3500,\n",
       "           -6.9285, -6.2311, -7.0855, -6.1452, -5.9419, -6.2121, -6.0511, -6.5603,\n",
       "           -5.8536, -7.1974, -6.7940, -6.5161, -5.8648, -6.3938, -6.3138, -6.6096,\n",
       "           -6.9748, -6.6163, -6.7844, -6.5824, -6.3049, -6.1851, -6.9251, -6.0726,\n",
       "           -6.5134, -6.3762, -6.1845, -6.7598, -6.5817, -6.0593, -6.2925, -6.5988,\n",
       "           -5.5005, -6.3597, -6.1898, -6.1300, -7.1211, -6.3423, -6.8625, -5.8134,\n",
       "           -7.1005, -6.6370, -6.7787, -6.5510, -8.0076, -6.6385, -6.6598, -6.2941,\n",
       "           -5.4613, -7.1200, -6.5916, -5.9628, -6.6684, -6.0795, -6.9287, -6.6751,\n",
       "           -6.5734, -6.6797, -5.8830, -6.0602, -6.4193, -6.0732, -6.2894, -6.6284,\n",
       "           -6.7165, -6.6457, -5.9085, -5.9738, -6.5143, -6.6726, -6.7938, -5.2257,\n",
       "           -5.7453, -6.7630, -5.5673, -7.2803, -6.2573, -6.9461, -6.9351, -6.6405,\n",
       "           -6.1422, -7.0069, -7.1347, -6.9864, -5.7884, -5.9185, -6.0182, -6.0625,\n",
       "           -6.9842, -6.2526, -6.3108, -6.1858, -6.5051, -6.6666, -6.8901, -6.4711,\n",
       "           -6.3870, -6.6575, -7.3610, -5.2940, -5.7238, -6.5883, -6.5916, -6.5819,\n",
       "           -6.3497, -6.0205, -6.8265, -6.9271, -6.7690, -7.2941, -6.6419, -6.1168,\n",
       "           -6.2545, -6.8802, -6.3496, -6.7692, -6.6883, -6.1096, -6.5414, -6.7677,\n",
       "           -6.2302, -6.4080, -6.3015, -6.1892, -6.4767, -6.2480, -5.8455, -6.1185,\n",
       "           -5.9801, -6.6543, -6.3807, -6.8084, -6.3389, -6.7854, -6.4461, -6.5937,\n",
       "           -5.8397, -6.1351, -7.6217, -6.9545, -6.0899, -6.5673, -6.7530, -7.0767,\n",
       "           -6.4345, -6.9043, -6.7145, -6.5672, -6.2052, -6.2794, -6.6330, -6.7207,\n",
       "           -6.6173, -6.2373, -5.9613, -6.0839, -6.5310, -6.5912, -5.5638, -6.3286,\n",
       "           -6.0293, -6.5303, -6.4118, -6.0361, -4.9817, -6.4707, -5.9046, -5.9925,\n",
       "           -6.4260, -6.5978, -6.3584, -6.6461, -6.7926, -6.0331, -6.4293, -6.9040,\n",
       "           -6.8222, -6.7261, -6.6421, -5.7168, -6.5245, -6.5623, -6.6839, -6.1325,\n",
       "           -6.8974, -5.4700, -6.7132, -6.3309, -6.2463, -6.7673, -6.4305, -6.0062,\n",
       "           -6.6765, -6.1547, -6.1229, -6.7996, -5.8085, -7.2874, -6.9510, -6.2395,\n",
       "           -6.1921, -6.5311, -6.7703, -6.9785, -6.6984, -6.7971, -5.6907, -6.0478,\n",
       "           -6.2346, -6.0089, -7.1222, -5.7508, -6.4731, -6.4856, -6.6149, -6.2788,\n",
       "           -6.2781, -6.5806, -6.8417, -6.5218, -6.6943, -6.4601, -6.1771, -6.6988,\n",
       "           -6.1676, -5.5371, -6.2148, -6.8867, -6.7724, -6.6151, -6.2219, -6.6831,\n",
       "           -6.3062, -6.2342, -6.4252, -6.7706, -5.6605, -5.8388, -6.3110, -6.3475,\n",
       "           -6.3636, -5.9257, -6.9915, -6.7026, -5.8096, -5.6675, -6.8994, -6.1282,\n",
       "           -6.4317, -6.5696, -6.6976, -7.0934, -6.2495, -6.9425, -6.6645, -6.5332,\n",
       "           -7.1017, -6.9547, -6.7508, -6.6721, -7.2678, -6.4264, -6.4922, -7.1781,\n",
       "           -6.8902, -6.6338, -7.5354, -7.0300, -5.7790, -7.0143, -5.7935, -6.0893,\n",
       "           -6.5944, -6.5464, -6.0816, -6.1041, -5.6774, -6.4744, -6.2211, -5.2856,\n",
       "           -6.1665, -6.4636, -6.4960, -6.7274, -6.2363, -6.0875, -6.8453, -6.9131,\n",
       "           -6.6297, -6.4980, -6.6343, -7.0528, -7.0341, -6.8551, -5.5492, -5.6040,\n",
       "           -5.5737, -6.5256, -5.9101, -5.9902, -5.9331, -5.8991, -6.4878, -6.0563,\n",
       "           -6.4823, -5.8865, -6.1808, -6.7093, -6.3187, -6.1496, -6.1879, -7.0760,\n",
       "           -5.7655, -6.4394, -6.8349, -6.6696, -5.9596, -6.6860, -6.4483, -6.6195,\n",
       "           -5.6470, -5.7266, -7.3329, -6.0654, -7.1101, -6.7455, -6.4329, -7.5019,\n",
       "           -6.2329, -6.3148, -6.0932, -5.0672, -5.2840, -6.6838, -6.5711, -7.1211,\n",
       "           -6.3494, -6.7715, -6.7265, -6.4408, -6.3754, -6.4147, -6.3398, -6.2831,\n",
       "           -6.5502, -5.8641, -6.5136, -6.3502, -6.4157, -6.7260, -6.3428, -6.6296,\n",
       "           -6.7696, -6.9788, -6.4203, -6.5101, -6.2583, -6.1176, -6.6382, -6.5771,\n",
       "           -6.5638, -6.7822, -6.8378, -5.8405, -6.4614, -6.8134, -5.6683, -6.3705,\n",
       "           -5.9461, -6.2911, -6.2126, -6.5795, -6.1524, -5.9014, -6.9484, -6.7696,\n",
       "           -6.5659, -6.5791, -6.1497, -5.4068, -6.2232, -6.5068, -6.1849, -6.3546,\n",
       "           -6.0909, -6.1272, -6.3116, -6.2078, -6.3537, -5.8588, -6.1742, -6.3980,\n",
       "           -5.9616, -7.3475, -5.9455, -6.0423, -6.5781, -5.8554, -6.2047, -5.7882,\n",
       "           -5.8337, -6.1073, -5.9387, -6.2726, -6.4594, -5.9652, -6.9020, -7.1689,\n",
       "           -6.4939, -6.6391, -7.1075, -5.5382, -6.0675, -6.4270, -6.6416, -6.8207,\n",
       "           -6.9686, -6.1602, -6.3000, -6.2956, -5.8869, -5.8110, -6.4270, -6.2205,\n",
       "           -6.0384, -6.8029, -7.1519, -6.0952, -6.5662, -6.7578, -6.5726, -6.3907,\n",
       "           -6.2695, -6.1985, -6.1734, -6.8167, -6.1312, -6.2967, -6.8833, -6.7134,\n",
       "           -6.7109, -6.6296, -5.1411, -6.2680, -7.3898, -6.9125, -6.9233, -5.6450,\n",
       "           -6.2731, -6.6384, -7.1274, -7.0603, -6.2623, -6.9120, -6.8329, -6.9949,\n",
       "           -6.2560, -6.2530, -6.6275, -6.7884, -7.2114, -6.5341, -6.9442, -6.2793,\n",
       "           -6.6252, -6.1507, -4.6161, -6.3871, -7.1365, -6.1496, -6.0111, -6.4930,\n",
       "           -6.8175, -6.6722, -6.4956, -6.5779, -5.6996, -5.9981, -6.8850, -6.4648,\n",
       "           -6.3947, -6.4700, -6.2210, -5.6463, -6.6714, -6.7305, -6.3790, -5.8037,\n",
       "           -6.3529, -6.3927, -6.3690, -6.8153, -6.7133, -6.9757, -7.1645, -6.0631,\n",
       "           -6.8738, -6.7630, -6.8005, -6.7490, -7.3210, -6.1281, -6.3103, -6.4792,\n",
       "           -5.9379, -5.6826, -6.4579, -6.4559, -6.3416, -6.1665, -6.3267, -6.7514,\n",
       "           -5.4164, -6.5654, -6.8090, -6.3088, -6.2451, -5.9202, -6.8736, -6.2947,\n",
       "           -6.7488, -7.0400, -6.4053, -6.3800, -6.8544, -6.6160, -6.5061, -6.7292,\n",
       "           -6.0373, -6.4183, -6.6124, -6.3146, -6.5705, -6.5272, -6.5967, -6.3289,\n",
       "           -5.6980, -6.5045, -7.1435, -6.7252, -5.9797, -6.5960, -5.8643, -6.5843,\n",
       "           -5.6777, -6.6450, -7.1168, -6.9865, -6.3567, -5.9603, -6.1668, -6.1736,\n",
       "           -6.5202, -6.0497, -5.1743, -6.5642, -5.7595, -6.2278, -5.9386, -6.4599,\n",
       "           -5.7990, -6.6355, -6.6027, -5.6999, -5.6944, -6.2326, -6.2573, -7.0076,\n",
       "           -6.4159, -6.0653, -5.8651, -6.5801, -6.6698, -6.0693, -6.5978, -6.4333,\n",
       "           -6.9513, -5.9783, -6.8113, -6.8523, -6.7654, -5.8421, -7.0492, -5.5056,\n",
       "           -6.5374, -6.4173, -6.3087, -6.3546, -5.9276, -6.2390, -6.2411, -6.8323,\n",
       "           -5.8442, -6.1546, -5.8692, -7.6196, -6.3265, -6.3470, -5.7598, -5.1638,\n",
       "           -7.1535, -6.4664, -7.1998, -6.3200, -6.5679, -6.5956, -6.4625, -6.3216,\n",
       "           -6.1964, -6.0730, -7.4404, -5.8887, -5.7670, -6.5140, -6.5711, -6.4566,\n",
       "           -6.7234, -5.9074, -7.0999, -6.3282, -7.0708, -6.1230, -7.1042, -7.1436,\n",
       "           -6.8728, -5.8848, -5.4492, -6.9028, -5.8628, -7.0339, -6.4901, -6.5940,\n",
       "           -6.1415, -6.0156, -6.3134, -6.5931, -6.2773, -7.0422, -7.1028, -5.5627,\n",
       "           -6.6217, -6.4378, -6.6479, -6.3143, -7.3918, -7.0670, -7.2828, -5.9559]),\n",
       "   'acceptance_rate': tensor(1.),\n",
       "   'penalty': None}],\n",
       " 'args': {'positional arguments': Namespace(),\n",
       "  'optional arguments': Namespace(help=None),\n",
       "  'Main args': Namespace(energy_module='MinGapEnergy', energy_threshold=-0.5, generator_module='FastSeqProp', max_attempts=40, monitor=None, n_proposals=[1000], params_module='StraightThroughParameters', penalty_module=None, proposal_path='/tmp/test__k562__fsp', reset_params=True, tolerate_unknown_args=False),\n",
       "  'Params Module args': Namespace(batch_dim=0, batch_size=256, cat_axis=-1, init_seqs=None, left_flank='GTACGGGAGGTATTGGACAGGCCGCAATAAAATATCTTTATTTTCATTACATCTGTGTGTTGGTTTTTTGTGTGAATCGATAGTACTAACATACGCTCTCCATCAAAACAAAACGAAACAAAACAAACTAGCAAAATAGGCTGTCCCCAGTGCAAGTGCAGGTGCCAGAACATTTCTCTGGCCTAACTGGCCGCTTGACG', length=200, n_channels=4, n_samples=10, right_flank='CACTGCGGCTCCTGCGATCTAACTGGCCGGTACCTGAGCTCGCTAGCCTCGAGGATATCAAGATCTGGCCTCGGCGGCCAAGCTTAGACACTAGAGGGTATATAATGGAAGCTCGACTTCCAGCTTGGCAATCCGGTACTGTTGGTAAAGCCACCATGGTGAGCAAGGGCGAGGAGCTGTTCACCGGGGTGGTGCCCATC', token_dim=-2, use_affine=False, use_norm=True),\n",
       "  'Energy Module args': Namespace(a_max=6.0, a_min=-2.0, bending_factor=1.0, model_artifact='/tmp/model_artifacts__20240216_162013__449584.tar.gz', target_alpha=1.0, target_feature=0),\n",
       "  'Generator Constructor args': Namespace(),\n",
       "  'Generator Runtime args': Namespace(learning_rate=0.5, lr_scheduler=True, n_steps=200, step_print=10)},\n",
       " 'timestamp': '20240216_162917',\n",
       " 'random_tag': 734583}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "fsp_props = !ls /tmp/\n",
    "fsp_props = [ p for p in fsp_props if 'test__k562__fsp' in p ][-1]\n",
    "torch.load(f'/tmp/{fsp_props}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28160bb-4aa0-407c-89e4-d18cec4b8612",
   "metadata": {},
   "source": [
    "### Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a021cf1-caa0-4171-b226-1861c6942548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive unpacked in /tmp/tmpv5iflges\n",
      "Loaded model from 20240216_162013 in eval mode\n",
      "Starting round: 0, generate 1000 proposals\n",
      "collect samples\n",
      "  0%|                                                  | 0/2000 [00:00<?, ?it/s]Penalty not implemented\n",
      "100%|███████████████████████████████████████| 2000/2000 [00:34<00:00, 58.01it/s]\n",
      "attempt 1 acceptance rate: 256/256\n",
      "collect samples\n",
      "100%|███████████████████████████████████████| 2000/2000 [00:31<00:00, 63.12it/s]\n",
      "attempt 2 acceptance rate: 256/256\n",
      "collect samples\n",
      "100%|███████████████████████████████████████| 2000/2000 [00:31<00:00, 62.88it/s]\n",
      "attempt 3 acceptance rate: 256/256\n",
      "collect samples\n",
      "100%|███████████████████████████████████████| 2000/2000 [00:31<00:00, 63.17it/s]\n",
      "attempt 4 acceptance rate: 256/256\n",
      "finished round\n",
      "Proposals deposited at:\n",
      "\t/tmp/test__k562__sa__20240216_163141__730829.pt\n"
     ]
    }
   ],
   "source": [
    "!python /home/ubuntu/boda2/src/generate.py \\\n",
    "    --params_module BasicParameters \\\n",
    "        --batch_size 256 --n_channels 4 \\\n",
    "        --length 200 \\\n",
    "    --energy_module MinGapEnergy \\\n",
    "        --target_feature 0 --bending_factor 0.0 --a_min -2.0 --a_max 6.0 \\\n",
    "        --model_artifact /tmp/{model_artifact} \\\n",
    "    --generator_module SimulatedAnnealing \\\n",
    "         --n_steps 2000 --n_positions 5 \\\n",
    "         --a 1.0 --b 1.0 --gamma 0.501 \\\n",
    "    --energy_threshold -0.5 --max_attempts 40 \\\n",
    "    --n_proposals 1000 \\\n",
    "    --proposal_path /tmp/test__k562__sa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd4a0b4f-4a19-4cb1-9d3b-012c7dd6a04d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proposals': [{'proposals': tensor([[[0., 1., 0.,  ..., 0., 0., 1.],\n",
       "            [0., 0., 0.,  ..., 1., 1., 0.],\n",
       "            [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 1.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "            [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 1., 1.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [1., 1., 0.,  ..., 0., 0., 1.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 1.,  ..., 1., 0., 0.]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[0., 0., 1.,  ..., 0., 0., 1.],\n",
       "            [0., 1., 0.,  ..., 1., 0., 0.],\n",
       "            [1., 0., 0.,  ..., 0., 1., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 1., 1.],\n",
       "            [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "            [1., 0., 0.,  ..., 1., 0., 0.],\n",
       "            [0., 1., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "            [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "            [1., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       "   'energies': tensor([-6.1278, -6.1035, -6.1154, -6.2683, -6.0488, -6.2889, -6.2745, -6.3026,\n",
       "           -6.3886, -6.2961, -6.3113, -6.4157, -6.1464, -6.0970, -6.4035, -6.3214,\n",
       "           -5.7265, -6.3854, -6.3792, -6.3514, -6.1014, -6.3787, -6.1732, -6.1601,\n",
       "           -5.8453, -6.1864, -6.0847, -5.9740, -5.8010, -6.2156, -6.0179, -6.2829,\n",
       "           -6.1555, -6.3711, -6.4322, -6.1931, -6.2620, -6.4330, -6.1422, -6.1316,\n",
       "           -6.2156, -6.1214, -6.2730, -5.9668, -6.1578, -6.1617, -5.9264, -5.9230,\n",
       "           -6.4582, -6.0452, -6.3199, -6.0415, -5.9808, -6.2522, -6.3345, -6.4339,\n",
       "           -6.1609, -6.1356, -6.1520, -6.0181, -5.8403, -6.2809, -6.0093, -6.2147,\n",
       "           -6.0442, -6.1900, -5.9923, -6.1850, -6.2043, -5.9503, -6.1001, -6.0353,\n",
       "           -6.2957, -6.2289, -6.2375, -6.2615, -6.0779, -6.2991, -6.2564, -6.1615,\n",
       "           -6.1044, -6.3214, -6.4109, -6.3000, -5.9321, -6.1872, -6.3407, -6.1198,\n",
       "           -6.1804, -5.9671, -6.4293, -6.2376, -6.3469, -6.2123, -5.9693, -6.0808,\n",
       "           -6.1980, -6.2552, -6.3938, -6.3209, -6.4076, -6.4285, -6.1747, -6.2679,\n",
       "           -6.1591, -6.3920, -6.0832, -6.0427, -6.1978, -6.2645, -5.9471, -6.5083,\n",
       "           -6.1986, -6.4373, -6.2332, -6.1030, -6.1798, -6.2539, -6.4387, -6.1852,\n",
       "           -6.0805, -6.3178, -6.2634, -5.9705, -6.0117, -6.0816, -5.8200, -6.2336,\n",
       "           -6.1910, -6.0670, -5.9838, -6.1008, -5.8283, -5.9835, -5.8671, -6.1184,\n",
       "           -6.4041, -6.2644, -5.9412, -6.2013, -5.9050, -5.9566, -6.2452, -6.0444,\n",
       "           -6.3466, -6.1326, -6.2080, -6.3527, -6.1212, -6.4149, -6.1992, -6.2316,\n",
       "           -6.1856, -6.2075, -6.4263, -6.0593, -6.5730, -5.8788, -6.2804, -6.0818,\n",
       "           -6.1652, -5.6409, -6.4735, -5.8836, -6.3250, -6.1224, -6.2531, -6.3753,\n",
       "           -5.9154, -6.1408, -6.4199, -5.9643, -5.9430, -6.1811, -5.9747, -5.8667,\n",
       "           -6.2141, -6.0480, -6.3998, -6.0359, -6.0628, -6.0079, -6.3184, -5.8654,\n",
       "           -6.1988, -6.2596, -6.2870, -6.3669, -6.2624, -6.2639, -6.3408, -6.0585,\n",
       "           -6.5566, -6.3352, -5.9639, -6.1126, -5.9913, -6.3152, -6.0539, -6.2206,\n",
       "           -6.3201, -6.1302, -6.3459, -6.0793, -6.2149, -5.9580, -5.9081, -6.2051,\n",
       "           -6.4406, -6.2213, -6.2935, -5.8170, -6.2809, -6.0256, -6.1285, -6.2700,\n",
       "           -6.1567, -6.1877, -6.2660, -6.1185, -6.2327, -5.6785, -5.6578, -6.2814,\n",
       "           -6.3194, -5.8732, -6.1179, -6.1908, -6.1778, -6.1641, -5.9623, -6.0948,\n",
       "           -6.2675, -6.4680, -5.9304, -6.0720, -6.2819, -6.2259, -6.0187, -6.2288,\n",
       "           -6.3115, -6.1581, -6.1957, -6.1503, -6.3085, -6.1726, -6.1989, -5.9102,\n",
       "           -6.1124, -5.9506, -6.1318, -6.1754, -6.1653, -5.9916, -6.0975, -6.0944,\n",
       "           -6.0371, -6.1784, -6.2204, -6.1800, -6.0912, -5.8570, -6.3098, -6.0060,\n",
       "           -6.4444, -6.5313, -6.3169, -6.0070, -6.4484, -6.1560, -6.3640, -6.2376,\n",
       "           -6.4209, -6.0207, -6.4682, -6.1427, -6.1378, -6.3637, -6.1016, -6.2877,\n",
       "           -5.9146, -6.3491, -6.0748, -6.2402, -5.8802, -6.2244, -6.1594, -5.7176,\n",
       "           -6.2627, -5.8313, -6.1677, -6.1773, -6.2486, -5.6903, -6.3743, -6.2333,\n",
       "           -6.4067, -6.1342, -6.3859, -6.2814, -6.1335, -6.4157, -6.1359, -6.2698,\n",
       "           -6.3585, -6.3404, -6.3926, -5.9976, -6.4580, -5.7750, -6.2898, -6.3927,\n",
       "           -6.2085, -6.0181, -6.2843, -6.1500, -6.1135, -6.0855, -6.0277, -6.2464,\n",
       "           -6.2034, -6.2916, -6.0854, -6.1148, -6.0693, -5.8286, -6.1132, -6.0738,\n",
       "           -6.0736, -5.9439, -6.0081, -6.3304, -5.6696, -6.3943, -6.1944, -6.1982,\n",
       "           -6.1570, -6.0182, -6.1644, -6.1065, -6.1801, -6.1724, -6.2197, -6.2488,\n",
       "           -6.2065, -6.3885, -6.0766, -6.1818, -6.4811, -6.2467, -6.2511, -5.9872,\n",
       "           -5.8001, -6.2809, -6.3999, -6.2824, -6.2954, -6.1482, -5.8116, -6.2737,\n",
       "           -6.0625, -6.2872, -6.3048, -6.0012, -6.3254, -6.1445, -6.2581, -6.4610,\n",
       "           -6.0858, -6.1541, -6.4106, -6.2698, -6.2817, -5.8079, -6.1986, -6.1618,\n",
       "           -6.1027, -6.1449, -6.4302, -6.2977, -6.1356, -5.7810, -6.3962, -5.8927,\n",
       "           -5.8415, -6.2957, -6.2911, -6.1046, -6.2929, -6.1700, -6.2328, -6.3919,\n",
       "           -6.0959, -5.6577, -6.0166, -6.2806, -6.2691, -5.9843, -6.3801, -6.4216,\n",
       "           -5.9362, -6.3125, -6.3687, -6.3001, -5.6485, -6.2009, -6.0995, -6.4294,\n",
       "           -5.9322, -6.3844, -6.2092, -6.3467, -6.2552, -5.7942, -6.0033, -6.2301,\n",
       "           -5.9327, -6.3195, -6.4668, -6.2183, -6.1268, -6.2397, -6.1043, -6.1611,\n",
       "           -6.1630, -6.4217, -5.9274, -6.4313, -6.1511, -6.0949, -6.0950, -5.9772,\n",
       "           -6.1867, -6.2725, -6.0924, -6.3518, -6.2865, -6.1503, -6.0206, -6.3010,\n",
       "           -6.1234, -6.0456, -6.2121, -6.3227, -6.3207, -6.2925, -6.1931, -6.1460,\n",
       "           -6.0974, -5.9198, -6.1193, -6.1056, -5.8497, -6.1373, -6.0728, -6.1277,\n",
       "           -6.5236, -6.0754, -6.2061, -6.2131, -6.2946, -6.0983, -6.4272, -6.1679,\n",
       "           -6.2918, -6.0447, -6.3890, -6.3638, -6.0267, -6.2046, -6.1257, -6.5393,\n",
       "           -6.2656, -6.3287, -6.1475, -5.6824, -6.2603, -6.0180, -6.1447, -5.9795,\n",
       "           -6.3295, -6.3552, -6.0751, -6.1767, -5.9954, -6.0330, -6.1598, -6.1596,\n",
       "           -6.2223, -5.9431, -6.1529, -6.0981, -6.3203, -6.2308, -6.3538, -6.0392,\n",
       "           -5.8722, -6.2121, -6.2627, -5.9447, -6.4145, -6.0685, -6.1808, -6.3504,\n",
       "           -6.2982, -6.2769, -6.2603, -6.0162, -6.1392, -6.3192, -6.1836, -6.4098,\n",
       "           -6.0915, -6.4295, -6.4366, -6.2082, -6.3929, -6.3734, -6.3273, -6.1035,\n",
       "           -6.4059, -6.2091, -6.1254, -6.3692, -6.3908, -6.2746, -6.3263, -5.4846,\n",
       "           -6.2812, -6.1002, -6.2093, -6.1611, -6.0254, -6.1809, -6.2086, -6.2961,\n",
       "           -6.5094, -6.0587, -6.2592, -6.4374, -6.0176, -5.7183, -6.0368, -6.1628,\n",
       "           -6.1588, -6.0522, -6.3373, -5.9544, -5.8426, -6.1036, -6.3073, -6.3268,\n",
       "           -6.4496, -5.7298, -6.2419, -6.2042, -5.9725, -6.0557, -6.3006, -6.3874,\n",
       "           -6.3431, -6.4010, -6.3825, -6.2507, -6.2665, -6.2644, -6.3352, -6.0737,\n",
       "           -5.9081, -6.4647, -6.1085, -6.1337, -6.1058, -5.7984, -5.9275, -6.2510,\n",
       "           -6.2773, -6.1644, -6.1780, -6.1244, -5.8579, -6.1403, -6.2068, -6.1591,\n",
       "           -6.1599, -6.0195, -6.4007, -6.2687, -6.4988, -5.9200, -6.2041, -6.2632,\n",
       "           -6.3096, -6.2525, -5.7966, -5.9865, -6.1493, -6.3944, -5.9691, -6.0424,\n",
       "           -6.1301, -6.2541, -6.1813, -6.2689, -6.0231, -6.1137, -5.9447, -6.0839,\n",
       "           -6.2945, -6.0770, -6.1830, -6.3056, -5.9702, -6.3344, -6.6173, -6.2296,\n",
       "           -6.2785, -6.3970, -6.1482, -6.1104, -6.0989, -6.4261, -6.3516, -6.0419,\n",
       "           -6.3923, -5.7606, -6.3295, -6.4216, -6.3003, -6.1024, -5.9539, -6.2891,\n",
       "           -6.3062, -6.2919, -6.0867, -6.4316, -6.2442, -6.1951, -6.4157, -6.1888,\n",
       "           -6.1489, -6.2706, -6.2426, -6.3974, -5.8636, -6.4174, -6.3082, -6.2347,\n",
       "           -6.2803, -6.4102, -6.0392, -6.3445, -6.4080, -6.4078, -6.4105, -6.3077,\n",
       "           -5.8729, -6.1365, -6.6097, -6.3292, -6.0915, -6.0916, -6.4261, -6.3139,\n",
       "           -5.9253, -6.1304, -6.1021, -6.3716, -5.9729, -6.3226, -6.0551, -6.3109,\n",
       "           -6.1317, -5.7375, -5.8769, -6.1957, -6.0247, -6.1778, -6.2181, -6.3390,\n",
       "           -6.2444, -6.0426, -5.9604, -6.1253, -6.2443, -6.0787, -6.4161, -6.1159,\n",
       "           -6.1875, -5.9981, -6.2138, -6.1396, -6.1045, -6.2408, -5.9651, -6.2406,\n",
       "           -6.0609, -6.2688, -6.1656, -6.2354, -6.0785, -6.0666, -6.2677, -6.3220,\n",
       "           -6.0957, -6.1743, -6.0622, -6.3088, -5.9744, -6.2461, -6.1655, -6.3004,\n",
       "           -6.1193, -5.7585, -6.0720, -6.2770, -6.1620, -6.3458, -6.0304, -6.1224,\n",
       "           -6.1692, -6.2583, -6.1199, -6.2064, -6.3695, -5.9705, -6.1016, -6.0826,\n",
       "           -6.2306, -6.3421, -6.0761, -6.2127, -6.0610, -6.0581, -6.1344, -6.2409,\n",
       "           -6.1519, -6.3579, -6.4154, -6.3519, -5.8879, -6.3003, -5.9800, -6.4198,\n",
       "           -6.3007, -6.0541, -6.1519, -6.2171, -6.2818, -6.2726, -6.1507, -6.1108,\n",
       "           -6.3608, -6.0702, -6.1845, -5.9013, -6.2538, -6.2089, -6.2191, -6.0577,\n",
       "           -6.3806, -6.0977, -6.3502, -6.1623, -6.1891, -5.7422, -6.2503, -6.2100,\n",
       "           -5.9439, -5.8746, -5.9304, -6.0807, -6.4291, -6.3056, -6.0728, -6.1546,\n",
       "           -6.3705, -6.1955, -6.1362, -5.9526, -6.2460, -6.3316, -6.0728, -6.0394,\n",
       "           -6.1808, -6.4199, -6.3712, -6.4804, -5.9948, -6.1527, -6.4664, -6.3604,\n",
       "           -6.1658, -6.3932, -6.3749, -6.1523, -6.2047, -6.0722, -6.2883, -5.9753,\n",
       "           -6.2576, -6.0648, -6.3003, -6.2095, -6.2939, -6.3435, -6.3799, -6.0334,\n",
       "           -6.2393, -6.2283, -6.1728, -6.3173, -6.2419, -6.2603, -6.4345, -5.9356,\n",
       "           -6.1463, -6.3173, -6.4183, -6.2689, -6.5084, -6.3514, -6.1811, -6.2267,\n",
       "           -6.0512, -6.3371, -6.2612, -5.9889, -6.4195, -6.2146, -5.8639, -6.2972,\n",
       "           -6.2794, -6.2459, -6.1271, -6.2104, -6.3552, -6.2717, -6.4082, -6.1518,\n",
       "           -5.8279, -6.3350, -6.2562, -6.0312, -6.3630, -6.4502, -5.9370, -6.2634,\n",
       "           -5.9533, -6.1238, -6.2833, -6.5290, -6.3504, -6.0642, -5.9052, -6.0863,\n",
       "           -6.0432, -6.2414, -5.9143, -6.3707, -6.1480, -5.6439, -6.4082, -6.0066,\n",
       "           -6.2397, -6.3727, -6.1905, -6.1787, -6.0653, -6.0589, -6.0362, -5.9338,\n",
       "           -6.2734, -6.0044, -6.1421, -6.0453, -6.3080, -6.3413, -6.1729, -6.3284,\n",
       "           -5.9284, -6.1599, -6.0431, -6.0367, -6.0995, -6.3546, -6.2916, -6.3081,\n",
       "           -6.2255, -6.1727, -5.8852, -6.3111, -5.9058, -6.3154, -6.0988, -5.9233,\n",
       "           -6.0464, -6.2409, -6.3495, -6.1171, -6.2676, -6.1331, -6.0178, -6.1694,\n",
       "           -6.0962, -5.9115, -5.9659, -6.2338, -6.1628, -6.1911, -6.2684, -6.2609,\n",
       "           -5.7817, -6.1755, -6.1451, -6.1159, -6.0926, -6.0889, -6.1916, -6.3279,\n",
       "           -6.3955, -6.0931, -6.2226, -6.1157, -6.0994, -6.2066, -5.9912, -6.0384,\n",
       "           -5.8692, -6.3861, -6.0922, -5.9374, -6.2814, -6.2051, -6.2966, -6.1264,\n",
       "           -6.1423, -6.2432, -5.8259, -6.2274, -6.3086, -6.1105, -6.4328, -6.1164,\n",
       "           -6.3574, -6.2272, -6.3243, -6.2860, -6.0650, -6.1861, -6.2652, -6.1551,\n",
       "           -6.1984, -6.3542, -6.2251, -6.1021, -6.1497, -6.2475, -6.1149, -6.1144,\n",
       "           -5.9278, -6.3913, -6.0947, -6.0728, -6.3010, -6.0964, -6.0304, -6.4151,\n",
       "           -6.1540, -6.2379, -6.3525, -6.3653, -6.1028, -6.3728, -5.9891, -6.3038,\n",
       "           -6.3468, -5.8621, -5.6320, -6.0923, -6.2537, -6.2823, -6.2135, -6.3476,\n",
       "           -6.3523, -6.3680, -6.1111, -6.1036, -6.4244, -6.2895, -6.0339, -5.8914,\n",
       "           -6.0468, -6.0508, -5.9032, -6.3991, -5.8493, -6.0441, -6.2422, -6.4582]),\n",
       "   'acceptance_rate': tensor([]),\n",
       "   'penalty': None}],\n",
       " 'args': {'positional arguments': Namespace(),\n",
       "  'optional arguments': Namespace(help=None),\n",
       "  'Main args': Namespace(energy_module='MinGapEnergy', energy_threshold=-0.5, generator_module='SimulatedAnnealing', max_attempts=40, monitor=None, n_proposals=[1000], params_module='BasicParameters', penalty_module=None, proposal_path='/tmp/test__k562__sa', reset_params=True, tolerate_unknown_args=False),\n",
       "  'Params Module args': Namespace(batch_dim=0, batch_size=256, cat_axis=-1, init_seqs=None, left_flank='GTACGGGAGGTATTGGACAGGCCGCAATAAAATATCTTTATTTTCATTACATCTGTGTGTTGGTTTTTTGTGTGAATCGATAGTACTAACATACGCTCTCCATCAAAACAAAACGAAACAAAACAAACTAGCAAAATAGGCTGTCCCCAGTGCAAGTGCAGGTGCCAGAACATTTCTCTGGCCTAACTGGCCGCTTGACG', length=200, n_channels=4, right_flank='CACTGCGGCTCCTGCGATCTAACTGGCCGGTACCTGAGCTCGCTAGCCTCGAGGATATCAAGATCTGGCCTCGGCGGCCAAGCTTAGACACTAGAGGGTATATAATGGAAGCTCGACTTCCAGCTTGGCAATCCGGTACTGTTGGTAAAGCCACCATGGTGAGCAAGGGCGAGGAGCTGTTCACCGGGGTGGTGCCCATC', token_dim=-2),\n",
       "  'Energy Module args': Namespace(a_max=6.0, a_min=-2.0, bending_factor=0.0, model_artifact='/tmp/model_artifacts__20240216_162013__449584.tar.gz', target_alpha=1.0, target_feature=0),\n",
       "  'Generator Constructor args': Namespace(a=1.0, b=1.0, gamma=0.501, n_positions=5),\n",
       "  'Generator Runtime args': Namespace(keep_burnin=False, n_burnin=0, n_steps=2000)},\n",
       " 'timestamp': '20240216_163141',\n",
       " 'random_tag': 730829}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "sa_props = !ls /tmp/\n",
    "sa_props = [ p for p in sa_props if 'test__k562__sa' in p ][-1]\n",
    "torch.load(f'/tmp/{sa_props}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
