{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec118925-12e7-4808-9e5f-4c7c0a875812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning.pytorch as ptl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import boda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c19de6-c871-4378-8144-f6580c3850c2",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca67283-514e-40a6-a673-2f15665d61b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pick modules\n",
    "Pick modules to define:\n",
    "1. The data, how it's preprocessed and train/val/test split\n",
    "2. The model, the architecture setup, loss function, etc.\n",
    "3. The graph, how the data is used to train the model (i.e. training loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd28d039-763d-4e2f-a89e-8246d1d96dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = boda.data.SeqDataModule\n",
    "model_module= boda.model.BassetBranched\n",
    "graph_module= boda.graph.CNNBasicTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914404be-3b63-4930-898a-7bc91c8bbf9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dummy dataset generation for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b4d496-8f80-4ec1-a0d2-4ef769f6003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy train with 200 sequences saved to 'dummy_train.tsv'.\n",
      "Dummy test with 200 sequences saved to 'dummy_test.tsv'.\n",
      "Dummy val with 200 sequences saved to 'dummy_val.tsv'.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Function to generate random DNA sequence\n",
    "def generate_dna_sequence(length):\n",
    "    return ''.join(random.choice('ACGT') for _ in range(length))\n",
    "\n",
    "# Function to generate fake numerical score\n",
    "def generate_numerical_score():\n",
    "    return random.uniform(-10, 10)\n",
    "\n",
    "# Number of sequences in the dataset\n",
    "num_sequences = 200\n",
    "\n",
    "# Length of DNA sequences\n",
    "sequence_length = 200\n",
    "\n",
    "header = [\"Sequence\", \"Random/Fake Score\"]  # Define the header\n",
    "\n",
    "## TRAIN\n",
    "# Generating dummy dataset\n",
    "dummy_train = []\n",
    "for _ in range(num_sequences):\n",
    "    sequence = generate_dna_sequence(sequence_length)\n",
    "    score = generate_numerical_score()\n",
    "    dummy_train.append((sequence, score))\n",
    "\n",
    "traintsv_file = \"dummy_train.tsv\"\n",
    "with open(traintsv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter='\\t')\n",
    "    writer.writerow(header)  # Write the header row\n",
    "    for sequence, score in dummy_train:\n",
    "        writer.writerow([sequence, score])\n",
    "\n",
    "## TEST\n",
    "# Generating dummy dataset\n",
    "dummy_test = []\n",
    "for _ in range(num_sequences):\n",
    "    sequence = generate_dna_sequence(sequence_length)\n",
    "    score = generate_numerical_score()\n",
    "    dummy_test.append((sequence, score))\n",
    "\n",
    "testtsv_file = \"dummy_test.tsv\"\n",
    "with open(testtsv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter='\\t')\n",
    "    writer.writerow(header)  # Write the header row\n",
    "    for sequence, score in dummy_test:\n",
    "        writer.writerow([sequence, score])\n",
    "\n",
    "## VALIDATE\n",
    "# Generating dummy dataset\n",
    "dummy_val = []\n",
    "for _ in range(num_sequences):\n",
    "    sequence = generate_dna_sequence(sequence_length)\n",
    "    score = generate_numerical_score()\n",
    "    dummy_val.append((sequence, score))\n",
    "\n",
    "valtsv_file = \"dummy_val.tsv\"\n",
    "with open(valtsv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter='\\t')\n",
    "    writer.writerow(header)  # Write the header row\n",
    "    for sequence, score in dummy_val:\n",
    "        writer.writerow([sequence, score])\n",
    "\n",
    "print(f\"Dummy train with {num_sequences} sequences saved to '{traintsv_file}'.\")\n",
    "print(f\"Dummy test with {num_sequences} sequences saved to '{testtsv_file}'.\")\n",
    "print(f\"Dummy val with {num_sequences} sequences saved to '{valtsv_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae98f7-b611-4502-96af-437b481457c8",
   "metadata": {},
   "source": [
    "## Initalize Data and Model\n",
    "I added chr1 to test and chr2 to val to speed up this example. I also removed the reverse complmentat data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "624f6e3b-f87a-43eb-a9e5-be5d735cba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_module(\n",
    "    train_file = \"/home/ubuntu/boda2/analysis/AR001__rotation/dummy_train.tsv\",\n",
    "    test_file = \"/home/ubuntu/boda2/analysis/AR001__rotation/dummy_test.tsv\",\n",
    "    val_file = \"/home/ubuntu/boda2/analysis/AR001__rotation/dummy_val.tsv\",\n",
    "    right_flank = boda.common.constants.MPRA_DOWNSTREAM[:200],\n",
    "    left_flank = boda.common.constants.MPRA_UPSTREAM[-200:],\n",
    "    use_revcomp = True,\n",
    "    skip_header=True\n",
    ")\n",
    "\n",
    "model = model_module(\n",
    "    n_outputs=2, \n",
    "    n_linear_layers=1, linear_channels=1000,\n",
    "    linear_activation='ReLU', linear_dropout_p=0.12, \n",
    "    n_branched_layers=3, branched_channels=140, \n",
    "    branched_activation='ReLU', branched_dropout_p=0.56, \n",
    "    loss_criterion='L1KLmixed', kl_scale=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893e23c-e199-4785-99fc-fe9931585968",
   "metadata": {},
   "source": [
    "## Append Graph to Model\n",
    "Augment the model class to append functions from the graph module. A downside to this structure is that you need to make sure all relevent Graph args are defined (even if None is an acceptable default). This is because the `__init__` block in the Graph class doesn't run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ebbdf8-c962-4287-b460-c9da88e93114",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_args = {\n",
    "    'optimizer': 'Adam', \n",
    "    'optimizer_args': {\n",
    "        'lr': 0.0033, 'betas':[0.9, 0.999], \n",
    "        'weight_decay': 3.43e-4, 'amsgrad': True\n",
    "    },\n",
    "    'scheduler': 'CosineAnnealingWarmRestarts', \n",
    "    'scheduler_monitor': None, \n",
    "    'scheduler_interval': 'step',\n",
    "    'scheduler_args': {\n",
    "        'T_0': 4096,\n",
    "    }\n",
    "}\n",
    "\n",
    "model.__class__ = type(\n",
    "    'BODA_module',\n",
    "    (model_module,graph_module),\n",
    "    graph_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cad28b57-8f50-4389-8a84-e646c07792ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CNNBasicTraining.training_step of CNNBasicTraining()>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = graph_module(**graph_args)\n",
    "graph.training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "960cce4f-5f60-40ce-a9f6-db792bb05a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CNNBasicTraining.training_step of BODA_module(\n",
       "  (pad1): ConstantPad1d(padding=(9, 9), value=0.0)\n",
       "  (conv1): Conv1dNorm(\n",
       "    (conv): Conv1d(4, 300, kernel_size=(19,), stride=(1,))\n",
       "    (bn_layer): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (pad2): ConstantPad1d(padding=(5, 5), value=0.0)\n",
       "  (conv2): Conv1dNorm(\n",
       "    (conv): Conv1d(300, 200, kernel_size=(11,), stride=(1,))\n",
       "    (bn_layer): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (pad3): ConstantPad1d(padding=(3, 3), value=0.0)\n",
       "  (conv3): Conv1dNorm(\n",
       "    (conv): Conv1d(200, 200, kernel_size=(7,), stride=(1,))\n",
       "    (bn_layer): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (pad4): ConstantPad1d(padding=(1, 1), value=0.0)\n",
       "  (maxpool_3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (maxpool_4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear1): LinearNorm(\n",
       "    (linear): Linear(in_features=2600, out_features=1000, bias=True)\n",
       "    (bn_layer): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (branched): BranchedLinear(\n",
       "    (nonlin): ReLU()\n",
       "    (dropout): Dropout(p=0.56, inplace=False)\n",
       "    (intake): RepeatLayer()\n",
       "    (branched_layer_1): GroupedLinear()\n",
       "    (branched_layer_2): GroupedLinear()\n",
       "    (branched_layer_3): GroupedLinear()\n",
       "  )\n",
       "  (output): GroupedLinear()\n",
       "  (nonlin): ReLU()\n",
       "  (dropout): Dropout(p=0.12, inplace=False)\n",
       "  (criterion): L1KLmixed(\n",
       "    (MSE): L1Loss()\n",
       "    (KL): KLDivLoss()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e22d5c9c-9f8d-4361-a190-85444aa6c492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efdf630f-7581-4135-a4e9-24cdc433ff0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0712,  0.0120],\n",
       "        [-0.0711,  0.0116],\n",
       "        [-0.0715,  0.0123],\n",
       "        [-0.0714,  0.0116],\n",
       "        [-0.0713,  0.0117],\n",
       "        [-0.0713,  0.0120],\n",
       "        [-0.0711,  0.0118],\n",
       "        [-0.0710,  0.0117],\n",
       "        [-0.0713,  0.0117],\n",
       "        [-0.0716,  0.0119]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(10,4,600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d239baf3-528a-40f6-9fd1-53b5082d3989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/boda2/boda/data/__init__.py'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boda.data.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90002f10-7e19-43d0-a065-af48622cc594",
   "metadata": {},
   "source": [
    "## Lightning trainer\n",
    "Normally we train for more epochs, but reduced in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eeaabe9-cd76-4c96-b27d-c9d7e7774f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1, \n",
    "    monitor='prediction_mean_spearman', \n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "stopping_callback = EarlyStopping(\n",
    "    monitor='prediction_mean_spearman', \n",
    "    patience=5,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "trainer = ptl.Trainer(\n",
    "    accelerator='gpu', devices=1, \n",
    "    min_epochs=5, max_epochs=20, \n",
    "    precision=16, callbacks= [\n",
    "        checkpoint_callback,\n",
    "        stopping_callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed051228-22c9-428f-a50e-6cadaecf5d66",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13d673c3-38bf-47a6-82dd-79683e8198b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type           | Params\n",
      "----------------------------------------------\n",
      "0  | pad1      | ConstantPad1d  | 0     \n",
      "1  | conv1     | Conv1dNorm     | 23.7 K\n",
      "2  | pad2      | ConstantPad1d  | 0     \n",
      "3  | conv2     | Conv1dNorm     | 660 K \n",
      "4  | pad3      | ConstantPad1d  | 0     \n",
      "5  | conv3     | Conv1dNorm     | 280 K \n",
      "6  | pad4      | ConstantPad1d  | 0     \n",
      "7  | maxpool_3 | MaxPool1d      | 0     \n",
      "8  | maxpool_4 | MaxPool1d      | 0     \n",
      "9  | linear1   | LinearNorm     | 2.6 M \n",
      "10 | branched  | BranchedLinear | 359 K \n",
      "11 | output    | GroupedLinear  | 282   \n",
      "12 | nonlin    | ReLU           | 0     \n",
      "13 | dropout   | Dropout        | 0     \n",
      "14 | criterion | L1KLmixed      | 0     \n",
      "----------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "7.855     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3927422 parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | arithmetic_mean_loss: 9.63547 | harmonic_mean_loss: 45.93113 | prediction_mean_spearman: 0.29549 | entropy_spearman: 0.55940 |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2917: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "/opt/conda/lib/python3.7/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:236: UserWarning: You called `self.log('current_epoch', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
      "/opt/conda/lib/python3.7/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/lightning/pytorch/trainer/trainer.py:1613: PossibleUserWarning: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f52d8b8fdc64154a39f527675ea55aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | arithmetic_mean_loss: 8.53853 | harmonic_mean_loss: 34.11116 | prediction_mean_spearman: -0.02668 | entropy_spearman: 0.01304 |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 1.00000 | arithmetic_mean_loss: 8.51866 | harmonic_mean_loss: 33.75887 | prediction_mean_spearman: 0.00712 | entropy_spearman: -0.00428 |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 2.00000 | arithmetic_mean_loss: 8.52140 | harmonic_mean_loss: 33.79566 | prediction_mean_spearman: 0.01159 | entropy_spearman: 0.02561 |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 3.00000 | arithmetic_mean_loss: 8.52013 | harmonic_mean_loss: 33.81696 | prediction_mean_spearman: -0.03085 | entropy_spearman: 0.08156 |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 4.00000 | arithmetic_mean_loss: 8.52349 | harmonic_mean_loss: 33.87461 | prediction_mean_spearman: -0.00494 | entropy_spearman: -0.04054 |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 5.00000 | arithmetic_mean_loss: 8.52471 | harmonic_mean_loss: 33.89266 | prediction_mean_spearman: -0.00451 | entropy_spearman: 0.05743 |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 6.00000 | arithmetic_mean_loss: 8.52635 | harmonic_mean_loss: 33.91988 | prediction_mean_spearman: -0.00711 | entropy_spearman: 0.01276 |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 7.00000 | arithmetic_mean_loss: 8.52860 | harmonic_mean_loss: 33.95908 | prediction_mean_spearman: -0.01607 | entropy_spearman: 0.20422 |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1bf75a6-1237-410f-9da0-7650c58cf327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best model stashed at: /home/ubuntu/boda2/analysis/AR001__rotation/lightning_logs/version_5/checkpoints/epoch=2-step=120.ckpt\n",
      "Exists: True\n",
      "Setting model from epoch: 2\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def set_best(my_model, callbacks):\n",
    "    \"\"\"\n",
    "    Set the best model checkpoint for the provided model.\n",
    "\n",
    "    This function sets the state of the provided model to the state of the best checkpoint,\n",
    "    as determined by the `ModelCheckpoint` callback.\n",
    "\n",
    "    Args:\n",
    "        my_model (nn.Module): The model to be updated.\n",
    "        callbacks (dict): Dictionary of callbacks, including 'model_checkpoint'.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The updated model.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        try:\n",
    "            best_path = callbacks['model_checkpoint'].best_model_path\n",
    "            get_epoch = re.search('epoch=(\\d*)', best_path).group(1)\n",
    "            if 'gs://' in best_path:\n",
    "                subprocess.call(['gsutil','cp',best_path,tmpdirname])\n",
    "                best_path = os.path.join( tmpdirname, os.path.basename(best_path) )\n",
    "            print(f'Best model stashed at: {best_path}', file=sys.stderr)\n",
    "            print(f'Exists: {os.path.isfile(best_path)}', file=sys.stderr)\n",
    "            ckpt = torch.load( best_path )\n",
    "            my_model.load_state_dict( ckpt['state_dict'] )\n",
    "            print(f'Setting model from epoch: {get_epoch}', file=sys.stderr)\n",
    "        except KeyError:\n",
    "            print('Setting most recent model', file=sys.stderr)\n",
    "    return my_model\n",
    "\n",
    "model = set_best(model, {'model_checkpoint': checkpoint_callback})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548346c6-110a-4f1e-9b20-30cccd1b8177",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a2a9fde-fc16-4fd8-952b-184d4adcfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = data.test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "925a4341-aaf7-455f-8891-e515aba903f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_path,'r') as f:\n",
    "    f.readline()\n",
    "    seq_tensor = torch.stack([ boda.common.utils.dna2tensor(line.split()[0]) for line in f ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb54b70c-c336-4627-9222-e3b1b366cac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 0.],\n",
       "         [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[0., 0., 1.,  ..., 0., 0., 1.],\n",
       "         [1., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 1., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 0.,  ..., 0., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 1., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 1.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
