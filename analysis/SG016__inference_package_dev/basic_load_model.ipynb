{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c0091b-760c-489a-91f2-b1f639e83e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import boda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddbeac7-20ac-4cd8-8b12-a88e0565c47e",
   "metadata": {},
   "source": [
    "# Helpers\n",
    "\n",
    "1. `load_model` checks GPUs, clears a spot for the model to be downloaded, downloads, and loads the model in `eval` mode.\n",
    "\n",
    "2. `FlankBuilder` is used to pad inputs with MPRA vector backbone sequence. For technical reasons, Malinois reads 600 nt sequences (i.e., n x 4 x 600 inputs) but it should be 200 nt of variable sequence padded with MPRA backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9311b55e-d59a-46fc-979d-0e8d96b1a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(artifact_path):\n",
    "    \n",
    "    USE_CUDA = torch.cuda.device_count() >= 1\n",
    "    if os.path.isdir('./artifacts'):\n",
    "        shutil.rmtree('./artifacts')\n",
    "\n",
    "    boda.common.utils.unpack_artifact(artifact_path)\n",
    "\n",
    "    model_dir = './artifacts'\n",
    "\n",
    "    my_model = boda.common.utils.model_fn(model_dir)\n",
    "    my_model.eval()\n",
    "    if USE_CUDA:\n",
    "        my_model.cuda()\n",
    "    \n",
    "    return my_model\n",
    "\n",
    "class FlankBuilder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 left_flank=None,\n",
    "                 right_flank=None,\n",
    "                 batch_dim=0,\n",
    "                 cat_axis=-1\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.register_buffer('left_flank', left_flank.detach().clone())\n",
    "        self.register_buffer('right_flank', right_flank.detach().clone())\n",
    "        \n",
    "        self.batch_dim = batch_dim\n",
    "        self.cat_axis  = cat_axis\n",
    "        \n",
    "    def add_flanks(self, my_sample):\n",
    "        *batch_dims, channels, length = my_sample.shape\n",
    "        \n",
    "        pieces = []\n",
    "        \n",
    "        if self.left_flank is not None:\n",
    "            pieces.append( self.left_flank.expand(*batch_dims, -1, -1) )\n",
    "            \n",
    "        pieces.append( my_sample )\n",
    "        \n",
    "        if self.right_flank is not None:\n",
    "            pieces.append( self.right_flank.expand(*batch_dims, -1, -1) )\n",
    "            \n",
    "        return torch.cat( pieces, axis=self.cat_axis )\n",
    "    \n",
    "    def forward(self, my_sample):\n",
    "        return self.add_flanks(my_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abcf9b1-4de4-4a7d-9f9f-5e1541e6ce65",
   "metadata": {},
   "source": [
    "# Get Malinois\n",
    "\n",
    "Can download directly from a Google Storage bucket you can access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04f82f1-8cca-4c4d-b5e0-24ca18d4a039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://syrgoth/aip_ui_test/model_artifacts__20211113_021200__287348.tar.gz...\n",
      "/ [1 files][ 49.3 MiB/ 49.3 MiB]                                                \n",
      "Operation completed over 1 objects/49.3 MiB.                                     \n",
      "archive unpacked in ./\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from 20211113_021200 in eval mode\n"
     ]
    }
   ],
   "source": [
    "malinois_path = 'gs://syrgoth/aip_ui_test/model_artifacts__20211113_021200__287348.tar.gz'\n",
    "my_model = load_model(malinois_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd2b0f-2e0e-4bfc-9860-d978e3a6c2e4",
   "metadata": {},
   "source": [
    "# Set flanks\n",
    "\n",
    "MPRA flanks are saved as constants in the `boda` repo. These need to be sized to (1, 4, 200) each and used to init `FlankBuilder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e324c2-8c15-4ef7-ac2f-1d50c05abaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left flank shape: torch.Size([1, 4, 200])\n",
      "right flank shape: torch.Size([1, 4, 200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FlankBuilder()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_flank = boda.common.utils.dna2tensor( \n",
    "    boda.common.constants.MPRA_UPSTREAM[-200:] \n",
    ").unsqueeze(0)\n",
    "print(f'left flank shape: {left_flank.shape}')\n",
    "\n",
    "right_flank= boda.common.utils.dna2tensor( \n",
    "    boda.common.constants.MPRA_DOWNSTREAM[:200] \n",
    ").unsqueeze(0)\n",
    "right_flank.shape\n",
    "print(f'right flank shape: {right_flank.shape}')\n",
    "\n",
    "flank_builder = FlankBuilder(\n",
    "    left_flank=left_flank,\n",
    "    right_flank=right_flank,\n",
    ")\n",
    "flank_builder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041c756-f6f2-464c-b690-909c33af6d1a",
   "metadata": {},
   "source": [
    "# Example call\n",
    "\n",
    "Using `torch.no_grad()` so the computation graph isn't saved to memory. Since sequences are passed to the model as onehots in `torch.float32` format, we can use `torch.randn` to validate the model setup. Here a batch of 10 variable 200 nt (fake) sequences are being padded to 600 nt, then being passed to the model. Note, `my_model` and `flank_builder` have been set on the GPU using `.cuda()` calls. Therefore, the fake sequence also needs to be sent to `cuda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7cc54ec-8709-478d-b804-bd0c58983a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.9237, -4.3324, -2.6590],\n",
      "        [ 1.8957, -1.6334,  7.3978],\n",
      "        [-1.3470, -1.1018,  2.1494],\n",
      "        [ 0.0840, -0.4010,  7.8278],\n",
      "        [-2.9887, -2.0866,  3.6742],\n",
      "        [ 0.1913, -1.0352,  8.1859],\n",
      "        [-1.4896, -0.7747,  5.5385],\n",
      "        [-0.8139, -1.0344,  3.1287],\n",
      "        [-0.5258, -0.4886,  4.0395],\n",
      "        [-1.6722, -0.6007,  3.7459]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print( \n",
    "        my_model( \n",
    "            flank_builder(                     # Need to add MPRA flanks\n",
    "                torch.randn((10,4,200)).cuda() # Simulate a batch_size x 4 nucleotide x 200 nt long sequence\n",
    "            ) \n",
    "        ) \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
