{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d12db62-a507-454b-8e70-6060ff4c38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.cuda\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F \n",
    "from IPython.display import clear_output\n",
    "\n",
    "import boda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e81156bb-da4a-4f74-aff9-b745b93487b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(artifact_path):\n",
    "    \n",
    "    USE_CUDA = torch.cuda.device_count() >= 1\n",
    "    if os.path.isdir('./artifacts'):\n",
    "        shutil.rmtree('./artifacts')\n",
    "\n",
    "    boda.common.utils.unpack_artifact(artifact_path)\n",
    "\n",
    "    model_dir = './artifacts'\n",
    "\n",
    "    my_model = boda.common.utils.model_fn(model_dir)\n",
    "    my_model.eval()\n",
    "    if USE_CUDA:\n",
    "        my_model.cuda()\n",
    "    \n",
    "    return my_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02314108-d1de-44cb-bf66-1a80a9f1f0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences saved to random_sequences_for_diffusion.tsv\n"
     ]
    }
   ],
   "source": [
    "# Constants for flanks\n",
    "right_flank = boda.common.constants.MPRA_DOWNSTREAM[:200]\n",
    "left_flank = boda.common.constants.MPRA_UPSTREAM[-200:]\n",
    "\n",
    "#Setting random seed\n",
    "random.seed(42)\n",
    "\n",
    "def generate_random_sequence(length):\n",
    "    return ''.join(random.choice('ATCG') for _ in range(length))\n",
    "\n",
    "def generate_random_sequences_with_flanks(num_sequences, sequence_length):\n",
    "    sequences = []\n",
    "    for _ in range(num_sequences):\n",
    "        sequence = generate_random_sequence(sequence_length)\n",
    "        sequence_with_flanks = left_flank + sequence + right_flank\n",
    "        sequences.append(sequence_with_flanks)\n",
    "    return sequences\n",
    "\n",
    "N = 10  # Number of sequences\n",
    "sequence_length = 200\n",
    "\n",
    "random_sequences_for_diffusion = generate_random_sequences_with_flanks(N, sequence_length)\n",
    "\n",
    "# Save sequences to a TSV file\n",
    "tsv_file_path = \"random_sequences_for_diffusion.tsv\"\n",
    "with open(tsv_file_path, \"w\") as tsv_file:\n",
    "    tsv_file.write(\"Sequence\\n\")  # Write header\n",
    "    for sequence in random_sequences_for_diffusion:\n",
    "        tsv_file.write(sequence + \"\\n\")\n",
    "\n",
    "print(f\"Sequences saved to {tsv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0728f-a4e4-4e1a-9b08-f8cc31814d17",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f44f200-d441-4c9f-b21b-3178b3e49848",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = boda.data.SeqDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a6e985-4ef1-4eda-9453-aba623652aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mut = (0,301) # limit of mutations from 0 to 300\n",
    "epochs = 200 # specify the number of training epochs\n",
    "batch_size = 1024 # number of sequences in a batch\n",
    "batch_per_epoch = 1000 # number of batches in one epoch \n",
    "num_workers = 8\n",
    "lr = 0.001\n",
    "device = torch.device(\"cuda:0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "290feef0-aaff-4d9e-b37f-42f1757f1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        n = m.kernel_size[0] * m.out_channels\n",
    "        m.weight.data.normal_(0, math.sqrt(2 / n))\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(0, 0.001)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "            \n",
    "class DataloaderWrapper:\n",
    "    def __init__(self, dataloader, batch_per_epoch):\n",
    "        self.batch_per_epoch = batch_per_epoch\n",
    "        self.dataloader = dataloader\n",
    "        self.iterator = iter(dataloader)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_per_epoch\n",
    "    \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            return next(self.iterator)\n",
    "        except StopIteration:\n",
    "            self.iterator = iter(self.dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.batch_per_epoch):\n",
    "            try:\n",
    "                yield next(self.iterator)\n",
    "            except StopIteration:\n",
    "                self.iterator = iter(self.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c79c6291-09b8-49d1-b106-c284d652fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://tewhey-public-data/CODA_resources/malinois_model__20211113_021200__287348.tar.gz...\n",
      "\\ [1 files][ 49.3 MiB/ 49.3 MiB]                                                \n",
      "Operation completed over 1 objects/49.3 MiB.                                     \n",
      "archive unpacked in ./\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from 20211113_021200 in eval mode\n"
     ]
    }
   ],
   "source": [
    "malinois_path = 'gs://tewhey-public-data/CODA_resources/malinois_model__20211113_021200__287348.tar.gz'\n",
    "pretrained_model = load_model(malinois_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e61e8d5-62db-45c2-8dc9-edac8b93e860",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_479/1765739483.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m data = data_module(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/ubuntu/boda2/analysis/AR001__rotation/dummy_train.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/ubuntu/boda2/analysis/AR001__rotation/dummy_test.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/ubuntu/boda2/analysis/AR001__rotation/dummy_val.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mright_flank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMPRA_DOWNSTREAM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_module' is not defined"
     ]
    }
   ],
   "source": [
    "data = data_module(\n",
    "    train_file = \"/home/ubuntu/boda2/analysis/AR001__rotation/dummy_train.tsv\",\n",
    "    test_file = \"/home/ubuntu/boda2/analysis/AR001__rotation/dummy_test.tsv\",\n",
    "    val_file = \"/home/ubuntu/boda2/analysis/AR001__rotation/dummy_val.tsv\",\n",
    "    right_flank = boda.common.constants.MPRA_DOWNSTREAM[:200],\n",
    "    batch_size = batch_size,\n",
    "    left_flank = boda.common.constants.MPRA_UPSTREAM[-200:]\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=lr) ###check, bc this is diff than their code\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "score_criterion=nn.MSELoss()\n",
    "\n",
    "dl_train = data.train_dataloader()\n",
    "##check tensor dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abe923a2-7b17-4b81-99af-72eb96fe18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not edited, but pretrained typo corrected\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "            model: torch.nn.Module, \n",
    "            pretrained_model: torch.nn.Module,\n",
    "            train_dataloader: torch.utils.data.DataLoader ,\n",
    "            test_dataloader: torch.utils.data.DataLoader ,\n",
    "            criterion: torch.nn.CrossEntropyLoss,\n",
    "            loss_criterion: torch.nn.CrossEntropyLoss,\n",
    "            optimizer: torch.optim.Optimizer,\n",
    "            epochs: int,\n",
    "            batch_size: int = 1024,\n",
    "            batch_per_epoch: int = 1000,\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            ):\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.score_criterion = loss_criterion\n",
    "        self.model = model\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.train_dl = train_dataloader\n",
    "        self.test_dl = test_dataloader\n",
    "        self.epochs = epochs\n",
    "        self.batch_per_epoch = batch_per_epoch\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.score_cor_mean = []\n",
    "        self.score = []\n",
    "            \n",
    "    def train(self, epoch):\n",
    "        print(f'start training, epoch = {epoch}')\n",
    "        self.model.train()\n",
    "        ltr = []\n",
    "        for _, data in tqdm(enumerate(self.train_dl), mininterval=60):\n",
    "            target_seq, mutated_seq, _ = data\n",
    "            target_seq, mutated_seq = target_seq.float().to(self.device), mutated_seq.float().to(self.device) \n",
    "            pred = self.model(mutated_seq)\n",
    "            loss = self.criterion(pred, target_seq)\n",
    "            ltr.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad() \n",
    "\n",
    "        mean_loss = np.mean(ltr)\n",
    "        return mean_loss\n",
    "    \n",
    "    def validate(self, epoch):\n",
    "        print(f'start validating, epoch = {epoch}')\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            self.pretrained_model.eval()\n",
    "            lte = []\n",
    "            score_losses = []\n",
    "            score_cores = []\n",
    "            \n",
    "            \n",
    "            for _, data in tqdm(enumerate(self.test_dl), mininterval=60):\n",
    "                target_seq_val, mutated_seq_val, _ = data\n",
    "                mutated_seq_val, target_seq_val = mutated_seq_val.float().to(self.device), target_seq_val.float().to(self.device)\n",
    "\n",
    "                target_score = mutated_seq_val[:,4,1].clone()\n",
    "                mutated_seq = mutated_seq_val[:,:4,:].clone()\n",
    "                mut_seqs = torch.concat((mutated_seq, torch.zeros(mutated_seq.shape[0],2,mutated_seq.shape[2], device=self.device)), dim=1)\n",
    "                left_batch = torch.broadcast_to(left_s2t, (mut_seqs.shape[0], left_s2t.shape[0], left_s2t.shape[1])).to(self.device)\n",
    "                right_batch = torch.broadcast_to(right_s2t, (mut_seqs.shape[0], right_s2t.shape[0], right_s2t.shape[1])).to(self.device)\n",
    "\n",
    "\n",
    "                pred = self.model(mutated_seq_val)\n",
    "                pred_seq = torch.softmax(pred, dim=1)\n",
    "                loss = self.criterion(pred_seq, target_seq_val)\n",
    "                lte.append(loss.item())\n",
    "                \n",
    "                seqs = torch.concat((pred_seq, torch.zeros(pred_seq.shape[0], 2, pred_seq.shape[2], device=device)), dim=1)\n",
    "                \n",
    "                long_pred = torch.concat((left_batch, seqs, right_batch), dim=2)\n",
    "                pred_score = self.pretrained_model(long_pred)[1]\n",
    "                \n",
    "                score_loss = self.score_criterion(pred_score, target_score)\n",
    "                score_losses.append(score_loss.item())\n",
    "                score_cor = stats.pearsonr(pred_score.cpu().numpy(), target_score.cpu().numpy())[0]\n",
    "                score_cores.append(score_cor)\n",
    "\n",
    "                \n",
    "            self.score.append(np.mean(score_losses))\n",
    "            self.score_cor_mean.append(np.mean(score_cores))\n",
    "            mean_loss_val = np.mean(lte)\n",
    "            return mean_loss_val\n",
    "       \n",
    "        \n",
    "    def training(self):\n",
    "        \n",
    "        self.save_dir = f\"../saved_model/model_epochs_{self.epochs}\"\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            tr_loss = self.train(epoch)\n",
    "            train_losses.append(tr_loss)\n",
    "  \n",
    "            test_loss = self.validate(epoch)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "            self.plotter(train_losses,test_losses, epoch)\n",
    "            self.save_model(epoch,train_losses)\n",
    "        return train_losses, test_losses, self.score\n",
    "    \n",
    "\n",
    "    def plotter(self, loss_train, loss_val, epoch):\n",
    "        fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1,  figsize=(7, 7))\n",
    "        \n",
    "        ax1.plot(loss_train, color='red')\n",
    "        ax3.plot(loss_train, color='red')\n",
    "        ax2.plot(loss_val, color='blue')\n",
    "        ax3.plot(loss_val, color='blue')\n",
    "        ax4.plot(self.score, color = 'black')\n",
    "        ax5.plot(self.score_cor_mean, color = 'black')\n",
    "        ax1.grid(axis='x')\n",
    "        ax2.grid(axis='x')\n",
    "        ax3.grid(axis='x')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Train Loss')\n",
    "        ax3.set_ylabel('Train and val Loss')\n",
    "        ax2.set_ylabel('Val Loss')\n",
    "        ax4.set_ylabel('Score MSE')\n",
    "        ax5.set_ylabel('Pearson cor')\n",
    "        \n",
    "        suptitle_string = f'epoch={epoch}'\n",
    "        fig.suptitle(suptitle_string, y=1.05, fontsize=10)\n",
    "\n",
    "        pic_test_name = os.path.join(self.save_dir, f\"lossestrainandtest_epoch={epoch}.png\")\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(pic_test_name)\n",
    "        fig.show()\n",
    "        np.save(f'../saved_model/model_epochs_{self.epochs}/train_loss.npy', np.array(loss_train))\n",
    "        np.save(f'../saved_model/model_epochs_{self.epochs}/test_loss.npy', np.array(loss_val))\n",
    "        np.save(f'../saved_model/model_epochs_{self.epochs}/score_loss.npy', np.array(self.score))\n",
    "        np.save(f'../saved_model/model_epochs_{self.epochs}/score_loss.npy', np.array(self.score_cor_mean))\n",
    "            \n",
    "    def save_model(self, epoch, losseshist):\n",
    "        PATH = os.path.join(self.save_dir, f\"model_{epoch}.pth\")\n",
    "            \n",
    "        torch.save({\n",
    "            'epoch' : epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': losseshist\n",
    "            }, PATH)\n",
    "\n",
    "        print(f'---------------  SAVED MODEL {PATH}-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a10f9a23-3e2a-4445-b263-8356666dfc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying to edit\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "            model: torch.nn.Module, \n",
    "            pretrained_model: torch.nn.Module,\n",
    "            train_dataloader: torch.utils.data.DataLoader ,\n",
    "            test_dataloader: torch.utils.data.DataLoader ,\n",
    "            criterion: torch.nn.CrossEntropyLoss,\n",
    "            loss_criterion: torch.nn.CrossEntropyLoss,\n",
    "            optimizer: torch.optim.Optimizer,\n",
    "            epochs: int,\n",
    "            batch_size: int = 1024,\n",
    "            batch_per_epoch: int = 1000,\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            ):\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.score_criterion = loss_criterion\n",
    "        self.model = model\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.train_dl = train_dataloader\n",
    "        self.test_dl = test_dataloader\n",
    "        self.epochs = epochs\n",
    "        self.batch_per_epoch = batch_per_epoch\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.score_cor_mean = []\n",
    "        self.score = []\n",
    "            \n",
    "    def train(self, epoch):\n",
    "        print(f'start training, epoch = {epoch}')\n",
    "        self.model.train()\n",
    "        ltr = []\n",
    "        for _, data in tqdm(enumerate(self.train_dl), mininterval=60):\n",
    "            target_seq, mutated_seq, _ = data\n",
    "            target_seq, mutated_seq = target_seq.float().to(self.device), mutated_seq.float().to(self.device) \n",
    "            pred = self.model(mutated_seq)\n",
    "            loss = self.criterion(pred, target_seq)\n",
    "            ltr.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad() \n",
    "\n",
    "        mean_loss = np.mean(ltr)\n",
    "        return mean_loss\n",
    "    \n",
    "    def validate(self, epoch):\n",
    "        print(f'start validating, epoch = {epoch}')\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            self.pretrained_model.eval()\n",
    "            lte = []\n",
    "            score_losses = []\n",
    "            score_cores = []\n",
    "            \n",
    "            \n",
    "            for _, data in tqdm(enumerate(self.test_dl), mininterval=60):\n",
    "                target_seq_val, mutated_seq_val, _ = data\n",
    "                mutated_seq_val, target_seq_val = mutated_seq_val.float().to(self.device), target_seq_val.float().to(self.device)\n",
    "\n",
    "                target_score = mutated_seq_val[:,4,1].clone()\n",
    "                mutated_seq = mutated_seq_val[:,:4,:].clone()\n",
    "                mut_seqs = torch.concat((mutated_seq, torch.zeros(mutated_seq.shape[0],2,mutated_seq.shape[2], device=self.device)), dim=1)\n",
    "                left_batch = torch.broadcast_to(left_s2t, (mut_seqs.shape[0], left_s2t.shape[0], left_s2t.shape[1])).to(self.device)\n",
    "                right_batch = torch.broadcast_to(right_s2t, (mut_seqs.shape[0], right_s2t.shape[0], right_s2t.shape[1])).to(self.device)\n",
    "\n",
    "\n",
    "                pred = self.model(mutated_seq_val)\n",
    "                pred_seq = torch.softmax(pred, dim=1)\n",
    "                loss = self.criterion(pred_seq, target_seq_val)\n",
    "                lte.append(loss.item())\n",
    "                \n",
    "                seqs = torch.concat((pred_seq, torch.zeros(pred_seq.shape[0], 2, pred_seq.shape[2], device=device)), dim=1)\n",
    "                \n",
    "                long_pred = torch.concat((left_batch, seqs, right_batch), dim=2)\n",
    "                pred_score = self.pretrained_model(long_pred)[1]\n",
    "                \n",
    "                score_loss = self.score_criterion(pred_score, target_score)\n",
    "                score_losses.append(score_loss.item())\n",
    "                score_cor = stats.pearsonr(pred_score.cpu().numpy(), target_score.cpu().numpy())[0]\n",
    "                score_cores.append(score_cor)\n",
    "\n",
    "                \n",
    "            self.score.append(np.mean(score_losses))\n",
    "            self.score_cor_mean.append(np.mean(score_cores))\n",
    "            mean_loss_val = np.mean(lte)\n",
    "            return mean_loss_val\n",
    "       \n",
    "        \n",
    "    def training(self):\n",
    "        \n",
    "        self.save_dir = f\"../saved_model/model_epochs_{self.epochs}\"\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            tr_loss = self.train(epoch)\n",
    "            train_losses.append(tr_loss)\n",
    "  \n",
    "            test_loss = self.validate(epoch)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "            self.plotter(train_losses,test_losses, epoch)\n",
    "            self.save_model(epoch,train_losses)\n",
    "        return train_losses, test_losses, self.score\n",
    "    \n",
    "\n",
    "    def plotter(self, loss_train, loss_val, epoch):\n",
    "        fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1,  figsize=(7, 7))\n",
    "        \n",
    "        ax1.plot(loss_train, color='red')\n",
    "        ax3.plot(loss_train, color='red')\n",
    "        ax2.plot(loss_val, color='blue')\n",
    "        ax3.plot(loss_val, color='blue')\n",
    "        ax4.plot(self.score, color = 'black')\n",
    "        ax5.plot(self.score_cor_mean, color = 'black')\n",
    "        ax1.grid(axis='x')\n",
    "        ax2.grid(axis='x')\n",
    "        ax3.grid(axis='x')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Train Loss')\n",
    "        ax3.set_ylabel('Train and val Loss')\n",
    "        ax2.set_ylabel('Val Loss')\n",
    "        ax4.set_ylabel('Score MSE')\n",
    "        ax5.set_ylabel('Pearson cor')\n",
    "        \n",
    "        suptitle_string = f'epoch={epoch}'\n",
    "        fig.suptitle(suptitle_string, y=1.05, fontsize=10)\n",
    "\n",
    "        pic_test_name = os.path.join(self.save_dir, f\"lossestrainandtest_epoch={epoch}.png\")\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(pic_test_name)\n",
    "        fig.show()\n",
    "        np.save(f'../saved_model/model_epochs_{self.epochs}/train_loss.npy', np.array(loss_train))\n",
    "        np.save(f'../saved_model/model_epochs_{self.epochs}/test_loss.npy', np.array(loss_val))\n",
    "        np.save(f'../saved_model/model_epochs_{self.epochs}/score_loss.npy', np.array(self.score))\n",
    "        np.save(f'../saved_model/model_epochs_{self.epochs}/score_loss.npy', np.array(self.score_cor_mean))\n",
    "            \n",
    "    def save_model(self, epoch, losseshist):\n",
    "        PATH = os.path.join(self.save_dir, f\"model_{epoch}.pth\")\n",
    "            \n",
    "        torch.save({\n",
    "            'epoch' : epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': losseshist\n",
    "            }, PATH)\n",
    "\n",
    "        print(f'---------------  SAVED MODEL {PATH}-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7403b-ea5d-41f4-9553-839b40ee1c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f81ebac-a8fc-4ae8-aeb5-cf1734f091b9",
   "metadata": {},
   "source": [
    "## Generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "107a023e-7d93-45e7-92de-a8e23c8b9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mutagenesis code (copied from LegNet)\n",
    "\n",
    "def mutagenesis(seqs, maxmut):\n",
    "    batchsize = seqs.shape[0]\n",
    "    seqlen = seqs.shape[2]\n",
    "    muts = torch.full((batchsize,), maxmut)\n",
    "    index = torch.arange(batchsize)\n",
    "    mut_positions = torch.zeros(batchsize, seqlen, dtype=bool)\n",
    "    for i in range(maxmut):\n",
    "        single_positions = torch.randint(high=seqlen, size=(batchsize,))\n",
    "        mut_positions[index, single_positions] |= muts > i\n",
    "\n",
    "    mut_positions = mut_positions[:,None,:].broadcast_to(seqs.shape)\n",
    "    x = seqs.permute(2, 0, 1)[mut_positions.permute(2, 0, 1)]\n",
    "    mut_number = x.shape[0] // 4\n",
    "    \n",
    "    myperm = torch.randint(high=ALLPERM.shape[0], size=(mut_number,))\n",
    "    myperm = (ALLPERM[myperm] + torch.arange(mut_number)[:,None] * 4).ravel()\n",
    "    seqs.permute(2, 0, 1)[mut_positions.permute(2, 0, 1)] = x[myperm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63199a94-9e03-4a27-96bd-eb53b0ff827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion-like sampling\n",
    "\n",
    "def predict_float(dl_test, mut_interval, intensities, start, end):    \n",
    "    seqs_batches = []\n",
    "    scores_batches = []\n",
    "    b_i = 0\n",
    "    with torch.no_grad():\n",
    "        diffusion_model.eval()\n",
    "        for data in dl_test:\n",
    "            b_i += 1\n",
    "            seq_batch = data.float().to(device)\n",
    "            score_chanels = seq_batch[:,4:5,:].clone().to(device)\n",
    "            seq_batch = seq_batch[:,:4,:]\n",
    "            target_score = torch.FloatTensor(seq_batch.shape[0], 1, 1).uniform_(start, end).to(device)\n",
    "            for intens, muts, in zip(intensities, mut_interval):\n",
    "                mutagenesis(seq_batch, muts)              \n",
    "                tmp = torch.broadcast_to(target_score, (target_score.shape[0], 1, 80))\n",
    "                seq_batch = torch.concat((seq_batch.to(device), tmp.to(device), torch.full_like(score_chanels, intens).to(device)), dim=1) \n",
    "                seq_batch = diffusion_model(seq_batch)\n",
    "                seq_batch = torch.softmax(seq_batch, dim=1) \n",
    "            seqs_batches.append(seq_batch.cpu().numpy()) \n",
    "            scores_batches.append((target_score.squeeze()).cpu().numpy())\n",
    "        return seqs_batches, target_score, scores_batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
