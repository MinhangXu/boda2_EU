{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: Tesla V100-SXM2-16GB (1 count)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchsummary import summary\n",
    "from functools import partial\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f'GPU available: {torch.cuda.get_device_name(0)} ({torch.cuda.device_count()} count)')\n",
    "else:\n",
    "    print('No GPU available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://syrgoth/my-model.epoch_5-step_19885.pkl...\n",
      "- [1 files][ 18.5 MiB/ 18.5 MiB]                                                \n",
      "Operation completed over 1 objects/18.5 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp gs://syrgoth/my-model.epoch_5-step_19885.pkl ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://syrgoth/checkpoints/manual_checkpoint_multioutput_lasthidden250_L1.ckpt...\n",
      "/ [1 files][ 21.4 MiB/ 21.4 MiB]                                                \n",
      "Operation completed over 1 objects/21.4 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp gs://syrgoth/checkpoints/manual_checkpoint_multioutput_lasthidden250_L1.ckpt ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basset\n",
    "def get_padding(kernel_size):\n",
    "    left = (kernel_size - 1) // 2\n",
    "    right= kernel_size - 1 - left\n",
    "    return [ max(0,x) for x in [left,right] ]\n",
    "\n",
    "class Conv1dNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, \n",
    "                 stride=1, padding=0, dilation=1, groups=1, \n",
    "                 bias=True, batch_norm=True, weight_norm=True):\n",
    "        super(Conv1dNorm, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, \n",
    "                              stride, padding, dilation, groups, bias)\n",
    "        if weight_norm:\n",
    "            self.conv = nn.utils.weight_norm(self.conv)\n",
    "        if batch_norm:\n",
    "            self.bn_layer = nn.BatchNorm1d(out_channels, eps=1e-05, momentum=0.1, \n",
    "                                           affine=True, track_running_stats=True)\n",
    "    def forward(self, input):\n",
    "        try:\n",
    "            return self.bn_layer( self.conv( input ) )\n",
    "        except AttributeError:\n",
    "            return self.conv( input )\n",
    "        \n",
    "class LinearNorm(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, \n",
    "                 batch_norm=True, weight_norm=True):\n",
    "        super(LinearNorm, self).__init__()\n",
    "        self.linear  = nn.Linear(in_features, out_features, bias=True)\n",
    "        if weight_norm:\n",
    "            self.linear = nn.utils.weight_norm(self.linear)\n",
    "        if batch_norm:\n",
    "            self.bn_layer = nn.BatchNorm1d(out_features, eps=1e-05, momentum=0.1, \n",
    "                                           affine=True, track_running_stats=True)\n",
    "    def forward(self, input):\n",
    "        try:\n",
    "            return self.bn_layer( self.linear( input ) )\n",
    "        except AttributeError:\n",
    "            return self.linear( input )\n",
    "\n",
    "class Basset(pl.LightningModule):\n",
    "    r\"\"\"Write docstring here.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        \n",
    "        parser.add_argument('--conv1_channels', type=int, default=300)\n",
    "        parser.add_argument('--conv1_kernel_size', type=int, default=19)\n",
    "        \n",
    "        parser.add_argument('--conv2_channels', type=int, default=200)\n",
    "        parser.add_argument('--conv2_kernel_size', type=int, default=11)\n",
    "        \n",
    "        parser.add_argument('--conv3_channels', type=int, default=200)\n",
    "        parser.add_argument('--conv3_kernel_size', type=int, default=7)\n",
    "        \n",
    "        parser.add_argument('--linear1_channels', type=int, default=1000)\n",
    "        parser.add_argument('--linear2_channels', type=int, default=1000)\n",
    "        parser.add_argument('--n_outputs', type=int, default=280)\n",
    "        \n",
    "        parser.add_argument('--dropout_p', type=float, default=0.3)\n",
    "        parser.add_argument('--use_batch_norm', type=utils.str2bool, default=True)\n",
    "        parser.add_argument('--use_weight_norm',type=utils.str2bool, default=False)\n",
    "        \n",
    "        parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
    "        \n",
    "        return parser\n",
    "    \n",
    "    def __init__(self, conv1_channels=300, conv1_kernel_size=19, \n",
    "                 conv2_channels=200, conv2_kernel_size=11, \n",
    "                 conv3_channels=200, conv3_kernel_size=7, \n",
    "                 linear1_channels=1000, linear2_channels=1000, \n",
    "                 n_outputs=280, activation='ReLU', \n",
    "                 dropout_p=0.3, use_batch_norm=True, use_weight_norm=False,\n",
    "                 learning_rate=1e-4):                                                \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1_channels    = conv1_channels\n",
    "        self.conv1_kernel_size = conv1_kernel_size\n",
    "        self.conv1_pad = get_padding(conv1_kernel_size)\n",
    "        \n",
    "        self.conv2_channels    = conv2_channels\n",
    "        self.conv2_kernel_size = conv2_kernel_size\n",
    "        self.conv2_pad = get_padding(conv2_kernel_size)\n",
    "\n",
    "        \n",
    "        self.conv3_channels    = conv3_channels\n",
    "        self.conv3_kernel_size = conv3_kernel_size\n",
    "        self.conv3_pad = get_padding(conv3_kernel_size)\n",
    "        \n",
    "        self.linear1_channels  = linear1_channels\n",
    "        self.linear2_channels  = linear2_channels\n",
    "        self.n_outputs         = n_outputs\n",
    "        \n",
    "        self.activation        = activation\n",
    "        \n",
    "        self.dropout_p         = dropout_p\n",
    "        self.use_batch_norm    = use_batch_norm\n",
    "        self.use_weight_norm   = use_weight_norm\n",
    "        \n",
    "        self.learning_rate     = learning_rate\n",
    "        \n",
    "        self.pad1  = nn.ConstantPad1d(self.conv1_pad, 0.)\n",
    "        self.conv1 = Conv1dNorm(4, \n",
    "                                self.conv1_channels, self.conv1_kernel_size, \n",
    "                                stride=1, padding=0, dilation=1, groups=1, \n",
    "                                bias=True, \n",
    "                                batch_norm=self.use_batch_norm, \n",
    "                                weight_norm=self.use_weight_norm)\n",
    "        self.pad2  = nn.ConstantPad1d(self.conv2_pad, 0.)\n",
    "        self.conv2 = Conv1dNorm(self.conv1_channels, \n",
    "                                self.conv2_channels, self.conv2_kernel_size, \n",
    "                                stride=1, padding=0, dilation=1, groups=1, \n",
    "                                bias=True, \n",
    "                                batch_norm=self.use_batch_norm, \n",
    "                                weight_norm=self.use_weight_norm)\n",
    "        self.pad3  = nn.ConstantPad1d(self.conv3_pad, 0.)\n",
    "        self.conv3 = Conv1dNorm(self.conv2_channels, \n",
    "                                self.conv3_channels, self.conv3_kernel_size, \n",
    "                                stride=1, padding=0, dilation=1, groups=1, \n",
    "                                bias=True, \n",
    "                                batch_norm=self.use_batch_norm, \n",
    "                                weight_norm=self.use_weight_norm)\n",
    "        \n",
    "        self.pad4 = nn.ConstantPad1d((1,1), 0.)\n",
    "\n",
    "        self.maxpool_3 = nn.MaxPool1d(3, padding=0)\n",
    "        self.maxpool_4 = nn.MaxPool1d(4, padding=0)\n",
    "        \n",
    "        self.linear1 = LinearNorm(self.conv3_channels*13, self.linear1_channels, \n",
    "                                  bias=True, \n",
    "                                  batch_norm=self.use_batch_norm, \n",
    "                                  weight_norm=self.use_weight_norm)\n",
    "        self.linear2 = LinearNorm(self.linear1_channels, self.linear2_channels, \n",
    "                                  bias=True, \n",
    "                                  batch_norm=self.use_batch_norm, \n",
    "                                  weight_norm=self.use_weight_norm)\n",
    "        self.output  = nn.Linear(self.linear2_channels, self.n_outputs)\n",
    "        \n",
    "        self.nonlin  = getattr(nn, self.activation)()                               \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hook = self.nonlin( self.conv1( self.pad1( x ) ) )\n",
    "        hook = self.maxpool_3( hook )\n",
    "        hook = self.nonlin( self.conv2( self.pad2( hook ) ) )\n",
    "        hook = self.maxpool_4( hook )\n",
    "        hook = self.nonlin( self.conv3( self.pad3( hook ) ) )\n",
    "        hook = self.maxpool_4( self.pad4( hook ) )        \n",
    "        hook = torch.flatten( hook, start_dim=1 )\n",
    "        hook = self.dropout( self.nonlin( self.linear1( hook ) ) )\n",
    "        hook = self.dropout( self.nonlin( self.linear2( hook ) ) )\n",
    "        output = self.output( hook )\n",
    "        return output, hook\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y   = batch\n",
    "        logits = self(x)\n",
    "        loss   = self.criterion(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y   = batch\n",
    "        logits = self(x)\n",
    "        loss   = self.criterion(logits, y)\n",
    "        self.log('valid_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MPRA Basset (multi branch)\n",
    "class MPRA_Basset(pl.LightningModule):\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        \n",
    "        parser.add_argument('--pretrained', type=bool, default=True, \n",
    "                            help='True if pretrained basset weights are going to be given')  \n",
    "        parser.add_argument('--target_width', type=int, default=3, \n",
    "                            help='Width (or length) of the target data') \n",
    "        parser.add_argument('--learning_rate', type=float, default=1e-4, \n",
    "                            help='Value of the learning rate')\n",
    "        parser.add_argument('--optimizer', type=str, default='Adam', \n",
    "                            help='Name of the optimizer')\n",
    "        parser.add_argument('--scheduler', type=bool, default=True, \n",
    "                            help='If true it implements cosine annealing LR')\n",
    "        parser.add_argument('--weight_decay', type=float, default=1e-6, \n",
    "                            help='Weight decay rate')\n",
    "        parser.add_argument('--epochs', type=int, default=1, \n",
    "                            help='Number of epochs passed to the trainer (used by the scheduler)') \n",
    "        return parser\n",
    "    \n",
    "    def __init__(self,\n",
    "                 basset_weights_path=None,\n",
    "                 pretrained=True,\n",
    "                 target_width=3,\n",
    "                 learning_rate=1e-4,\n",
    "                 optimizer='Adam',\n",
    "                 scheduler=False,\n",
    "                 weight_decay=1e-6,\n",
    "                 epochs=1,\n",
    "                 extra_hidden_size = 100,\n",
    "                 criterion = 'MSELoss',\n",
    "                 last_activation='Tanh',\n",
    "                 sneaky_factor=1,\n",
    "                 #basset_kwards=None,\n",
    "                 **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "        self.pretrained = pretrained\n",
    "        self.basset_weights_path = basset_weights_path\n",
    "        self.target_width = target_width\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.extra_hidden_size = extra_hidden_size\n",
    "        self.sneaky_factor = sneaky_factor\n",
    "        \n",
    "        self.criterion = getattr(nn, criterion)() #nn.MSELoss()   \n",
    "        self.last_activation = getattr(nn, last_activation)()\n",
    "        \n",
    "        self.basset_net = Basset()\n",
    "        if self.pretrained:\n",
    "            try:\n",
    "                self.basset_net.load_state_dict(torch.load(self.basset_weights_path))\n",
    "            except:\n",
    "                self.basset_net.load_state_dict(torch.load(self.basset_weights_path, map_location=torch.device('cpu')))\n",
    "        \n",
    "        self.basset_last_hidden_width = self.basset_net.linear2_channels\n",
    "\n",
    "        self.output_1 = nn.Sequential(\n",
    "            nn.Linear(self.basset_last_hidden_width, self.extra_hidden_size),\n",
    "            self.last_activation,\n",
    "            # nn.Linear(self.extra_hidden_size, self.extra_hidden_size),\n",
    "            # self.last_activation,\n",
    "            nn.Linear(self.extra_hidden_size, 1)\n",
    "            )\n",
    "        \n",
    "        self.output_2 = nn.Sequential(\n",
    "            nn.Linear(self.basset_last_hidden_width, self.extra_hidden_size),\n",
    "            self.last_activation,\n",
    "            # nn.Linear(self.extra_hidden_size, self.extra_hidden_size),\n",
    "            # self.last_activation,\n",
    "            nn.Linear(self.extra_hidden_size, 1)\n",
    "            )\n",
    "        \n",
    "        self.output_3 = nn.Sequential(\n",
    "            nn.Linear(self.basset_last_hidden_width, self.extra_hidden_size),\n",
    "            self.last_activation,\n",
    "            # nn.Linear(self.extra_hidden_size, self.extra_hidden_size),\n",
    "            # self.last_activation,\n",
    "            nn.Linear(self.extra_hidden_size, 1)\n",
    "            )       \n",
    "        #self.mpra_output = nn.Linear(self.basset_last_hidden_width, self.target_width)\n",
    "\n",
    "        self.example_input_array = torch.rand(1, 4, 600)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, basset_last_hidden = self.basset_net(x)\n",
    "        output_1 = self.output_1(basset_last_hidden)\n",
    "        output_2 = self.output_2(basset_last_hidden)\n",
    "        output_3 = self.output_3(basset_last_hidden)\n",
    "        mpra_pred = torch.cat((output_1, output_2, output_3), dim=1)\n",
    "        #mpra_pred = self.mpra_output(basset_last_hidden)\n",
    "        return mpra_pred\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        shannon_pred, shannon_target = Shannon_entropy(y_pred), Shannon_entropy(y)\n",
    "        loss = self.criterion(y_pred, y) + self.sneaky_factor*self.criterion(shannon_pred, shannon_target)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return {'loss': loss, 'pred': y_pred, 'target': y}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        preds = torch.cat([out['pred'] for out in validation_step_outputs], dim=0)\n",
    "        targets  = torch.cat([out['target'] for out in validation_step_outputs], dim=0)\n",
    "        pearsons, mean_pearson = Pearson_correlation(preds, targets)\n",
    "        shannon_pred, shannon_target = Shannon_entropy(preds), Shannon_entropy(targets)\n",
    "        specificity_pearson, specificity_mean_pearson = Pearson_correlation(shannon_pred, shannon_target)\n",
    "        self.log('Pearson', mean_pearson)\n",
    "        self.log('Pearson_Shannon', specificity_mean_pearson)\n",
    "        res_str = '|'\n",
    "        res_str += ' Prediction correlation: {:.5f} | Specificity correlation: {:.5f} |' \\\n",
    "                    .format(mean_pearson.item(), specificity_mean_pearson.item())\n",
    "        print(res_str)\n",
    "        print('-'*len(res_str))\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = getattr(torch.optim, self.optimizer)(self.parameters(), lr=self.learning_rate,\n",
    "                                                         weight_decay=self.weight_decay)#, amsgrad=True) #I hard-coded amsgrad for Adam  \n",
    "        if self.scheduler:\n",
    "            lr_scheduler = {\n",
    "                'scheduler' : torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.epochs, eta_min=1e-6),\n",
    "                'name': 'learning_rate'\n",
    "                           }\n",
    "            return [optimizer], [lr_scheduler]\n",
    "        else:\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MPRA_Basset(basset_weights_path='my-model.epoch_5-step_19885.pkl',\n",
    "                    extra_hidden_size = 250,\n",
    "                    criterion='L1Loss',\n",
    "                    last_activation='Tanh',\n",
    "                    sneaky_factor=1,\n",
    "                    pretrained=True)\n",
    "\n",
    "if not next(model.parameters()).is_cuda:\n",
    "    model.cuda()\n",
    "    print('Model moved to cuda')\n",
    "\n",
    "checkpoint = torch.load('manual_checkpoint_multioutput_lasthidden250_L1.ckpt')\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use model with prototype generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class constants():\n",
    "    def __init__(self):\n",
    "        self.STANDARD_NT = ['A','G','T','C']\n",
    "        self.MPRA_UPSTREAM  = 'ACGAAAATGTTGGATGCTCATACTCGTCCTTTTTCAATATTATTGAAGCATTTATCAGGGTTACTAGTACGTCTCTCAAGGATAAGTAAGTAATATTAAGGTACGGGAGGTATTGGACAGGCCGCAATAAAATATCTTTATTTTCATTACATCTGTGTGTTGGTTTTTTGTGTGAATCGATAGTACTAACATACGCTCTCCATCAAAACAAAACGAAACAAAACAAACTAGCAAAATAGGCTGTCCCCAGTGCAAGTGCAGGTGCCAGAACATTTCTCTGGCCTAACTGGCCGCTTGACG'\n",
    "        self.MPRA_DOWNSTREAM= 'CACTGCGGCTCCTGCGATCTAACTGGCCGGTACCTGAGCTCGCTAGCCTCGAGGATATCAAGATCTGGCCTCGGCGGCCAAGCTTAGACACTAGAGGGTATATAATGGAAGCTCGACTTCCAGCTTGGCAATCCGGTACTGTTGGTAAAGCCACCATGGTGAGCAAGGGCGAGGAGCTGTTCACCGGGGTGGTGCCCATCCTGGTCGAGCTGGACGGCGACGTAAACGGCCACAAGTTCAGCGTGTCCGGCGAGGGCGAGGGCGATGCCACCTACGGCAAGCTGACCCTGAAGTTCATCT'\n",
    "constants = constants()\n",
    "\n",
    "class utils():\n",
    "    @staticmethod\n",
    "    def shannon_entropy(x):\n",
    "        p_c = nn.Softmax(dim=1)(x)    \n",
    "        return torch.sum(- p_c * torch.log(p_c), axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def shannon_entropy_mean(x):\n",
    "        p_c = nn.Softmax(dim=1)(x)    \n",
    "        return torch.mean(- p_c * torch.log(p_c))\n",
    "\n",
    "    @staticmethod\n",
    "    def neg_reward_loss(x):\n",
    "        return -torch.sum(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_paddingTensors(num_sequences, padding_len, num_st_samples=1, for_multi_sampling=True):\n",
    "        assert padding_len >= 0 and type(padding_len) == int, 'Padding must be a nonnegative integer'\n",
    "        upPad_logits, downPad_logits = None, None  \n",
    "        if padding_len > 0:\n",
    "            assert padding_len <= (len(constants.MPRA_UPSTREAM) + len(constants.MPRA_DOWNSTREAM)), 'Not enough padding available'\n",
    "            upPad_logits, downPad_logits = utils.dna2tensor(constants.MPRA_UPSTREAM), \\\n",
    "                                        utils.dna2tensor(constants.MPRA_DOWNSTREAM)\n",
    "            upPad_logits, downPad_logits = upPad_logits[:,-padding_len//2 + padding_len%2:], \\\n",
    "                                        downPad_logits[:,:padding_len//2 + padding_len%2]\n",
    "            if for_multi_sampling:\n",
    "                upPad_logits, downPad_logits = upPad_logits.repeat(num_st_samples, num_sequences, 1, 1), \\\n",
    "                                            downPad_logits.repeat(num_st_samples, num_sequences, 1, 1)                                     \n",
    "            else:\n",
    "                upPad_logits, downPad_logits = upPad_logits.repeat(num_sequences, 1, 1), \\\n",
    "                                            downPad_logits.repeat(num_sequences, 1, 1)  \n",
    "        return upPad_logits, downPad_logits\n",
    "\n",
    "    @staticmethod\n",
    "    def dna2tensor(sequence_str, vocab_list=constants.STANDARD_NT):\n",
    "        seq_tensor = np.zeros((len(vocab_list), len(sequence_str)))\n",
    "        for letterIdx, letter in enumerate(sequence_str):\n",
    "            seq_tensor[vocab_list.index(letter), letterIdx] = 1\n",
    "        seq_tensor = torch.Tensor(seq_tensor)\n",
    "        return seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSP_Parameteres(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_sequences=1,\n",
    "                 seq_len=200,\n",
    "                 padding_len=400,\n",
    "                 num_st_samples=1,\n",
    "                 vocab_len=4,\n",
    "                 affine=True,\n",
    "                 onehot_start=True,\n",
    "                 **kwrags):\n",
    "        super().__init__()\n",
    "        self.num_sequences = num_sequences\n",
    "        self.seq_len = seq_len  \n",
    "        self.padding_len = padding_len\n",
    "        self.num_st_samples = num_st_samples\n",
    "        self.vocab_len = vocab_len\n",
    "        self.affine = affine\n",
    "        self.onehot_start = onehot_start\n",
    "\n",
    "        self.noise_factor = 0\n",
    "\n",
    "        self.create_differentiable_input_logits(one_hot=self.onehot_start)\n",
    "\n",
    "        left_flank, right_flank = utils.create_paddingTensors(self.num_sequences, self.padding_len,\n",
    "                                                                   self.num_st_samples, True)\n",
    "        self.register_buffer('left_flank', left_flank)\n",
    "        self.register_buffer('right_flank', right_flank)    \n",
    "\n",
    "        self.instance_norm = nn.InstanceNorm1d(num_features=self.vocab_len, affine=self.affine)  \n",
    "        \n",
    "    def forward(self):\n",
    "        return self.get_samples_as_input()\n",
    "\n",
    "    def get_distributions(self):\n",
    "        normalized_logits = self.instance_norm(self.differentiable_logits) + \\\n",
    "            self.noise_factor * torch.randn_like(self.differentiable_logits)\n",
    "        return F.softmax(normalized_logits, dim=1)\n",
    "\n",
    "    def get_samples(self):\n",
    "        softmaxed_logits = self.get_distributions()\n",
    "        nucleotide_probs = Categorical(torch.transpose(softmaxed_logits, 1, 2))\n",
    "        sampled_idxs = nucleotide_probs.sample((self.num_st_samples, ))\n",
    "        sampled_nucleotides_T = F.one_hot(sampled_idxs, num_classes=self.vocab_len)        \n",
    "        sampled_nucleotides = torch.transpose(sampled_nucleotides_T, -2, -1)\n",
    "        multi_softmaxed_logits = softmaxed_logits.repeat(self.num_st_samples, 1, 1, 1)\n",
    "        sampled_nucleotides = sampled_nucleotides - multi_softmaxed_logits.detach() + multi_softmaxed_logits #ST estimator trick \n",
    "        return sampled_nucleotides\n",
    "        \n",
    "    def get_samples_as_input(self):\n",
    "        sampled_nucleotides = self.get_samples()\n",
    "        sampled_nucleotides = self.pad(sampled_nucleotides)\n",
    "        sampled_nucleotides = sampled_nucleotides.view(self.num_st_samples * self.num_sequences, self.vocab_len, -1)\n",
    "        return sampled_nucleotides\n",
    "\n",
    "    def get_padded_distributions(self):\n",
    "        softmaxed_logits = self.get_distributions()\n",
    "        return self.pad(softmaxed_logits, multi_sampling=False)\n",
    "\n",
    "    def rebatch(self, input):\n",
    "        return input.unflatten(0, (self.num_st_samples, self.num_sequences))\n",
    "\n",
    "    def pad(self, tensor, multi_sampling=True):\n",
    "        if self.padding_len > 0:\n",
    "            if not multi_sampling:\n",
    "                size = tensor.shape\n",
    "                left_flank, right_flank = self.left_flank[0].view(size), self.right_flank[0].view(size)\n",
    "                return torch.cat([ left_flank, tensor, right_flank], dim=-1)\n",
    "            else:\n",
    "                return torch.cat([ self.left_flank, tensor, self.right_flank], dim=-1)\n",
    "        else: \n",
    "            return tensor\n",
    "\n",
    "    def create_differentiable_input_logits(self, one_hot=True):\n",
    "        size = (self.num_sequences, self.vocab_len, self.seq_len)\n",
    "        if one_hot:\n",
    "            differentiable_logits = np.zeros(size)\n",
    "            for seqIdx in range(self.num_sequences):\n",
    "                for step in range(self.seq_len):\n",
    "                    randomNucleotide = np.random.randint(self.vocab_len)\n",
    "                    differentiable_logits[seqIdx, randomNucleotide, step] = 1       \n",
    "            self.differentiable_logits = nn.Parameter(torch.tensor(differentiable_logits, dtype=torch.float))  \n",
    "        else:\n",
    "            self.differentiable_logits = nn.Parameter(torch.randn(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastSeqProp(nn.Module):\n",
    "    def __init__(self,\n",
    "                 fitness_fn,\n",
    "                 params,\n",
    "                 loss_fn,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.fitness_fn = fitness_fn\n",
    "        self.params = params\n",
    "        self.loss_fn = loss_fn                              \n",
    "\n",
    "        try: fitness_fn.eval()\n",
    "        except: pass\n",
    "    \n",
    "    def optimize(self, steps=20, learning_rate=0.5, step_print=5, lr_scheduler=True, noise_factor=0):          \n",
    "        self.params.noise_factor = noise_factor\n",
    "        if lr_scheduler:\n",
    "            etaMin = 1e-6\n",
    "        else:\n",
    "            etaMin = learning_rate\n",
    "        optimizer = torch.optim.Adam(self.params.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps, eta_min=etaMin)      \n",
    "        loss_hist = []\n",
    "        for step in range(1, steps+1):\n",
    "            optimizer.zero_grad()\n",
    "            sampled_nucleotides = self.params()\n",
    "            fitnesses = self.fitness_fn(sampled_nucleotides)\n",
    "            loss = self.loss_fn(fitnesses)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            loss_hist.append(loss.item())\n",
    "            self.params.noise_factor = self.params.noise_factor / np.sqrt(step)\n",
    "            if step % step_print == 0:\n",
    "                print(f'step: {step}, loss: {round(loss.item(),6)}, learning_rate: {scheduler.get_last_lr()}, noise factor: {self.params.noise_factor}')                      \n",
    "        self.params.noise_factor = 0\n",
    "        self.loss_hist = loss_hist\n",
    "        plt.plot(loss_hist)\n",
    "        plt.xlabel('Steps')\n",
    "        vert_label=plt.ylabel('Loss')\n",
    "        vert_label.set_rotation(90)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 20, loss: 0.010841, learning_rate: [0.4877641535455303], noise factor: 3.205587942683902e-11\n",
      "step: 40, loss: 0.009268, learning_rate: [0.4522543440852398], noise factor: 5.535380382431696e-26\n",
      "step: 60, loss: 0.008678, learning_rate: [0.3969465191804924], noise factor: 5.481287471136054e-43\n",
      "step: 80, loss: 0.008742, learning_rate: [0.3272545940852398], noise factor: 1.8689864082375674e-61\n",
      "step: 100, loss: 0.008662, learning_rate: [0.2500005000000002], noise factor: 5.175689055878133e-81\n",
      "step: 120, loss: 0.008546, learning_rate: [0.17274640591476056], noise factor: 1.9331835044937796e-101\n",
      "step: 140, loss: 0.008423, learning_rate: [0.10305448081950795], noise factor: 1.3627462951100945e-122\n",
      "step: 160, loss: 0.008324, learning_rate: [0.04774665591476038], noise factor: 2.3027239819684916e-144\n",
      "step: 180, loss: 0.008346, learning_rate: [0.012236846454469772], noise factor: 1.1155378025706445e-166\n",
      "step: 200, loss: 0.008297, learning_rate: [1e-06], noise factor: 1.7804332011346016e-189\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdjklEQVR4nO3da5Bcd33m8e/TN11HFpbGxliyLdsCr7bWJmLWmDKXuHbxWuQiCBuwlwKHy2pVaxdLpdjCWbao7FL7wmHDC4iJVrBKQhZjNhVUqAqBTYiDNwE7GhH5ImOZQcjRRLY0kgFJSNZMT//2xTk9c6Z1Wu6RdLrH6udTNdXd/3NO90+nW/30/39uigjMzMxalXpdgJmZzU0OCDMzy+WAMDOzXA4IMzPL5YAwM7NclV4XcD4tX748rrrqql6XYWb2irFz587DETGYN+2CCoirrrqK4eHhXpdhZvaKIem5dtM8xGRmZrkcEGZmlssBYWZmuRwQZmaWywFhZma5HBBmZpar0ICQdJukPZJGJN2TM329pCck7ZI0LOnNmWn7JD3ZnFZknWZmdrrCjoOQVAbuA94OjAI7JG2LiKczs30X2BYRIel64P8C12Wm3xIRh4uqESAi+KO/HuH6lUt522tzjxUxM+tLRfYgbgRGImJvRIwDDwDrszNExPGYviDFIqDrF6eQxOb/t5eHnznU7Zc2M5vTigyIy4H9mcejadsMkt4l6Rngm8CHMpMCeEjSTkkb2r2IpA3p8NTw2NjYWRU6ODCPseOnzmpZM7MLVZEBoZy203oIEbE1Iq4D3gl8OjPp5ohYC6wD7pL01rwXiYjNETEUEUODg2c3RDS4eB5jxxwQZmZZRQbEKLAy83gFcKDdzBHxCHCNpOXp4wPp7SFgK8mQVSEGB+Zx2AFhZjZDkQGxA1gtaZWkGnA7sC07g6RrJSm9vxaoAUckLZI0kLYvAm4Fniqq0MEB9yDMzFoVthdTRNQl3Q08CJSBLRGxW9LGdPom4N3AByRNACeB96Z7NF0KbE2zowLcHxHfLqrWwYF5HDtV5+T4JAtq5aJexszsFaXQ031HxHZge0vbpsz9e4F7c5bbC9xQZG1Zg4vnAXD4+ClWXrywWy9rZjan+Uhqkh4EwCEPM5mZTXFAMB0QY8de6nElZmZzhwOCbEC4B2Fm1uSAAJYtmkdJDggzsywHBFAuiYsX+WhqM7MsB0TKx0KYmc3kgEg5IMzMZnJApC5xQJiZzeCASDXP6Dp99nEzs/7mgEgtW1RjYjI4+lK916WYmc0JDojU/GpyDqbxeqPHlZiZzQ0OiFS1nFy+YmLSAWFmBg6IKdVysiocEGZmCQdEygFhZjaTAyI1HRDei8nMDBwQU7wNwsxsJgdEykNMZmYzOSBSHmIyM5vJAZHyEJOZ2UyFBoSk2yTtkTQi6Z6c6eslPSFpl6RhSW/udNnzzUNMZmYzFRYQksrAfcA6YA1wh6Q1LbN9F7ghIl4PfAj40iyWPa+aATFe9xCTmRkU24O4ERiJiL0RMQ48AKzPzhARx2P67HiLgOh02fOtVkmGmOoN9yDMzKDYgLgc2J95PJq2zSDpXZKeAb5J0ovoeNnzqVLyEJOZWVaRAaGcttPGbyJia0RcB7wT+PRslgWQtCHdfjE8NjZ2trVSraQB4SEmMzOg2IAYBVZmHq8ADrSbOSIeAa6RtHw2y0bE5ogYioihwcHBsy62uRfTuHsQZmZAsQGxA1gtaZWkGnA7sC07g6RrJSm9vxaoAUc6WfZ8q6ZDTHUHhJkZAJWinjgi6pLuBh4EysCWiNgtaWM6fRPwbuADkiaAk8B7043WucsWVStkhph8oJyZGVBgQABExHZge0vbpsz9e4F7O122SB5iMjObyUdSp6rei8nMbAYHRKpUEuWSqHuIycwMcEDMUC3LPQgzs5QDIqNaLnkbhJlZygGRUS2XPMRkZpZyQGR4iMnMbJoDIsNDTGZm0xwQGbVyyQfKmZmlHBAZlbJ8qg0zs5QDIqNaLnkbhJlZygGRkWyD8BCTmRk4IGaolUtM1N2DMDMDB8QMlbJ8yVEzs5QDIsNDTGZm0xwQGVUPMZmZTXFAZFQ9xGRmNsUBkVH1gXJmZlMcEBnVcolxDzGZmQEOiBlqFZ+sz8ysyQGRUSmVqDc8xGRmBgUHhKTbJO2RNCLpnpzp75P0RPr3fUk3ZKbtk/SkpF2Shouss8l7MZmZTasU9cSSysB9wNuBUWCHpG0R8XRmtp8Cb4uIn0laB2wG3piZfktEHC6qxlbViny6bzOzVJE9iBuBkYjYGxHjwAPA+uwMEfH9iPhZ+vBRYEWB9bysaskn6zMzayoyIC4H9mcej6Zt7XwY+FbmcQAPSdopaUO7hSRtkDQsaXhsbOycCq6WSzQCJr0dwsysuCEmQDltud+8km4hCYg3Z5pvjogDki4BviPpmYh45LQnjNhMMjTF0NDQOX2zVytJyROTDcql8rk8lZnZK16RPYhRYGXm8QrgQOtMkq4HvgSsj4gjzfaIOJDeHgK2kgxZFapWTlaHh5nMzIoNiB3AakmrJNWA24Ft2RkkXQF8HXh/RDybaV8kaaB5H7gVeKrAWgGolJIeRN1HU5uZFTfEFBF1SXcDDwJlYEtE7Ja0MZ2+CfgUsAz4giSAekQMAZcCW9O2CnB/RHy7qFqbqhX3IMzMmorcBkFEbAe2t7Rtytz/CPCRnOX2Aje0thetmg4xeVdXMzMfST3D9DYIDzGZmTkgMirl5jYI9yDMzBwQGR5iMjOb5oDI8BCTmdk0B0RGc4jJezGZmTkgZqj6QDkzsykOiIyqh5jMzKY4IDKmtkH4mhBmZg6IrKndXBsOCDMzB0TG9G6uHmIyM3NAZHiIycxsmgMiw7u5mplNc0BkTO3F5CvKmZk5ILI8xGRmNs0BkZG95KiZWb9zQGRUSj6S2sysyQGRUZ3aSO1tEGZmDogMSVTLcg/CzAwHxGmq5ZIDwswMB8RpKiV5iMnMjIIDQtJtkvZIGpF0T87090l6Iv37vqQbOl22KLWKexBmZlBgQEgqA/cB64A1wB2S1rTM9lPgbRFxPfBpYPMsli2Eh5jMzBJF9iBuBEYiYm9EjAMPAOuzM0TE9yPiZ+nDR4EVnS5blErZQ0xmZlBsQFwO7M88Hk3b2vkw8K3ZLitpg6RhScNjY2PnUG7CPQgzs0SRAaGcttyf5pJuIQmIT8x22YjYHBFDETE0ODh4VoVm1RwQZmYAVAp87lFgZebxCuBA60ySrge+BKyLiCOzWbYISQ/CQ0xmZkX2IHYAqyWtklQDbge2ZWeQdAXwdeD9EfHsbJYtSsUHypmZAR32ICQtAk5GREPSa4HrgG9FxES7ZSKiLulu4EGgDGyJiN2SNqbTNwGfApYBX5AEUE+Hi3KXPft/Zue8DcLMLNHpENMjwFskvQr4LjAMvBd435kWiojtwPaWtk2Z+x8BPtLpst1QK5c4OTHZ7Zc1M5tzOh1iUkScAH4L+HxEvIvk+IQLjoeYzMwSHQeEpDeR9Bi+mbYVuYG7Z7yR2sws0WlAfAz4PWBruh3hauDhwqrqIe/mamaW6KgXEBHfA74HIKkEHI6IjxZZWK/4dN9mZomOehCS7pe0JN2b6Wlgj6T/XGxpvVEpl3xNajMzOh9iWhMRR4F3kuxZdAXw/qKK6qVqucREw9sgzMw6DYiqpCpJQHwjPf7hgvwWrXmIycwM6Dwg/hewD1gEPCLpSuBoUUX1koeYzMwSnW6k/hzwuUzTc+kJ9i443s3VzCzR6UbqiyR9tnlabUl/SNKbuODUymKi0SDCIWFm/a3TIaYtwDHgPenfUeBPiiqql6rlEhEw6Q3VZtbnOj0a+pqIeHfm8X+TtKuAenquUk4yc2IyqJR7XIyZWQ912oM4KenNzQeSbgZOFlNSb1XLybWKJhreUG1m/a3THsRG4MuSLkof/wy4s5iSeqtWSXsQ3pPJzPpcp3sxPQ7cIGlJ+viopI8BTxRYW09UM0NMZmb9bFZXlIuIo+kR1QC/W0A9PVcppUNMPljOzPrcuVxyVOetijlkaojJAWFmfe5cAuKCHIPxEJOZWeKM2yAkHSM/CAQsKKSiHvMQk5lZ4owBERED3SpkrqimQ0zjDggz63PnMsT0siTdJmmPpBFJ9+RMv07SDySdkvTxlmn7JD0paZek4SLrzKqlQ0x1DzGZWZ8r7LrSksrAfcDbgVFgh6RtEfF0ZrYXgY+SnEY8zy0RcbioGvNMb4NwD8LM+luRPYgbgZGI2BsR48ADwPrsDBFxKCJ2ABMF1jErlfRIag8xmVm/KzIgLgf2Zx6Ppm2dCuAhSTslbWg3k6QNzbPMjo2NnWWp0zzEZGaWKDIg8o6TmM237s0RsRZYB9wl6a15M0XE5ogYioihwcHBs6lzBg8xmZkligyIUWBl5vEK4ECnC0fEgfT2ELCVZMiqcM0hJgeEmfW7IgNiB7Ba0ipJNeB2YFsnC0paJGmgeR+4FXiqsEozmkNM4z5Zn5n1ucL2YoqIuqS7gQeBMrAlInZL2phO3yTp1cAwsARopCcAXAMsB7ZKatZ4f0R8u6has5pDTHVfMMjM+lxhAQEQEduB7S1tmzL3XyAZemp1FLihyNraqXqIycwMKPhAuVeiioeYzMwAB8Rpaj5Zn5kZ4IA4TXOIqe4hJjPrcw6IFuWSkLwNwszMAdFCEtVSiXEPMZlZn3NA5KiW5SEmM+t7Dogc1UrJQ0xm1vccEDkqHmIyM3NA5KmV5R6EmfU9B0SOaqXkbRBm1vccEDmq5ZIPlDOzvueAyFEpyVeUM7O+54DIUfNeTGZmDog81XLJlxw1s77ngMjhISYzMwdELg8xmZk5IHJ5iMnMzAGRq+oD5czMHBB5KuWSt0GYWd9zQOSolb0Nwsys0ICQdJukPZJGJN2TM/06ST+QdErSx2ezbJGS0317G4SZ9bfCAkJSGbgPWAesAe6QtKZltheBjwL/8yyWLUzFPQgzs0J7EDcCIxGxNyLGgQeA9dkZIuJQROwAJma7bJFq5RLjdQeEmfW3IgPicmB/5vFo2nZel5W0QdKwpOGxsbGzKrRVsheTh5jMrL8VGRDKaev0W7fjZSNic0QMRcTQ4OBgx8WdSbVcot5wD8LM+luRATEKrMw8XgEc6MKy56ySnu47wr0IM+tfRQbEDmC1pFWSasDtwLYuLHvOauWkA+NhJjPrZ5Winjgi6pLuBh4EysCWiNgtaWM6fZOkVwPDwBKgIeljwJqIOJq3bFG1tqqWk9ysNxrUfKiImfWpwgICICK2A9tb2jZl7r9AMnzU0bLd0gyIiXpArRcVmJn1nn8e56imQ0w+3YaZ9TMHRI6pHoQDwsz6mAMiR62SrJZTPljOzPqYAyLHwPwqAMdfqve4EjOz3nFA5FgyP9l2f/Sl1jOAmJn1DwdEjiULkh7E0ZMOCDPrXw6IHFMB4R6EmfUxB0SOqSGmk94GYWb9ywGRY1GtggTH3IMwsz7mgMhRKomBeRWOei8mM+tjDog2liyoeiO1mfU1B0QbS+ZXvZHazPqaA6KNJQsq3khtZn3NAdGGexBm1u8cEG0MzK9yzBupzayPOSDaSIaY3IMws/7lgGhjyfwqx07VmWz4sqNm1p8cEG00T7fhM7qaWb9yQLThM7qaWb9zQLThE/aZWb8rNCAk3SZpj6QRSffkTJekz6XTn5C0NjNtn6QnJe2SNFxknXkGfMI+M+tzlaKeWFIZuA94OzAK7JC0LSKezsy2Dlid/r0R+OP0tumWiDhcVI1nsmS+exBm1t+K7EHcCIxExN6IGAceANa3zLMe+HIkHgWWSrqswJo6dpEvGmRmfa7IgLgc2J95PJq2dTpPAA9J2ilpQ7sXkbRB0rCk4bGxsfNQdqLZg/DBcmbWr4oMCOW0tR5UcKZ5bo6ItSTDUHdJemvei0TE5ogYioihwcHBs6+2xWLvxWRmfa7IgBgFVmYerwAOdDpPRDRvDwFbSYasuqbcvCaEN1KbWZ8qMiB2AKslrZJUA24HtrXMsw34QLo3003ALyLieUmLJA0ASFoE3Ao8VWCtuQbmV9yDMLO+VdheTBFRl3Q38CBQBrZExG5JG9Ppm4DtwDuAEeAE8MF08UuBrZKaNd4fEd8uqtZ2Llkyn/0vnuj2y5qZzQmFBQRARGwnCYFs26bM/QDuylluL3BDkbV14voVF/GXO0eZbATlUt7mEjOzC5ePpD6DG1Ys5Zfjk/xk7HivSzEz6zoHxBncsHIpALv2/7yndZiZ9YID4gyuXr6IgXkVnhj9ea9LMTPrOgfEGZRK4l+suIjH9/+i16WYmXWdA+JlXL9iKc+8cJSXJiZ7XYqZWVc5IF7GG658FROTwV8/c6jXpZiZdZUD4mXc8rpBVl+ymD/49jOM1xu9LsfMrGscEC+jUi7xX37tn7HvyAm+/IN9vS7HzKxrHBAd+NXXDnLL6wb5zIN72H3AG6zNrD84IDogic/89g0sXVjlP37lh/zsl+O9LsnMrHAOiA4tXzyPL7xvLc//4iXu+OKjHD5+qtclmZkVygExC2+48mL+5Hf+Jc8dOcFvfP5veWj3CySnkzIzu/A4IGbp5muX87X/cBMXLaiy4c938u+++BiP7j3ioDCzC44upC+2oaGhGB4e7sprTUw2+D+PPsd9D49w+Pg4q5Yv4oM3X8V7hlYyv1ruSg1mZudK0s6IGMqd5oA4NyfG62x/8gW+8thz/MM//px5lRKrL13M6y5dwnWvHuCfv2YJa16zhKULa12ty8ysEw6ILogIHvvpi/zV0wfZc/AYzx48xsGj0xuyL7toPlcuW8iVFy9i5cULuHTJfC5ZMp/li2ssqJY5MT7JwaMvcejYKRZUy1x7yWKuvWSxeyNmVqgzBUShFwzqJ5K46epl3HT1sqm2I8dP8fTzR9l94CjPvnCM5148wXefOdTxHlAlwcqLF3LlskWUBfVGMF5vUG8EE5MNJiaT20pJXHbRfBbPr1Iti2qpRL0R1BsNFtbKLKxVWFgrUyuXmFctUSuXqJRLVEqiXBKVsiiXMo+nbkuZ6a3tAEICASU17ye3yTpJ1oua9zPz0/JYEqW0DTGjvXX55nNHpH8EjYBGRNqW3DYiCJJbAhqZeWNq3ux7OPN+87XJvP70/eaElrrSmqfvt3xOyL5Iy7TT5s1OU9tpp9V+2tTTnztP63s09Z52srBdkBwQBVq2eB5vWT3IW1YPzmh/aWKSQ0dPcfDYSxw+dopT9QYLamUuGZjHJUvm88tTdZ49eIxnDx7nJ4eO84/pZU8rZVEtl1hQLTMwv0K1nHzZn6o3eP4XJ3nuyAnGJxtpaCRf7icnJjlxqs6JiUkuoM6i9UDbMGc6XErZYM2GvNLAoRlW2R8H2WmZED4PuXS+PvOt4Zn9N5L8/khfL/lR0vzh0nz9vB8i0z8q1PL45UO5OfLTfK1XLarxjbtuPqd/Yx4HRA/Mr5a5YtlCrli2sO08r7104Ly+ZkRQbwSn6o20F9JgshHUJyO5bTRvGzMfT7Zpb8T0h7T5y7zR/MAm/0nI/Cdp/U8T6YJT7VO/9jntefOWh5lfStlfu8mv3+kvMjJfaKVm7yTzhZT+H5/xn65Z+3QdzKhpRnvLf9bm+m79bsp+SZw+rf03Weuk1mc+0/PmLX/a9BlfZDPXd2PqvTz9i6/ZIyOm11+2vXVdZJ8v0pXc7MU1Wp6/+Zznpe9yrk+S/fdBZv3EVI2n9zZbQoTk8zf13s28Oe0L/2XKmdE7FTAwv3pO/8R2HBB9QlIy/FQuwbxeV2NmrwSFHgch6TZJeySNSLonZ7okfS6d/oSktZ0ua2ZmxSosICSVgfuAdcAa4A5Ja1pmWwesTv82AH88i2XNzKxARfYgbgRGImJvRIwDDwDrW+ZZD3w5Eo8CSyVd1uGyZmZWoCID4nJgf+bxaNrWyTydLAuApA2ShiUNj42NnXPRZmaWKDIg8vYdaN0+326eTpZNGiM2R8RQRAwNDg7mzWJmZmehyL2YRoGVmccrgAMdzlPrYFkzMytQkT2IHcBqSask1YDbgW0t82wDPpDuzXQT8IuIeL7DZc3MrECF9SAioi7pbuBBoAxsiYjdkjam0zcB24F3ACPACeCDZ1q2qFrNzOx0F9TJ+iSNAc+d5eLLgcPnsZzzxXXN3lytzXXNjuuavbOp7cqIyN2Ae0EFxLmQNNzujIa95Lpmb67W5rpmx3XN3vmuzVeUMzOzXA4IMzPL5YCYtrnXBbThumZvrtbmumbHdc3eea3N2yDMzCyXexBmZpbLAWFmZrn6PiDmynUnJK2U9LCkH0naLek/pe2/L+mfJO1K/97Ro/r2SXoyrWE4bbtY0nck/Ti9fVWXa3pdZr3sknRU0sd6sc4kbZF0SNJTmba260fS76WfuT2S/k0PavuMpGfS67BslbQ0bb9K0snMutvU5bravnfdWmdt6vpapqZ9knal7d1cX+2+I4r7nCWXCezPP5KjtH8CXE1y/qfHgTU9quUyYG16fwB4luRaGL8PfHwOrKt9wPKWtj8A7knv3wPc2+P38gXgyl6sM+CtwFrgqZdbP+n7+jjJtf1WpZ/BcpdruxWopPfvzdR2VXa+Hqyz3Peum+ssr66W6X8IfKoH66vdd0Rhn7N+70HMmetORMTzEfHD9P4x4Ee0OcX5HLIe+LP0/p8B7+xdKfwr4CcRcbZH0p+TiHgEeLGlud36WQ88EBGnIuKnJKeaubGbtUXEQxFRTx8+SnJCzK5qs87a6do6O1NdkgS8B/hqEa99Jmf4jijsc9bvAdHxdSe6SdJVwK8Aj6VNd6dDAVu6PYyTEcBDknZK2pC2XRrJyRVJby/pUW2QnNAx+592Lqyzdutnrn3uPgR8K/N4laR/kPQ9SW/pQT15791cWWdvAQ5GxI8zbV1fXy3fEYV9zvo9IDq+7kS3SFoM/CXwsYg4SnIZ1muA1wPPk3Rve+HmiFhLchnYuyS9tUd1nEbJGX9/E/iLtGmurLN25sznTtIngTrwlbTpeeCKiPgV4HeB+yUt6WJJ7d67ubLO7mDmD5Gur6+c74i2s+a0zWqd9XtAdHLNiq6RVCV5478SEV8HiIiDETEZEQ3gixQ4FHEmEXEgvT0EbE3rOKjkErGkt4d6URtJaP0wIg6mNc6JdUb79TMnPneS7gR+HXhfpIPW6XDEkfT+TpJx69d2q6YzvHc9X2eSKsBvAV9rtnV7feV9R1Dg56zfA2LOXHciHdv838CPIuKzmfbLMrO9C3iqddku1LZI0kDzPskGzqdI1tWd6Wx3At/odm2pGb/q5sI6S7VbP9uA2yXNk7QKWA38fTcLk3Qb8AngNyPiRKZ9UFI5vX91WtveLtbV7r3r+ToD/jXwTESMNhu6ub7afUdQ5OesG1vf5/IfyfUoniVJ/k/2sI43k3T/ngB2pX/vAP4ceDJt3wZc1oParibZG+JxYHdzPQHLgO8CP05vL+5BbQuBI8BFmbaurzOSgHoemCD55fbhM60f4JPpZ24PsK4HtY2QjE83P2ub0nnfnb7HjwM/BH6jy3W1fe+6tc7y6krb/xTY2DJvN9dXu++Iwj5nPtWGmZnl6vchJjMza8MBYWZmuRwQZmaWywFhZma5HBBmZpbLAWF2FiR9Mj2j5hPpWTzfqORMsgt7XZvZ+eLdXM1mSdKbgM8CvxoRpyQtJzkb8PeBoYg43NMCzc4T9yDMZu8y4HBEnAJIA+HfAq8BHpb0MICkWyX9QNIPJf1Feg6d5rU17pX09+nftWn7b0t6StLjkh7pzT/NbJp7EGazlH7R/y3JUdx/BXwtIr4naR9pDyLtVXyd5OjVX0r6BDAvIv57Ot8XI+J/SPoA8J6I+HVJTwK3RcQ/SVoaET/vxb/PrMk9CLNZiojjwBuADcAY8DVJv9My200kF2z5u/TqY3eSXMyo6auZ2zel9/8O+FNJ/57kAkhmPVXpdQFmr0QRMQn8DfA36S//O1tmEfCdiLij3VO03o+IjZLeCPwasEvS6yM9U6hZL7gHYTZLSq6FvTrT9HrgOeAYyaUgIblK282Z7QsLJWVPA/3ezO0P0nmuiYjHIuJTwGFmnqrZrOvcgzCbvcXA5yUtJbnYzgjJcNMdwLckPR8Rt6TDTl+VNC9d7r+SnDkYYJ6kx0h+pDV7GZ9Jg0ckZ+V8vBv/GLN2vJHarMuyG7N7XYvZmXiIyczMcrkHYWZmudyDMDOzXA4IMzPL5YAwM7NcDggzM8vlgDAzs1z/H/QLwklnxFCiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create parameters\n",
    "params = FSP_Parameteres(num_sequences=100,\n",
    "                        num_st_samples=20,\n",
    "                        affine=True,\n",
    "                        onehot_start=False)\n",
    "\n",
    "#create generator\n",
    "generator = FastSeqProp(fitness_fn=model,\n",
    "                        params=params,\n",
    "                        loss_fn=utils.shannon_entropy_mean)\n",
    "generator.cuda()\n",
    "\n",
    "#optimize parameters\n",
    "generator.optimize(steps=200,\n",
    "                   learning_rate=0.5,\n",
    "                   step_print=20,\n",
    "                   lr_scheduler=True,\n",
    "                   noise_factor=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = params.get_samples_as_input()\n",
    "predictions = model(samples)\n",
    "avg_preds = params.rebatch(predictions).mean(dim=0)\n",
    "distributions = params.get_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.9037, -0.8753, -0.9666],\n",
       "        [ 5.8349, -0.8168, -0.7947],\n",
       "        [ 5.7480, -0.9180, -0.9763],\n",
       "        [ 5.8773, -0.8818, -0.9727],\n",
       "        [ 5.7991, -0.5571, -0.5105],\n",
       "        [ 5.9983, -0.8575, -0.8413],\n",
       "        [ 5.9465, -0.9657, -1.0392],\n",
       "        [ 5.8168, -0.7355, -0.7128],\n",
       "        [ 5.8046, -0.9048, -1.0015],\n",
       "        [ 5.8657, -0.9192, -0.9852],\n",
       "        [ 5.9493, -0.8801, -0.8473],\n",
       "        [ 5.9880, -0.9104, -0.9877],\n",
       "        [ 5.9051, -0.9309, -1.0493],\n",
       "        [ 5.8876, -0.8957, -0.9283],\n",
       "        [ 5.8848, -0.9168, -0.9543],\n",
       "        [ 5.6005, -0.8775, -0.9423],\n",
       "        [ 5.9397, -0.8636, -0.7724],\n",
       "        [ 5.9156, -0.8145, -0.7326],\n",
       "        [ 5.9196, -0.9074, -1.0214],\n",
       "        [ 5.8784, -0.8345, -0.9318],\n",
       "        [ 5.8677, -0.8606, -0.9517],\n",
       "        [ 5.8953, -0.8786, -0.8118],\n",
       "        [ 5.9178, -0.8752, -0.8669],\n",
       "        [ 6.0605, -0.9129, -0.9933],\n",
       "        [ 5.9182, -0.8703, -0.9220],\n",
       "        [ 5.8031, -0.8515, -0.8777],\n",
       "        [ 5.8570, -0.9274, -0.9959],\n",
       "        [ 5.9643, -0.9374, -1.0782],\n",
       "        [ 0.0536,  1.6797,  5.0628],\n",
       "        [ 5.9151, -0.8992, -0.9778],\n",
       "        [ 5.6877, -0.6378, -0.5426],\n",
       "        [ 5.8302, -0.8339, -0.9248],\n",
       "        [ 0.3313,  2.2748,  5.7170],\n",
       "        [ 5.7765, -0.8576, -0.8233],\n",
       "        [ 5.8306, -0.8928, -0.8438],\n",
       "        [ 5.9792, -0.9666, -1.0704],\n",
       "        [ 5.9017, -0.8900, -0.9340],\n",
       "        [ 5.7926, -0.9261, -0.9735],\n",
       "        [ 5.8529, -0.8662, -0.9035],\n",
       "        [ 5.8693, -0.8491, -0.8428],\n",
       "        [ 5.9014, -0.9101, -1.0301],\n",
       "        [ 6.0744, -0.9524, -1.0292],\n",
       "        [ 5.7899, -0.7799, -0.7089],\n",
       "        [ 5.9436, -0.8878, -0.8375],\n",
       "        [ 5.7505, -0.7722, -0.8081],\n",
       "        [ 5.8706, -0.9000, -0.9722],\n",
       "        [ 5.8811, -0.8619, -0.8797],\n",
       "        [ 5.7787, -0.8630, -0.8966],\n",
       "        [ 5.8643, -0.8982, -0.9704],\n",
       "        [ 5.9313, -0.9087, -0.9350],\n",
       "        [ 5.9147, -0.8876, -0.8982],\n",
       "        [ 5.9231, -0.8940, -0.9399],\n",
       "        [ 5.8981, -0.8317, -0.9012],\n",
       "        [ 5.9555, -0.9192, -0.9463],\n",
       "        [ 5.9118, -0.8522, -0.7998],\n",
       "        [ 5.9146, -0.8509, -0.7089],\n",
       "        [ 5.9650, -0.9599, -1.1240],\n",
       "        [ 5.8917, -0.8466, -0.8402],\n",
       "        [ 5.8730, -0.8382, -0.8202],\n",
       "        [ 5.9323, -0.8818, -0.8835],\n",
       "        [ 5.7614, -0.8536, -0.8892],\n",
       "        [ 5.9861, -0.9418, -1.0514],\n",
       "        [ 5.8953, -0.8881, -1.0466],\n",
       "        [ 5.9366, -0.9056, -0.9019],\n",
       "        [ 5.8330, -0.7704, -0.7337],\n",
       "        [ 5.8759, -0.9083, -0.9770],\n",
       "        [ 5.6632, -0.7508, -0.8213],\n",
       "        [ 5.6623, -0.6289, -0.5541],\n",
       "        [ 5.8338, -0.4028, -0.4236],\n",
       "        [ 0.0944,  2.0400,  5.3906],\n",
       "        [ 5.9807, -0.9322, -1.0661],\n",
       "        [ 5.9680, -0.9022, -0.9262],\n",
       "        [ 5.8818, -0.9260, -1.0399],\n",
       "        [ 0.2431,  2.3209,  5.6386],\n",
       "        [ 5.7222, -0.6035, -0.5448],\n",
       "        [ 5.9049, -0.8194, -0.9926],\n",
       "        [ 5.8245, -0.7809, -0.8713],\n",
       "        [ 5.9189, -0.8953, -0.8680],\n",
       "        [ 5.7576, -0.7904, -0.8156],\n",
       "        [ 5.8927, -0.8522, -0.8580],\n",
       "        [ 5.8414, -0.8830, -0.8896],\n",
       "        [ 5.8703, -0.7630, -0.8238],\n",
       "        [ 5.9416, -0.7483, -0.6397],\n",
       "        [ 5.8847, -0.8605, -0.8410],\n",
       "        [ 6.0110, -0.8869, -0.9361],\n",
       "        [ 5.7731, -0.7257, -0.8143],\n",
       "        [ 6.0273, -0.9482, -1.1245],\n",
       "        [ 5.9312, -0.9147, -0.9824],\n",
       "        [ 5.8713, -0.8732, -0.9786],\n",
       "        [ 5.8564, -0.8478, -0.9058],\n",
       "        [ 5.8472, -0.5335, -0.4291],\n",
       "        [ 5.7910, -0.7835, -0.8523],\n",
       "        [ 5.9412, -0.8715, -0.7996],\n",
       "        [ 5.7936, -0.8110, -0.9357],\n",
       "        [ 5.9290, -0.8089, -0.8427],\n",
       "        [ 5.8345, -0.6898, -0.6167],\n",
       "        [ 5.8640, -0.7288, -0.7805],\n",
       "        [ 5.7122, -0.7581, -0.7156],\n",
       "        [ 5.7196, -0.7272, -0.6029],\n",
       "        [ 5.8668, -0.8214, -0.8294]], device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
