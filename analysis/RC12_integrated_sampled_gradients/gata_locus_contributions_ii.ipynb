{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "minimal-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import tarfile\n",
    "import shutil\n",
    "import random\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (random_split, DataLoader, TensorDataset, ConcatDataset)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits import mplot3d\n",
    "from Bio import motifs\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "import boda\n",
    "from boda.common import constants, utils\n",
    "\n",
    "boda_src = os.path.join( os.path.dirname( os.path.dirname( os.getcwd() ) ), 'src' )\n",
    "sys.path.insert(0, boda_src)\n",
    "\n",
    "from main import unpack_artifact, model_fn\n",
    "from pymeme import streme, parse_streme_output\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "from boda.generator.plot_tools import matrix_to_dms, ppm_to_IC, ppm_to_pwm, counts_to_ppm\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-supplement",
   "metadata": {},
   "source": [
    "### Get coordinates of the last window of the previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "temporal-wesley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d288520ff4e4b93af52d8f9e072ea74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff16a4dd8a594078a124565dc4efa955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rootdir = 'gata_locus/contributions_v1'\n",
    "\n",
    "chunk_dicts = {}\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in tqdm(files):\n",
    "        full_coordinates = file.split('__')[-1].rstrip('.pt')\n",
    "        chunk_dicts[full_coordinates] = torch.load(os.path.join(rootdir, file))\n",
    "        \n",
    "all_window_coordinates = [coordinate for key in sorted(chunk_dicts.keys()) for coordinate in chunk_dicts[key]['window_coordinates']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "neural-alaska",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chrX:49644750-49644949'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_window_coordinates[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legendary-bargain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrX:49644750-49880599\n"
     ]
    }
   ],
   "source": [
    "real_end_idx = 49880397 + 202\n",
    "second_file_coordinates = f'chrX:49644750-{real_end_idx}'\n",
    "print(second_file_coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-judgment",
   "metadata": {},
   "source": [
    "### Now get the contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "egyptian-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "archive unpacked in ./\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from 20211113_021200 in eval mode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir('./artifacts'):\n",
    "    shutil.rmtree('./artifacts')\n",
    "hpo_rec = 'gs://syrgoth/aip_ui_test/model_artifacts__20211113_021200__287348.tar.gz'\n",
    "unpack_artifact(hpo_rec)\n",
    "\n",
    "model_dir = './artifacts'\n",
    "model = model_fn(model_dir)\n",
    "#model.cuda()\n",
    "model.eval()\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "advance-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mpra_predictor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 pred_idx=0,\n",
    "                 ini_in_len=200,\n",
    "                 model_in_len=600,\n",
    "                 cat_axis=-1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.pred_idx = pred_idx\n",
    "        self.ini_in_len = ini_in_len \n",
    "        self.model_in_len = model_in_len\n",
    "        self.cat_axis = cat_axis       \n",
    "        \n",
    "        try: self.model.eval()\n",
    "        except: pass\n",
    "        \n",
    "        self.register_flanks()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pieces = [self.left_flank.repeat(x.shape[0], 1, 1), x, self.right_flank.repeat(x.shape[0], 1, 1)]\n",
    "        in_tensor = torch.cat( pieces, axis=self.cat_axis)\n",
    "        out_tensor = self.model(in_tensor)[:, self.pred_idx]\n",
    "        return out_tensor\n",
    "    \n",
    "    def register_flanks(self):\n",
    "        missing_len = self.model_in_len - self.ini_in_len\n",
    "        left_idx = - missing_len//2 + missing_len%2\n",
    "        right_idx = missing_len//2 + missing_len%2\n",
    "        left_flank = utils.dna2tensor(constants.MPRA_UPSTREAM[left_idx:]).unsqueeze(0)\n",
    "        right_flank = utils.dna2tensor(constants.MPRA_DOWNSTREAM[:right_idx]).unsqueeze(0)         \n",
    "        self.register_buffer('left_flank', left_flank)\n",
    "        self.register_buffer('right_flank', right_flank) \n",
    "\n",
    "        \n",
    "def df_to_onehot_tensor(in_df, seq_column='sequence'):\n",
    "    onehot_sequences = torch.stack([utils.dna2tensor(subsequence) \\\n",
    "                                for subsequence in tqdm(in_df[seq_column])])\n",
    "    return onehot_sequences\n",
    "\n",
    "def fasta_to_tensor(file_name):\n",
    "    fasta_dict = {}\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            line_str = str(line)\n",
    "            if line_str[0] == '>':\n",
    "                my_id = line_str.lstrip('>').rstrip('\\n')\n",
    "                fasta_dict[my_id] = ''\n",
    "            else:\n",
    "                fasta_dict[my_id] += line_str.rstrip('\\n')\n",
    "    seq_tensors = []\n",
    "    for sequence in list(fasta_dict.values()):\n",
    "        seq_tensors.append(utils.dna2tensor(sequence))\n",
    "    return torch.stack(seq_tensors, dim=0)\n",
    "\n",
    "def dna2tensor_approx(sequence_str, vocab_list=constants.STANDARD_NT, N_value=0.25):\n",
    "    seq_tensor = np.zeros((len(vocab_list), len(sequence_str)))\n",
    "    for letterIdx, letter in enumerate(sequence_str):\n",
    "        try:\n",
    "            seq_tensor[vocab_list.index(letter), letterIdx] = 1\n",
    "        except:\n",
    "            seq_tensor[:, letterIdx] = N_value\n",
    "    seq_tensor = torch.Tensor(seq_tensor)\n",
    "    return seq_tensor\n",
    "\n",
    "def frame_print(string, marker='*', left_space=25):\n",
    "    left_spacer = left_space * ' '\n",
    "    string = marker + ' ' + string.upper() + ' ' + marker\n",
    "    n = len(string)\n",
    "    print('', flush=True)\n",
    "    print('', flush=True)\n",
    "    print(left_spacer + n * marker, flush=True)\n",
    "    print(left_spacer + string, flush=True)\n",
    "    print(left_spacer + n * marker, flush=True)\n",
    "    print('', flush=True)\n",
    "    print('', flush=True)\n",
    "    \n",
    "def decor_print(string):\n",
    "    decor = 15*'-'\n",
    "    print('', flush=True)\n",
    "    print(decor + ' ' + string + ' ' + decor, flush=True)\n",
    "    print('', flush=True)\n",
    "    \n",
    "def isg_contributions(sequences,\n",
    "                      predictor,\n",
    "                      num_steps=50,\n",
    "                      num_samples=20,\n",
    "                      eval_batch_size=1024,\n",
    "                      theta_factor=15):\n",
    "    \n",
    "    batch_size = eval_batch_size // num_samples\n",
    "    temp_dataset = TensorDataset(sequences)\n",
    "    temp_dataloader = DataLoader(temp_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    all_salient_maps = []\n",
    "    for local_batch in tqdm(temp_dataloader):\n",
    "        target_thetas = (theta_factor * local_batch[0].cuda()).requires_grad_()\n",
    "        line_gradients = []\n",
    "        for i in range(0, num_steps + 1):\n",
    "            point_thetas = (i / num_steps * target_thetas)\n",
    "            point_distributions = F.softmax(point_thetas, dim=-2)\n",
    "\n",
    "            nucleotide_probs = Categorical(torch.transpose(point_distributions, -2, -1))\n",
    "            sampled_idxs = nucleotide_probs.sample((num_samples, ))\n",
    "            sampled_nucleotides_T = F.one_hot(sampled_idxs, num_classes=4)\n",
    "            sampled_nucleotides = torch.transpose(sampled_nucleotides_T, -2, -1)\n",
    "            distribution_repeater = point_distributions.repeat(num_samples, *[1 for i in range(3)])\n",
    "            sampled_nucleotides = sampled_nucleotides - distribution_repeater.detach() + distribution_repeater \n",
    "            samples = sampled_nucleotides.flatten(0,1)\n",
    "\n",
    "            preds = predictor(samples)\n",
    "            point_predictions = preds.unflatten(0, (num_samples, target_thetas.shape[0])).mean(dim=0)\n",
    "            point_gradients = torch.autograd.grad(point_predictions.sum(), inputs=point_thetas, retain_graph=True)[0]\n",
    "            line_gradients.append(point_gradients)\n",
    "            \n",
    "        gradients = torch.stack(line_gradients).mean(dim=0) \n",
    "        all_salient_maps.append(gradients * target_thetas)\n",
    "    return torch.cat(all_salient_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "grateful-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "k562_predictor = mpra_predictor(model=model, pred_idx=0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "combined-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235840 235840\n"
     ]
    }
   ],
   "source": [
    "#! gsutil cp gs://syrgoth/data/locus_select/chrX-49,644,750-49,880,599.txt ./\n",
    "\n",
    "left_jump = 10\n",
    "gata_chr = 'X'\n",
    "\n",
    "gata_locus_file = 'chrX-49,644,750-49,880,599.txt'\n",
    "gata_locus_str = ''\n",
    "with open(gata_locus_file) as f:\n",
    "    for line in f:\n",
    "        if line[0] != '>':\n",
    "            gata_locus_str += line.strip()\n",
    "\n",
    "gata_locus_str = gata_locus_str[left_jump:]         \n",
    "print(len(gata_locus_str), len(range(49644750+left_jump, 49880600)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "intended-carter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 235840])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gata_locus_tensor = dna2tensor_approx(gata_locus_str, N_value=0.)\n",
    "gata_locus_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "qualified-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create windows\n",
    "window_len = 200\n",
    "step_size = 10\n",
    "locus_tensor_windows = [gata_locus_tensor[:, start:start+window_len] for start in range(0, gata_locus_tensor.shape[1]-window_len+1, step_size)]\n",
    "locus_tensor_windows = torch.stack(locus_tensor_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "limiting-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "gata_locus_start = 49644750 + left_jump\n",
    "\n",
    "windows_coordinates = [f'chr{gata_chr}:{gata_locus_start + start}-{gata_locus_start + start + window_len-1}' for start in range(0, gata_locus_tensor.shape[1]-window_len+1, step_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "lucky-elevation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1020, 4, 200])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_example = locus_tensor_windows[:1020,...]\n",
    "chunk_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "sapphire-tucson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved at gata_locus/contributions_v1\n",
      "\n",
      "\n",
      "--------------- Processing chunk 1/3 ---------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267f94e4ba9445b5ac6a25bf941f3285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contributions saved in gata_locus/contributions_v1/gata_locus_contributions__k562__window_len_200__step_size_10__chrX:49644760-49744969.pt\n",
      "\n",
      "Chunk processing time: 0:16:38.003676\n",
      "\n",
      "Estimated time remaining: 0:33:16.007352\n",
      "\n",
      "\n",
      "--------------- Processing chunk 2/3 ---------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6871a7bf5997441eb5b191d3b91bf5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contributions saved in gata_locus/contributions_v1/gata_locus_contributions__k562__window_len_200__step_size_10__chrX:49744780-49844989.pt\n",
      "\n",
      "Chunk processing time: 0:16:39.534007\n",
      "\n",
      "Estimated time remaining: 0:16:39.534007\n",
      "\n",
      "\n",
      "--------------- Processing chunk 3/3 ---------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c30799e79bc46a89e99dadcab2141eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contributions saved in gata_locus/contributions_v1/gata_locus_contributions__k562__window_len_200__step_size_10__chrX:49844800-49880599.pt\n",
      "\n",
      "Chunk processing time: 0:05:56.229789\n",
      "\n",
      "Estimated time remaining: 0:00:00\n",
      "\n",
      "CPU times: user 39min 12s, sys: 3.04 s, total: 39min 15s\n",
      "Wall time: 39min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_tensor = locus_tensor_windows #locus_tensor_windows\n",
    "chunk_size = 10002 #10002 #204\n",
    "eval_batch_size = 1040\n",
    "\n",
    "cell_type = 'k562'\n",
    "targetdir = 'gata_locus/contributions_v1'\n",
    "\n",
    "print(f'Results will be saved at {targetdir}', flush=True)\n",
    "print('', flush=True)\n",
    "\n",
    "num_chunks = math.ceil(data_tensor.shape[0] / chunk_size)\n",
    "processed_chunks = 0\n",
    "for i in range(0, data_tensor.shape[0], chunk_size):\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    decor_print(f'Processing chunk {processed_chunks+1}/{num_chunks}')\n",
    "        \n",
    "    chunk = data_tensor[i:i+chunk_size, ...]    \n",
    "    \n",
    "    salient_maps = isg_contributions(chunk, k562_predictor, eval_batch_size=eval_batch_size)\n",
    "    coordinate_list = windows_coordinates[i:i+chunk.shape[0]]\n",
    "    \n",
    "    save_dict = {}\n",
    "    save_dict['window_contributions'] = salient_maps\n",
    "    save_dict['window_coordinates'] = coordinate_list\n",
    "    \n",
    "    first_coordinate = coordinate_list[0].split('-')[0]\n",
    "    last_coordinate = coordinate_list[-1].split('-')[1]\n",
    "    chunk_name = f'gata_locus_contributions__{cell_type}__window_len_{window_len}__step_size_{step_size}'\n",
    "    chunk_name += f'__{first_coordinate}-{last_coordinate}' + '.pt'\n",
    "    \n",
    "    save_path = os.path.join(targetdir, chunk_name)   \n",
    "    torch.save(save_dict, save_path)\n",
    "    \n",
    "    print(f'Contributions saved in {save_path}')\n",
    "    print('', flush=True)\n",
    "    \n",
    "    processed_chunks += 1\n",
    "    left_chunks = num_chunks - processed_chunks\n",
    "    end_time = datetime.now()\n",
    "    chunk_time = end_time - start_time\n",
    "    \n",
    "    print(f'Chunk processing time: {chunk_time}', flush=True)\n",
    "    print('', flush=True)\n",
    "    print(f'Estimated time remaining: {chunk_time*left_chunks}', flush=True)\n",
    "    print('', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-opinion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
