{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liable-discovery",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collectible-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "def create_custom_job_sample(\n",
    "    project: str = 'sabeti-encode',\n",
    "    display_name: str = 'simple-test',\n",
    "    container_image_uri: str = 'gcr.io/sabeti-encode/boda/production:0.0.3',\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "    custom_job = {\n",
    "        \"display_name\": display_name,\n",
    "        \"job_spec\": {\n",
    "            \"worker_pool_specs\": [\n",
    "                {\n",
    "                    \"machine_spec\": {\n",
    "                        \"machine_type\": \"n1-standard-4\",\n",
    "                        \"accelerator_type\": aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_V100,\n",
    "                        \"accelerator_count\": 1,\n",
    "                    },\n",
    "                    \"replica_count\": 1,\n",
    "                    \"container_spec\": {\n",
    "                        \"image_uri\": 'gcr.io/sabeti-encode/boda/production:0.0.3',\n",
    "                        \"command\": [],\n",
    "                        \"args\": [\n",
    "                            '--data_module=BODA2_DataModule',\n",
    "                            '--datafile_path=gs://syrgoth/data/BODA.MPRA.txt',\n",
    "                            '--valid_pct=5',\n",
    "                            '--test_pct=5',\n",
    "                            '--batch_size=32',\n",
    "                            '--padded_seq_len=600',\n",
    "                            '-num_workers=1',\n",
    "                            '--model_module=Basset',\n",
    "                            '--n_outputs=3',\n",
    "                            '--loss_criterion=MSELoss',\n",
    "                            '--graph_module=CNNBasicTraining',\n",
    "                            '--gpus=1',\n",
    "                            '--min_epochs=5',\n",
    "                            '--max_epochs=5',\n",
    "                            '--default_root_dir=/tmp/output/artifacts',\n",
    "                            '--artifact_path=gs://haddath/sgosai/deposit_test'\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "    parent = f\"projects/{project}/locations/{location}\"\n",
    "    response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "    print(\"response:\", response)\n",
    "    \n",
    "    return client, custom_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "great-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_job(client, name):\n",
    "    try:\n",
    "        response = clients[\"job\"].cancel_custom_job(name=name)\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addressed-kenya",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: name: \"projects/482032041325/locations/us-central1/customJobs/9074955559291060224\"\n",
      "display_name: \"simple-test\"\n",
      "job_spec {\n",
      "  worker_pool_specs {\n",
      "    machine_spec {\n",
      "      machine_type: \"n1-standard-4\"\n",
      "      accelerator_type: NVIDIA_TESLA_V100\n",
      "      accelerator_count: 1\n",
      "    }\n",
      "    replica_count: 1\n",
      "    disk_spec {\n",
      "      boot_disk_type: \"pd-ssd\"\n",
      "      boot_disk_size_gb: 100\n",
      "    }\n",
      "    container_spec {\n",
      "      image_uri: \"gcr.io/sabeti-encode/boda/production:0.0.3\"\n",
      "      args: \"--data_module=BODA2_DataModule\"\n",
      "      args: \"--datafile_path=gs://syrgoth/data/BODA.MPRA.txt\"\n",
      "      args: \"--valid_pct=5\"\n",
      "      args: \"--test_pct=5\"\n",
      "      args: \"--batch_size=32\"\n",
      "      args: \"--padded_seq_len=600\"\n",
      "      args: \"-num_workers=1\"\n",
      "      args: \"--model_module=Basset\"\n",
      "      args: \"--n_outputs=3\"\n",
      "      args: \"--loss_criterion=MSELoss\"\n",
      "      args: \"--graph_module=CNNBasicTraining\"\n",
      "      args: \"--gpus=1\"\n",
      "      args: \"--min_epochs=5\"\n",
      "      args: \"--max_epochs=5\"\n",
      "      args: \"--default_root_dir=/tmp/output/artifacts\"\n",
      "      args: \"--artifact_path=gs://haddath/sgosai/deposit_test\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "state: JOB_STATE_PENDING\n",
      "create_time {\n",
      "  seconds: 1620942836\n",
      "  nanos: 305639000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1620942836\n",
      "  nanos: 305639000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "j_client, j_specs = create_custom_job_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-product",
   "metadata": {},
   "source": [
    "# Training a BODA model with a CustomJob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-habitat",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This tutorial demonstrates how to use the AI Platform (Unified) Python client library to train a custom MPRA sequence function model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-speech",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We will be using a private MPRA dataset stashed at `gs://syrgoth/data/BODA.MPRA.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "juvenile-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conscious-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'sabeti-encode'\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = 'syrgoth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "elegant-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "golden-number",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://syrgoth/checkpoints/\r\n",
      "                                 gs://syrgoth/data/\r\n",
      "                                 gs://syrgoth/my_test/\r\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-suggestion",
   "metadata": {},
   "source": [
    "#### AI Platform constants\n",
    "\n",
    "Set some constants for AI Platform:\n",
    "\n",
    "- API_ENDPOINT: The AI Platform API service endpoint for the Job, Model, Endpoint, and Prediction services.\n",
    "- PARENT: The AI Platform location root path for dataset, model and endpoint resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API service endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "# AI Platform (Unified) location root path for your dataset, model and endpoint resources\n",
    "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-cycle",
   "metadata": {},
   "source": [
    "#### Machine Type\n",
    "\n",
    "Next, set the machine type to use for training and prediction.\n",
    "\n",
    "- Set the variables TRAIN_COMPUTE  to configure the compute resources for the VMs you will use for training.\n",
    "    - machine type\n",
    "        - n1-standard: 3.75GB of memory per vCPU.\n",
    "        - n1-highmem: 6.5GB of memory per vCPU\n",
    "        - n1-highcpu: 0.9 GB of memory per vCPU\n",
    "    - vCPUs: number of [2, 4, 8, 16, 32, 64, 96 ]\n",
    "\n",
    "Note: The following is not supported for training\n",
    "\n",
    "- standard: 2 vCPUs\n",
    "- highcpu: 2, 4 and 8 vCPUs\n",
    "\n",
    "Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs\n",
    "\n",
    "#### Hardware Accelerators\n",
    "\n",
    "Set the hardware accelerators (e.g., GPU), if any, for training and prediction.\n",
    "\n",
    "Set the variables TRAIN_GPU/TRAIN_NGPU and DEPLOY_GPU/DEPLOY_NGPU to use a container image supporting a GPU and the number of GPUs allocated the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, specify:\n",
    "\n",
    "(aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "For GPU, available accelerators include:\n",
    "\n",
    "- aip.AcceleratorType.NVIDIA_TESLA_K80\n",
    "- aip.AcceleratorType.NVIDIA_TESLA_P100\n",
    "- aip.AcceleratorType.NVIDIA_TESLA_P4\n",
    "- aip.AcceleratorType.NVIDIA_TESLA_T4\n",
    "- aip.AcceleratorType.NVIDIA_TESLA_V100\n",
    "\n",
    "Otherwise specify (None, None) to use a container image to run on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "TRAIN_GPU, TRAIN_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_V100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-cricket",
   "metadata": {},
   "source": [
    "### Clients\n",
    "The AI Platform Python client library works as a client/server model. On your side, the Python script, you will create a client that sends requests and receives responses from the server -- AI Platform.\n",
    "\n",
    "Use several clients in this tutorial, so you will set these up upfront.\n",
    "\n",
    "- Job Service for custom jobs.\n",
    "- Model Service for managed models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client options same for all services\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "predict_client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "\n",
    "def create_job_client():\n",
    "    client = aip.JobServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_model_client():\n",
    "    client = aip.ModelServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "clients = {}\n",
    "clients[\"job\"] = create_job_client()\n",
    "clients[\"model\"] = create_model_client()\n",
    "\n",
    "for client in clients.items():\n",
    "    print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-phenomenon",
   "metadata": {},
   "source": [
    "## Prepare your `CustomJob` specification\n",
    "\n",
    "Now that your clients are ready, your first step is to create a `CustomJob` specification for your custom training job.\n",
    "\n",
    "To practice using the Job service, start by training an **empty job**. In other words, create a `CustomJob` specification that provisions resources for training a job, and initiate the job using the client library's Job service, but configure the `CustomJob` so it doesn't actually train an ML model.\n",
    "\n",
    "This lets you focus on understanding the basic steps. Afterwards, you can create another `CustomJob` with a focus on adding the Python training package for training a CIFAR10 custom model.\n",
    "\n",
    "### Define a container specification\n",
    "\n",
    "Let's first start by defining a job name and then a container specification:\n",
    "\n",
    "- `JOB_NAME`: A unique name for your custom training job. For convenience, append a timestamp to make the name unique.\n",
    "- `MODEL_DIR`: A location in your Cloud Storage bucket for storing the model artificats.\n",
    "- `image_uri`: The location of the container image in Artifact Registry, Container Registry, or Docker Hub. This can be either a Google Cloud pre-built image or your own container image.\n",
    "- `--model-dir`: A command-line parameter to the container indicating the location to store the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
