{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "parliamentary-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import tempfile\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset, ConcatDataset, Dataset\n",
    "\n",
    "import boda\n",
    "from boda.common import constants, utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "medical-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNAActivityDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dna_tensor, activity_tensor, sort_tensor=None, \n",
    "                 duplication_cutoff=None, use_reverse_complements=False):\n",
    "        self.dna_tensor = dna_tensor\n",
    "        self.activity_tensor = activity_tensor\n",
    "        self.duplication_cutoff = duplication_cutoff\n",
    "        self.use_reverse_complements = use_reverse_complements\n",
    "        \n",
    "        self.n_examples   = self.dna_tensor.shape[0]\n",
    "        self.n_duplicated = 0\n",
    "        \n",
    "        if duplication_cutoff is not None:\n",
    "            _, sort_order = torch.sort(sort_tensor, descending=True, stable=True)\n",
    "            self.dna_tensor = dna_tensor[sort_order]\n",
    "            self.activity_tensor = self.activity_tensor[sort_order]\n",
    "            \n",
    "            self.n_duplicated = (sort_tensor >= duplication_cutoff).sum().item()\n",
    "        \n",
    "    def __len__(self):\n",
    "        dataset_len = self.dna_tensor.shape[0] + self.n_duplicated\n",
    "        if self.use_reverse_complements:\n",
    "            dataset_len = 2 * dataset_len\n",
    "        return dataset_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(f\"index {idx} is out of bounds for dataset with size {len(self)}\")\n",
    "        if idx < 0:\n",
    "            if -idx > len(self):\n",
    "                raise ValueError(f\"absolute value of {idx} is out of bounds for dataset with size {len(self)}\")\n",
    "            \n",
    "        if self.use_reverse_complements:\n",
    "            take_rc = idx % 2 == 1\n",
    "            item_idx= (idx // 2) % self.n_examples\n",
    "        else:\n",
    "            take_rc = False            \n",
    "            item_idx= idx % self.n_examples\n",
    "            \n",
    "        dna      = self.dna_tensor[item_idx]\n",
    "        activity = self.activity_tensor[item_idx]\n",
    "\n",
    "        if take_rc:\n",
    "            dna = utils.reverse_complement_onehot(dna)\n",
    "        \n",
    "        return dna, activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compatible-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPRA_DataModule(pl.LightningDataModule):\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_data_specific_args(parent_parser):\n",
    "        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        group  = parser.add_argument_group('Data Module args')\n",
    "        \n",
    "        group.add_argument('--datafile_path', type=str, required=True)\n",
    "        group.add_argument('--data_project', type=str, nargs='+', default=['BODA', 'UKBB', 'GTEX'])\n",
    "        group.add_argument('--project_column', type=str, default='data_project')\n",
    "        group.add_argument('--sequence_column', type=str, default='nt_sequence')\n",
    "        group.add_argument('--activity_columns', type=str, nargs='+', default=['K562_mean', 'HepG2_mean', 'SKNSH_mean'])\n",
    "        group.add_argument('--exclude_chr_train', type=str, nargs='+', default=[''])\n",
    "        group.add_argument('--val_chrs', type=str, nargs='+', default=['19','21','X'])\n",
    "        group.add_argument('--test_chrs', type=str, nargs='+', default=['7','13'])\n",
    "        group.add_argument('--chr_column', type=str, default='chr')\n",
    "        group.add_argument('--std_multiple_cut', type=float, default=6.0)\n",
    "        group.add_argument('--up_cutoff_move', type=float, default=3.0)\n",
    "        group.add_argument('--synth_chr', type=str, default='synth')\n",
    "        group.add_argument('--synth_val_pct', type=float, default=10.0)\n",
    "        group.add_argument('--synth_test_pct', type=float, default=10.0)\n",
    "        group.add_argument('--synth_seed', type=int, default=0)\n",
    "        group.add_argument('--batch_size', type=int, default=32, \n",
    "                           help='Number of examples in each mini batch')         \n",
    "        group.add_argument('--padded_seq_len', type=int, default=600, \n",
    "                           help='Desired total sequence length after padding') \n",
    "        group.add_argument('--num_workers', type=int, default=8, \n",
    "                           help='number of gpus or cpu cores to be used') \n",
    "        group.add_argument('--normalize', type=utils.str2bool, default=False, \n",
    "                           help='apply standard score normalization') \n",
    "        return parser\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_conditional_args(parser, known_args):\n",
    "        return parser\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_args(grouped_args):\n",
    "        data_args    = grouped_args['Data Module args']\n",
    "        return data_args\n",
    "\n",
    "    def __init__(self,\n",
    "                 datafile_path,\n",
    "                 data_project=['BODA', 'UKBB', 'GTEX'],\n",
    "                 project_column='data_project',\n",
    "                 sequence_column='nt_sequence',\n",
    "                 activity_columns=['K562_mean', 'HepG2_mean', 'SKNSH_mean'],\n",
    "                 exclude_chr_train=[''],\n",
    "                 val_chrs=['19','21','X'],\n",
    "                 test_chrs=['7','13'],\n",
    "                 chr_column='chr',\n",
    "                 std_multiple_cut=6.0,\n",
    "                 up_cutoff_move=4.0,\n",
    "                 synth_chr='synth',\n",
    "                 synth_val_pct=10.0,\n",
    "                 synth_test_pct=10.0,\n",
    "                 synth_seed=0,\n",
    "                 batch_size=32,\n",
    "                 padded_seq_len=600, \n",
    "                 num_workers=8,\n",
    "                 normalize=False,\n",
    "                 duplication_cutoff=None,\n",
    "                 use_reverse_complements=False,\n",
    "                 **kwargs):       \n",
    "        \"\"\"\n",
    "        Takes a .txt file with a column cotaining DNA sequences,\n",
    "        column(s) containing log2FC, and a chromosome column.\n",
    "        Preprocesses, tokenizes, creates Train/Val/Test dataloaders.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        datafile_path : str\n",
    "            Path to the .txt file with the data (space-separated)..\n",
    "        data_project : str, optional\n",
    "            DESCRIPTION. The default is ['BODA', 'UKBB'].\n",
    "        project_column : str, optional\n",
    "            DESCRIPTION. The default is 'data_project'.\n",
    "        sequence_column : str, optional\n",
    "            Name of the column of the DNA sequences. The default is 'nt_sequence'.\n",
    "        activity_columns : list, optional\n",
    "            List of names of the columns with log2FC. The default is ['K562_mean', 'HepG2_mean', 'SKNSH_mean'].\n",
    "        exclude_chr_train : list, optional\n",
    "            List of chromosomes to be excluded from train. The default is [''].\n",
    "        val_chrs : list, optional\n",
    "            DESCRIPTION. The default is ['17','19','21','X'].\n",
    "        test_chrs : list, optional\n",
    "            DESCRIPTION. The default is ['7','13'].\n",
    "        chr_column : str, optional\n",
    "            Name of the column of the chromosome number. The default is 'chr'.\n",
    "        std_multiple_cut : float, optional\n",
    "            DESCRIPTION. The default is 6.0.\n",
    "        up_cutoff_move : float, optional\n",
    "            DESCRIPTION. The default is 3.0.\n",
    "        synth_chr : str, optional\n",
    "            DESCRIPTION. The default is 'synth'.\n",
    "        synth_val_pct : float, optional\n",
    "            DESCRIPTION. The default is 10.0.\n",
    "        synth_test_pct : float, optional\n",
    "            DESCRIPTION. The default is 10.0.\n",
    "        synth_seed : int, optional\n",
    "            DESCRIPTION. The default is 0.\n",
    "        batch_size : int, optional\n",
    "            Number of examples in each mini batch. The default is 32.\n",
    "        padded_seq_len : int, optional\n",
    "            Desired total sequence length after padding. The default is 600.\n",
    "        num_workers : int, optional\n",
    "            number of gpus or cpu cores to be used, right?. The default is 8.\n",
    "        normalize : bool, optional\n",
    "            DESCRIPTION. The default is False.\n",
    "        duplication_cutoff: float, optional\n",
    "            All sequences with max activity across cell types above this value will be\n",
    "            duplicated during training.\n",
    "        use_reverse_complements: bool, optional\n",
    "            If true, reverse complements of training sequences will be added to \n",
    "            training set.\n",
    "        **kwargs : TYPE\n",
    "            DESCRIPTION.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.datafile_path = datafile_path\n",
    "        self.data_project = data_project\n",
    "        self.project_column = project_column\n",
    "        self.sequence_column = sequence_column\n",
    "        self.activity_columns = activity_columns\n",
    "        self.exclude_chr_train = set(exclude_chr_train) - {''}\n",
    "        self.val_chrs = set(val_chrs) - {''}\n",
    "        self.test_chrs = set(test_chrs) - {''}\n",
    "        self.chr_column = chr_column\n",
    "        self.std_multiple_cut = std_multiple_cut\n",
    "        self.up_cutoff_move = up_cutoff_move\n",
    "        self.synth_chr = synth_chr\n",
    "        self.synth_val_pct = synth_val_pct\n",
    "        self.synth_test_pct = synth_test_pct\n",
    "        self.synth_seed = synth_seed\n",
    "        self.batch_size = batch_size\n",
    "        self.padded_seq_len = padded_seq_len        \n",
    "        self.num_workers = num_workers\n",
    "        self.normalize = normalize\n",
    "        self.duplication_cutoff = duplication_cutoff\n",
    "        self.use_reverse_complements = use_reverse_complements\n",
    "        \n",
    "        self.pad_column_name = 'padded_seq'\n",
    "        self.tensor_column_name = 'onehot_seq'\n",
    "        self.activity_means = None\n",
    "        self.activity_stds = None\n",
    "        self.synth_chr_as_set = {synth_chr}\n",
    "        \n",
    "        self.padding_fn = partial(utils.row_pad_sequence,\n",
    "                                  in_column_name=self.sequence_column,\n",
    "                                  padded_seq_len=self.padded_seq_len\n",
    "                                  )\n",
    "        self.tokenize_fn = partial(utils.row_dna2tensor,\n",
    "                                   in_column_name=self.pad_column_name\n",
    "                                   )\n",
    "        self.chr_dataset_train = None\n",
    "        self.chr_dataset_val = None\n",
    "        self.chr_dataset_test = None\n",
    "        self.synth_dataset_train = None\n",
    "        self.synth_dataset_val = None\n",
    "        self.synth_dataset_test = None\n",
    "                \n",
    "    def setup(self, stage='train'):\n",
    "        #--------- parse data from MPRA file ---------\n",
    "        columns = [self.sequence_column, *self.activity_columns, self.chr_column, self.project_column]\n",
    "        temp_df = utils.parse_file(file_path=self.datafile_path, columns=columns)\n",
    "\n",
    "        temp_df = temp_df[temp_df[self.project_column].isin(self.data_project)].reset_index(drop=True)\n",
    "        \n",
    "        #--------- cut-off and standard score norm ---------\n",
    "        means = temp_df[self.activity_columns].mean().to_numpy()\n",
    "        stds  = temp_df[self.activity_columns].std().to_numpy()\n",
    "        \n",
    "        up_cut   = means + stds * self.std_multiple_cut + self.up_cutoff_move\n",
    "        down_cut = means - stds * self.std_multiple_cut \n",
    "        \n",
    "        non_extremes_filter_up = (temp_df[self.activity_columns] < up_cut).to_numpy().all(axis=1)\n",
    "        temp_df = temp_df.loc[non_extremes_filter_up]\n",
    "        \n",
    "        non_extremes_filter_down = (temp_df[self.activity_columns] > down_cut).to_numpy().all(axis=1)\n",
    "        temp_df = temp_df.loc[non_extremes_filter_down]\n",
    "        \n",
    "        self.num_examples = len(temp_df)\n",
    "        if self.normalize:   \n",
    "            temp_df[self.activity_columns] = (temp_df[self.activity_columns] - means) / stds\n",
    "            self.activity_means = torch.Tensor(means)\n",
    "            self.activity_stds = torch.Tensor(stds)        \n",
    "        \n",
    "        #--------- print cut-off info ---------\n",
    "        print('-'*50)\n",
    "        print('')\n",
    "        for idx, cell in enumerate(self.activity_columns):\n",
    "            cell_name = cell.rstrip('_mean')\n",
    "            top_cut_value = round(up_cut[idx], 2)\n",
    "            bottom_cut_value = round(down_cut[idx], 2)\n",
    "            print(f'{cell_name} | top cut value: {top_cut_value}, bottom cut value: {bottom_cut_value}')\n",
    "        print('')    \n",
    "        num_up_cuts   = np.sum(~non_extremes_filter_up)\n",
    "        num_down_cuts = np.sum(~non_extremes_filter_down)\n",
    "        print(f'Number of examples discarded from top: {num_up_cuts}')\n",
    "        print(f'Number of examples discarded from bottom: {num_down_cuts}')\n",
    "        print('')\n",
    "        print(f'Number of examples available: {self.num_examples}')\n",
    "        print('')\n",
    "        print('-'*50)\n",
    "        print('')\n",
    "        \n",
    "        #--------- pad sequences, convert to one-hots ---------\n",
    "        print('Padding sequences...')\n",
    "        temp_df[self.pad_column_name] = temp_df.apply(self.padding_fn, axis=1)\n",
    "        print('Tokenizing sequences...')\n",
    "        \n",
    "        temp_df[self.tensor_column_name] = temp_df.apply(self.tokenize_fn, axis=1)\n",
    "        \n",
    "        #--------- split dataset in train/val/test sets ---------\n",
    "        print('Creating train/val/test datasets...')\n",
    "        all_chrs = set(temp_df[self.chr_column])\n",
    "        self.train_chrs = all_chrs - self.val_chrs - self.test_chrs - self.synth_chr_as_set - self.exclude_chr_train\n",
    "        \n",
    "        if len(self.train_chrs) > 0:\n",
    "            sequences_train  = list(temp_df[temp_df[self.chr_column].isin(self.train_chrs)][self.tensor_column_name])\n",
    "            activities_train = temp_df[temp_df[self.chr_column].isin(self.train_chrs)][self.activity_columns].to_numpy()\n",
    "            sequences_train  = torch.stack(sequences_train)\n",
    "            activities_train = torch.Tensor(activities_train)    \n",
    "            self.chr_dataset_train = TensorDataset(sequences_train, activities_train)\n",
    "            self.chr_dataset_train = DNAActivityDataset(sequences_train, activities_train, \n",
    "                                                        sort_tensor=torch.max(activities_train, dim=-1).values, \n",
    "                                                        duplication_cutoff=self.duplication_cutoff, \n",
    "                                                        use_reverse_complements=self.use_reverse_complements)\n",
    "        \n",
    "        if len(self.val_chrs) > 0:\n",
    "            sequences_val  = list(temp_df[temp_df[self.chr_column].isin(self.val_chrs)][self.tensor_column_name])\n",
    "            activities_val = temp_df[temp_df[self.chr_column].isin(self.val_chrs)][self.activity_columns].to_numpy()\n",
    "            sequences_val  = torch.stack(sequences_val)\n",
    "            activities_val = torch.Tensor(activities_val)  \n",
    "            self.chr_dataset_val = TensorDataset(sequences_val, activities_val)\n",
    "        \n",
    "        if len(self.test_chrs) > 0:\n",
    "            sequences_test    = list(temp_df[temp_df[self.chr_column].isin(self.test_chrs)][self.tensor_column_name])                      \n",
    "            activities_test   = temp_df[temp_df[self.chr_column].isin(self.test_chrs)][self.activity_columns].to_numpy()    \n",
    "            sequences_test    = torch.stack(sequences_test)        \n",
    "            activities_test   = torch.Tensor(activities_test)\n",
    "            self.chr_dataset_test = TensorDataset(sequences_test, activities_test)\n",
    "             \n",
    "        if self.synth_chr in all_chrs:\n",
    "            synth_sequences  = list(temp_df[temp_df[self.chr_column].isin(self.synth_chr_as_set)][self.tensor_column_name])\n",
    "            synth_activities = temp_df[temp_df[self.chr_column].isin(self.synth_chr_as_set)][self.activity_columns].to_numpy()\n",
    "            synth_sequences  = torch.stack(synth_sequences)\n",
    "            synth_activities = torch.Tensor(synth_activities)\n",
    "            synth_dataset = TensorDataset(synth_sequences, synth_activities)\n",
    "        \n",
    "            synth_num_examples = synth_activities.shape[0]\n",
    "            synth_val_size     = int(synth_num_examples * self.synth_val_pct // 100)\n",
    "            synth_test_size    = int(synth_num_examples * self.synth_test_pct // 100)\n",
    "            synth_train_size   = synth_num_examples - synth_val_size - synth_test_size  \n",
    "    \n",
    "            synth_dataset_split = random_split(synth_dataset,\n",
    "                                               [synth_train_size, synth_val_size, synth_test_size],\n",
    "                                               generator=torch.Generator().manual_seed(self.synth_seed))\n",
    "            self.synth_dataset_train, self.synth_dataset_val, self.synth_dataset_test = synth_dataset_split\n",
    "            \n",
    "            # Repackage training synth\n",
    "            dna, activities = list(zip(*list(self.synth_dataset_train)))\n",
    "            dna = torch.stack(dna, dim=0)\n",
    "            activities = torch.stack(activities, dim=0)\n",
    "            self.synth_dataset_train = DNAActivityDataset(dna, activities, \n",
    "                                                          sort_tensor=torch.max(activities, dim=-1).values, \n",
    "                                                          duplication_cutoff=self.duplication_cutoff, \n",
    "                                                          use_reverse_complements=self.use_reverse_complements)\n",
    "\n",
    "            \n",
    "            if self.chr_dataset_train is None:\n",
    "                if self.synth_chr not in self.exclude_chr_train:\n",
    "                    self.dataset_train = self.synth_dataset_train\n",
    "            else:\n",
    "                self.dataset_train = ConcatDataset([self.chr_dataset_train, self.synth_dataset_train])\n",
    "            if self.chr_dataset_val is None:\n",
    "                self.dataset_val = self.synth_dataset_val\n",
    "            else:\n",
    "                self.dataset_val = ConcatDataset([self.chr_dataset_val, self.synth_dataset_val])\n",
    "            if self.chr_dataset_test is None:\n",
    "                self.dataset_test = self.synth_dataset_test\n",
    "            else:\n",
    "                self.dataset_test = ConcatDataset([self.chr_dataset_test, self.synth_dataset_test])\n",
    "        else:\n",
    "            self.dataset_train = self.chr_dataset_train\n",
    "            self.dataset_val = self.chr_dataset_val\n",
    "            self.dataset_test = self.chr_dataset_test\n",
    "        \n",
    "        #--------- print train/val/test info ---------\n",
    "        if self.dataset_train is not None: self.train_size = len(self.dataset_train)\n",
    "        else: self.train_size = 0\n",
    "            \n",
    "        if self.dataset_val is not None: self.val_size = len(self.dataset_val)\n",
    "        else: self.val_size = 0\n",
    "            \n",
    "        if self.dataset_test is not None: self.test_size = len(self.dataset_test)\n",
    "        else: self.test_size = 0\n",
    "            \n",
    "        train_pct = round(100 * self.train_size / self.num_examples, 2)\n",
    "        val_pct   = round(100 * self.val_size / self.num_examples, 2)\n",
    "        test_pct  = round(100 * self.test_size / self.num_examples, 2)\n",
    "        excluded_size = self.num_examples - self.train_size - self.val_size - self.test_size\n",
    "        excluded_pct = round(100 * excluded_size / self.num_examples, 2)\n",
    "        print('-'*50)\n",
    "        print('')\n",
    "        print(f'Number of examples in train: {self.train_size} ({train_pct}%)')\n",
    "        print(f'Number of examples in val:   {self.val_size} ({val_pct}%)')\n",
    "        print(f'Number of examples in test:  {self.test_size} ({test_pct}%)')\n",
    "        print('')\n",
    "        print(f'Excluded from train: {excluded_size} ({excluded_pct})%')\n",
    "        print('-'*50)    \n",
    "                \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset_train, batch_size=self.batch_size,\n",
    "                          shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset_val, batch_size=self.batch_size,\n",
    "                          shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset_test, batch_size=self.batch_size,\n",
    "                          shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    def synth_train_dataloader(self):\n",
    "        return DataLoader(self.synth_dataset_train, batch_size=self.batch_size,\n",
    "                          shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def synth_val_dataloader(self):\n",
    "        return DataLoader(self.synth_dataset_val, batch_size=self.batch_size,\n",
    "                          shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def synth_test_dataloader(self):\n",
    "        return DataLoader(self.synth_dataset_test, batch_size=self.batch_size,\n",
    "                          shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    def chr_train_dataloader(self):\n",
    "        return DataLoader(self.chr_dataset_train, batch_size=self.batch_size,\n",
    "                          shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def chr_val_dataloader(self):\n",
    "        return DataLoader(self.chr_dataset_val, batch_size=self.batch_size,\n",
    "                          shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def chr_test_dataloader(self):\n",
    "        return DataLoader(self.chr_dataset_test, batch_size=self.batch_size,\n",
    "                          shuffle=False, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "downtown-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "K562 | top cut value: 10.95, bottom cut value: -6.0\n",
      "HepG2 | top cut value: 9.99, bottom cut value: -5.26\n",
      "SKNSH | top cut value: 10.14, bottom cut value: -5.51\n",
      "\n",
      "Number of examples discarded from top: 0\n",
      "Number of examples discarded from bottom: 8\n",
      "\n",
      "Number of examples available: 358539\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Padding sequences...\n",
      "Tokenizing sequences...\n",
      "Creating train/val/test datasets...\n",
      "--------------------------------------------------\n",
      "\n",
      "Number of examples in train: 310023 (86.47%)\n",
      "Number of examples in val:   17257 (4.81%)\n",
      "Number of examples in test:  31259 (8.72%)\n",
      "\n",
      "Excluded from train: 0 (0.0)%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "datamodule = MPRA_DataModule(datafile_path='gs://syrgoth/data/MPRA_ALL_v3.txt',\n",
    "                     data_project=['BODA', 'UKBB'],\n",
    "                     synth_seed=102202,\n",
    "                     batch_size=32,\n",
    "                     padded_seq_len=600, \n",
    "                     num_workers=1)\n",
    "datamodule.setup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
