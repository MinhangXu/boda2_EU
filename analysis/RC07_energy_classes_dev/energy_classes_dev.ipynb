{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excited-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits import mplot3d\n",
    "from Bio import motifs\n",
    "import pickle\n",
    "\n",
    "import boda\n",
    "from boda.generator.parameters import StraightThroughParameters\n",
    "from boda.generator import FastSeqProp, AdaLead\n",
    "from boda.generator.plot_tools import matrix_to_dms, ppm_to_IC, ppm_to_pwm\n",
    "from boda.model.mpra_basset import MPRA_Basset\n",
    "from boda.common import constants, utils\n",
    "from boda.generator.energy import BaseEnergy\n",
    "\n",
    "boda_src = os.path.join( os.path.dirname( os.path.dirname( os.getcwd() ) ), 'src' )\n",
    "sys.path.insert(0, boda_src)\n",
    "\n",
    "from main import unpack_artifact, model_fn\n",
    "from pymeme import streme, parse_streme_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "banned-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_complement(in_tensor, alphabet=constants.STANDARD_NT):\n",
    "    rc_dict = {'A':'T', 'G':'C', 'T':'A', 'C':'G'}\n",
    "    reversed_alphabet = [rc_dict[nt] for nt in alphabet]\n",
    "    out_tensor = align_to_alphabet(in_tensor, in_alphabet=alphabet,  out_alphabet=reversed_alphabet)\n",
    "    out_tensor = torch.flip(out_tensor, dims=[1])\n",
    "    return out_tensor\n",
    "\n",
    "def show_streme_motifs(parsed_output):\n",
    "    motif_dict = parsed_output['motif_results']\n",
    "    results_alphabet = parsed_output['meta_data']['alphabet']\n",
    "    for motif_idx in range(len(motif_dict)):\n",
    "        motif_ppm = torch.tensor((motif_dict[motif_idx]['ppm']))\n",
    "        motif_ppm = align_to_alphabet(motif_ppm, in_alphabet=results_alphabet)\n",
    "        motif_ppm_rc = reverse_complement(motif_ppm)\n",
    "        print(motif_dict[motif_idx]['summary'])\n",
    "        matrix_to_dms(ppm_to_IC(motif_ppm), y_max=2)\n",
    "        plt.show()\n",
    "        matrix_to_dms(ppm_to_IC(motif_ppm_rc), y_max=2)\n",
    "        plt.show()\n",
    "        \n",
    "def fasta_to_input_tensor(file_name, left_flank, right_flank):\n",
    "    fasta_dict = {}\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            line_str = str(line)\n",
    "            if line_str[0] == '>':\n",
    "                my_id = line_str.lstrip('>').rstrip('\\n')\n",
    "                fasta_dict[my_id] = ''\n",
    "            else:\n",
    "                fasta_dict[my_id] += line_str.rstrip('\\n')\n",
    "    seq_tensors = []\n",
    "    for sequence in list(fasta_dict.values()):\n",
    "        seq_tensors.append(utils.dna2tensor(sequence))\n",
    "    sequences = torch.stack(seq_tensors, dim=0)\n",
    "    pieces = [left_flank.repeat(sequences.shape[0], 1, 1), sequences,  right_flank.repeat(sequences.shape[0], 1, 1)]\n",
    "    return torch.cat(pieces, axis=-1)\n",
    "\n",
    "def plot3D_activities(activities_tensor, color='blue', fig_size=(15, 10), alpha=0.2, ax_lims=(-2, 8)):\n",
    "    xdata = activities_tensor[:,0].cpu().detach().numpy()\n",
    "    ydata = activities_tensor[:,1].cpu().detach().numpy()\n",
    "    zdata = activities_tensor[:,2].cpu().detach().numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    # Data for a three-dimensional line\n",
    "    xAxisLine = (ax_lims, (0, 0), (0,0))\n",
    "    ax.plot(xAxisLine[0], xAxisLine[1], xAxisLine[2], 'r')\n",
    "    yAxisLine = ((0, 0), ax_lims, (0,0))\n",
    "    ax.plot(yAxisLine[0], yAxisLine[1], yAxisLine[2], 'r')\n",
    "    zAxisLine = ((0, 0), (0,0), ax_lims)\n",
    "    ax.plot(zAxisLine[0], zAxisLine[1], zAxisLine[2], 'r')\n",
    "    dAxisLine = (ax_lims, ax_lims, ax_lims)\n",
    "    ax.plot(dAxisLine[0], dAxisLine[1], dAxisLine[2], 'gray', linestyle='dashed')\n",
    "\n",
    "    ax.scatter3D(xdata, ydata, zdata, c=color, alpha=alpha)\n",
    "    ax.set_xlabel('K562')\n",
    "    ax.set_ylabel('HepG2')\n",
    "    ax.set_zlabel('SKNSH')\n",
    "    ax.view_init(15, -45)\n",
    "    \n",
    "def unpickle_logs(log_path):\n",
    "    log_df = pd.read_pickle(log_path + 'sequence_data.pkl')\n",
    "    with open(log_path + 'pmms_list.pkl', 'rb') as fp:\n",
    "        pmms_list = pickle.load(fp)\n",
    "    return log_df, pmms_list\n",
    "\n",
    "\n",
    "def create_new_log_folder_in(super_folder):\n",
    "    log_idx = 0\n",
    "    folder_name = 'log_' + str(log_idx)\n",
    "    while os.path.isdir(super_folder + folder_name):\n",
    "        log_idx += 1\n",
    "        folder_name = 'log_' + str(log_idx)\n",
    "    log_path = super_folder + folder_name \n",
    "    os.makedirs(log_path)\n",
    "    return log_path + '/'\n",
    "\n",
    "def frame_print(string, marker='*', left_space=25):\n",
    "    left_spacer = left_space * ' '\n",
    "    string = marker + ' ' + string.upper() + ' ' + marker\n",
    "    n = len(string)\n",
    "    print('')\n",
    "    print('')\n",
    "    print(left_spacer + n * marker)\n",
    "    print(left_spacer + string)\n",
    "    print(left_spacer + n * marker)\n",
    "    print('')\n",
    "    print('')\n",
    "    \n",
    "def decor_print(string):\n",
    "    decor = 15*'-'\n",
    "    print('')\n",
    "    print(decor + ' ' + string + ' ' + decor)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handy-amendment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPRA_Basset(\n",
       "  (criterion): MSELoss()\n",
       "  (last_activation): Tanh()\n",
       "  (basset_net): Basset(\n",
       "    (pad1): ConstantPad1d(padding=(9, 9), value=0.0)\n",
       "    (conv1): Conv1dNorm(\n",
       "      (conv): Conv1d(4, 300, kernel_size=(19,), stride=(1,))\n",
       "      (bn_layer): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (pad2): ConstantPad1d(padding=(5, 5), value=0.0)\n",
       "    (conv2): Conv1dNorm(\n",
       "      (conv): Conv1d(300, 200, kernel_size=(11,), stride=(1,))\n",
       "      (bn_layer): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (pad3): ConstantPad1d(padding=(3, 3), value=0.0)\n",
       "    (conv3): Conv1dNorm(\n",
       "      (conv): Conv1d(200, 200, kernel_size=(7,), stride=(1,))\n",
       "      (bn_layer): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (pad4): ConstantPad1d(padding=(1, 1), value=0.0)\n",
       "    (maxpool_3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    (maxpool_4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (linear1): LinearNorm(\n",
       "      (linear): Linear(in_features=2600, out_features=1000, bias=True)\n",
       "      (bn_layer): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (linear2): LinearNorm(\n",
       "      (linear): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "      (bn_layer): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (output): Linear(in_features=1000, out_features=280, bias=True)\n",
       "    (nonlin): ReLU()\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (criterion): CrossEntropyLoss()\n",
       "  )\n",
       "  (output_1): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=250, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=250, out_features=1, bias=True)\n",
       "  )\n",
       "  (output_2): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=250, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=250, out_features=1, bias=True)\n",
       "  )\n",
       "  (output_3): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=250, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=250, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------- Artisanal model -----------------------\n",
    "# ! gsutil cp gs://syrgoth/checkpoints/manual_checkpoint_multioutput_lasthidden250_L1Loss_ReLU6_sneak1_double0_ACGT.ckpt ./\n",
    "\n",
    "artisan_model = MPRA_Basset(extra_hidden_size = 250)\n",
    "checkpoint = torch.load('manual_checkpoint_multioutput_lasthidden250_L1Loss_ReLU6_sneak1_double0_ACGT.ckpt')\n",
    "artisan_model.load_state_dict(checkpoint['state_dict'])\n",
    "artisan_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "other-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_flank = boda.common.utils.dna2tensor(constants.MPRA_UPSTREAM[-200:]).unsqueeze(0)\n",
    "right_flank = boda.common.utils.dna2tensor(constants.MPRA_DOWNSTREAM[:200] ).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accurate-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverMaxEnergy(BaseEnergy):\n",
    "    def __init__(self, model, bias_cell=0, bias_alpha=1.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.bias_cell = bias_cell\n",
    "        self.bias_alpha= bias_alpha\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hook = x.to(self.model.device)\n",
    "        \n",
    "        hook = self.model(hook)\n",
    "        \n",
    "        return hook[...,[ x for x in range(hook.shape[-1]) if x != self.bias_cell]].max(-1).values \\\n",
    "                 - hook[...,self.bias_cell].mul(self.bias_alpha)\n",
    "    \n",
    "    def register_penalty(self, x):\n",
    "        try:\n",
    "            self.penalty = x.type_as(self.penalty)\n",
    "        except AttributeError:\n",
    "            self.register_buffer('penalty', x)\n",
    "            \n",
    "    def streme_penalty(self, streme_output):\n",
    "        motif_data = parse_streme_output(streme_results['output'])\n",
    "        top_ppm    = motif_data['motif_results'][0]['ppm']\n",
    "       \n",
    "    \n",
    "    \n",
    "class AvgDiffEnergy(BaseEnergy):\n",
    "    def __init__(self, model, bias_cell=0, bending=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        try: self.model.eval()\n",
    "        except: pass\n",
    "        \n",
    "        self.bias_cell = bias_cell\n",
    "        self.silent_cells = np.r_\n",
    "        self.bending = bending\n",
    "        \n",
    "        self.silenced_cells = [0,1,2]\n",
    "        self.silenced_cells.remove(self.bias_cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "checked-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = AvgDiffEnergy(model=artisan_model,\n",
    "                       bias_cell=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "essential-independence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy.silenced_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "english-microphone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2]\n"
     ]
    }
   ],
   "source": [
    "silenced_cells = [0,1,2]\n",
    "silenced_cells.remove(1)\n",
    "silenced_cells = np.r_[silenced_cells]\n",
    "print(silenced_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unusual-traveler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0885, -1.4613],\n",
       "        [-0.2209, -0.2344],\n",
       "        [-1.3188, -0.7413],\n",
       "        [-0.9203,  1.6975],\n",
       "        [ 0.6752,  0.7456]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, np.r_[energy.silenced_cells]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "wound-disabled",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastSeqProp(\n",
       "  (energy_fn): AvgDiffEnergy(\n",
       "    (model): MPRA_Basset(\n",
       "      (criterion): MSELoss()\n",
       "      (last_activation): Tanh()\n",
       "      (basset_net): Basset(\n",
       "        (pad1): ConstantPad1d(padding=(9, 9), value=0.0)\n",
       "        (conv1): Conv1dNorm(\n",
       "          (conv): Conv1d(4, 300, kernel_size=(19,), stride=(1,))\n",
       "          (bn_layer): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (pad2): ConstantPad1d(padding=(5, 5), value=0.0)\n",
       "        (conv2): Conv1dNorm(\n",
       "          (conv): Conv1d(300, 200, kernel_size=(11,), stride=(1,))\n",
       "          (bn_layer): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (pad3): ConstantPad1d(padding=(3, 3), value=0.0)\n",
       "        (conv3): Conv1dNorm(\n",
       "          (conv): Conv1d(200, 200, kernel_size=(7,), stride=(1,))\n",
       "          (bn_layer): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (pad4): ConstantPad1d(padding=(1, 1), value=0.0)\n",
       "        (maxpool_3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "        (maxpool_4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "        (linear1): LinearNorm(\n",
       "          (linear): Linear(in_features=2600, out_features=1000, bias=True)\n",
       "          (bn_layer): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (linear2): LinearNorm(\n",
       "          (linear): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "          (bn_layer): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (output): Linear(in_features=1000, out_features=280, bias=True)\n",
       "        (nonlin): ReLU()\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (criterion): CrossEntropyLoss()\n",
       "      )\n",
       "      (output_1): Sequential(\n",
       "        (0): Linear(in_features=1000, out_features=250, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=250, out_features=1, bias=True)\n",
       "      )\n",
       "      (output_2): Sequential(\n",
       "        (0): Linear(in_features=1000, out_features=250, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=250, out_features=1, bias=True)\n",
       "      )\n",
       "      (output_3): Sequential(\n",
       "        (0): Linear(in_features=1000, out_features=250, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=250, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (params): StraightThroughParameters(\n",
       "    (instance_norm): InstanceNorm1d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artisan_model.cuda()\n",
    "\n",
    "batch_size  = 50 \n",
    "n_samples  = 20    \n",
    "num_steps  = 300   \n",
    "\n",
    "affine_trans = False\n",
    "scheduler    = True\n",
    "loss_plots   = False\n",
    "\n",
    "energy = OverMaxEnergy(model=artisan_model,\n",
    "                       bias_cell=0)\n",
    "\n",
    "energy = AvgDiffEnergy(model=artisan_model,\n",
    "                       bias_cell=0)\n",
    "\n",
    "theta_ini = torch.randn(batch_size, 4, 200)\n",
    "params = StraightThroughParameters(data=theta_ini,\n",
    "                                   left_flank=left_flank,\n",
    "                                   right_flank=right_flank,\n",
    "                                   n_samples=n_samples,\n",
    "                                   affine=affine_trans)\n",
    "generator = FastSeqProp(energy_fn=energy,\n",
    "                        params=params)\n",
    "generator.cuda()\n",
    "# generator.run(steps=num_steps,\n",
    "#               learning_rate=0.5,\n",
    "#               step_print=5,\n",
    "#               lr_scheduler=scheduler,\n",
    "#               create_plot=loss_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-workstation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
